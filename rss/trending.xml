<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 08 Aug 2022 21:07:35 +0000</lastBuildDate>
    <item>
      <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
      <link>https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</link>
      <description><![CDATA[This paper deals with the problem of audio source separation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</guid>
    </item>
    <item>
      <title>GAUDI: A Neural Architect for Immersive 3D Scene Generation</title>
      <link>https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</link>
      <description><![CDATA[We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</guid>
    </item>
    <item>
      <title>MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training</title>
      <link>https://paperswithcode.com/paper/minvis-a-minimal-video-instance-segmentation</link>
      <description><![CDATA[By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minvis-a-minimal-video-instance-segmentation</guid>
    </item>
    <item>
      <title>Hybrid Spectrogram and Waveform Source Separation</title>
      <link>https://paperswithcode.com/paper/hybrid-spectrogram-and-waveform-source</link>
      <description><![CDATA[Source separation models either work on the spectrogram or waveform domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hybrid-spectrogram-and-waveform-source</guid>
    </item>
    <item>
      <title>Expanding Language-Image Pretrained Models for General Video Recognition</title>
      <link>https://paperswithcode.com/paper/expanding-language-image-pretrained-models</link>
      <description><![CDATA[Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/expanding-language-image-pretrained-models</guid>
    </item>
    <item>
      <title>MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures</title>
      <link>https://paperswithcode.com/paper/mobilenerf-exploiting-the-polygon</link>
      <description><![CDATA[Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilenerf-exploiting-the-polygon</guid>
    </item>
    <item>
      <title>NAFSSR: Stereo Image Super-Resolution Using NAFNet</title>
      <link>https://paperswithcode.com/paper/nafssr-stereo-image-super-resolution-using</link>
      <description><![CDATA[This paper inherits a strong and simple image restoration model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nafssr-stereo-image-super-resolution-using</guid>
    </item>
    <item>
      <title>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</title>
      <link>https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</link>
      <description><![CDATA[Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</guid>
    </item>
    <item>
      <title>Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language</title>
      <link>https://paperswithcode.com/paper/multi-scale-2d-temporal-adjacent-networks-for</link>
      <description><![CDATA[It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-2d-temporal-adjacent-networks-for</guid>
    </item>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>A Conversational Paradigm for Program Synthesis</title>
      <link>https://paperswithcode.com/paper/a-conversational-paradigm-for-program</link>
      <description><![CDATA[We train a family of large language models, called CodeGen, on natural language and programming language data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-conversational-paradigm-for-program</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>Pretraining is All You Need for Image-to-Image Translation</title>
      <link>https://paperswithcode.com/paper/pretraining-is-all-you-need-for-image-to</link>
      <description><![CDATA[We propose to use pretraining to boost general image-to-image translation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pretraining-is-all-you-need-for-image-to</guid>
    </item>
    <item>
      <title>In Defense of Online Models for Video Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/in-defense-of-online-models-for-video</link>
      <description><![CDATA[In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-defense-of-online-models-for-video</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet of Vehicles</title>
      <link>https://paperswithcode.com/paper/mth-ids-a-multi-tiered-hybrid-intrusion</link>
      <description><![CDATA[In this paper, the vulnerabilities of intra-vehicle and external networks are discussed, and a multi-tiered hybrid IDS that incorporates a signature-based IDS and an anomaly-based IDS is proposed to detect both known and unknown attacks on vehicular networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mth-ids-a-multi-tiered-hybrid-intrusion</guid>
    </item>
    <item>
      <title>Masked Autoencoders that Listen</title>
      <link>https://paperswithcode.com/paper/masked-autoencoders-that-listen</link>
      <description><![CDATA[Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/masked-autoencoders-that-listen</guid>
    </item>
    <item>
      <title>AvatarGen: a 3D Generative Model for Animatable Human Avatars</title>
      <link>https://paperswithcode.com/paper/avatargen-a-3d-generative-model-for</link>
      <description><![CDATA[Unsupervised generation of clothed virtual humans with various appearance and animatable poses is important for creating 3D human avatars and other AR/VR applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/avatargen-a-3d-generative-model-for</guid>
    </item>
    <item>
      <title>AdaCat: Adaptive Categorical Discretization for Autoregressive Models</title>
      <link>https://paperswithcode.com/paper/adacat-adaptive-categorical-discretization</link>
      <description><![CDATA[Autoregressive generative models can estimate complex continuous data distributions, like trajectory rollouts in an RL environment, image intensities, and audio.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adacat-adaptive-categorical-discretization</guid>
    </item>
    <item>
      <title>OCR-free Document Understanding Transformer</title>
      <link>https://paperswithcode.com/paper/donut-document-understanding-transformer</link>
      <description><![CDATA[Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/donut-document-understanding-transformer</guid>
    </item>
  </channel>
</rss>
