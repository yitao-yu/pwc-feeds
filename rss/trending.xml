<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 15 Jul 2024 09:15:37 +0000</lastBuildDate>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>SEED-Story: Multimodal Long Story Generation with Large Language Model</title>
      <link>https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</link>
      <description><![CDATA[We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</guid>
    </item>
    <item>
      <title>Grounding Image Matching in 3D with MASt3R</title>
      <link>https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</link>
      <description><![CDATA[Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>MambaVision: A Hybrid Mamba-Transformer Vision Backbone</title>
      <link>https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</link>
      <description><![CDATA[We propose a novel hybrid Mamba-Transformer backbone, denoted as MambaVision, which is specifically tailored for vision applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</guid>
    </item>
    <item>
      <title>OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training</title>
      <link>https://paperswithcode.com/paper/opendiloco-an-open-source-framework-for</link>
      <description><![CDATA[OpenDiLoCo is an open-source implementation and replication of the Distributed Low-Communication (DiLoCo) training method for large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opendiloco-an-open-source-framework-for</guid>
    </item>
    <item>
      <title>MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</title>
      <link>https://paperswithcode.com/paper/mobilellm-optimizing-sub-billion-parameter</link>
      <description><![CDATA[The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0. 7%/0. 8% than MobileLLM 125M/350M.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilellm-optimizing-sub-billion-parameter</guid>
    </item>
    <item>
      <title>ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation</title>
      <link>https://paperswithcode.com/paper/anole-an-open-autoregressive-native-large</link>
      <description><![CDATA[Previous open-source large multimodal models (LMMs) have faced several limitations: (1) they often lack native integration, requiring adapters to align visual representations with pre-trained large language models (LLMs); (2) many are restricted to single-modal generation; (3) while some support multimodal generation, they rely on separate diffusion models for visual modeling and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anole-an-open-autoregressive-native-large</guid>
    </item>
    <item>
      <title>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</title>
      <link>https://paperswithcode.com/paper/learning-to-learn-at-test-time-rnns-with</link>
      <description><![CDATA[We evaluate our instantiations at the scale of 125M to 1. 3B parameters, comparing with a strong Transformer and Mamba, a modern RNN.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-learn-at-test-time-rnns-with</guid>
    </item>
    <item>
      <title>MAVIS: Mathematical Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mavis-mathematical-visual-instruction-tuning</link>
      <description><![CDATA[We identify three key areas within MLLMs that need to be improved: visual encoding of math diagrams, diagram-language alignment, and mathematical reasoning skills.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mavis-mathematical-visual-instruction-tuning</guid>
    </item>
    <item>
      <title>Video Diffusion Alignment via Reward Gradients</title>
      <link>https://paperswithcode.com/paper/video-diffusion-alignment-via-reward</link>
      <description><![CDATA[We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment of the video diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-diffusion-alignment-via-reward</guid>
    </item>
    <item>
      <title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title>
      <link>https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</link>
      <description><![CDATA[With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</guid>
    </item>
    <item>
      <title>Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</title>
      <link>https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</link>
      <description><![CDATA[The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception</title>
      <link>https://paperswithcode.com/paper/densefusion-1m-merging-vision-experts-for</link>
      <description><![CDATA[To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/densefusion-1m-merging-vision-experts-for</guid>
    </item>
    <item>
      <title>Cradle: Empowering Foundation Agents Towards General Computer Control</title>
      <link>https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</link>
      <description><![CDATA[To handle this issue, we propose the General Computer Control (GCC) setting to restrict foundation agents to interact with software through the most unified and standardized interface, i. e., using screenshots as input and keyboard and mouse actions as output.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</guid>
    </item>
    <item>
      <title>FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds</title>
      <link>https://paperswithcode.com/paper/foleycrafter-bring-silent-videos-to-life-with</link>
      <description><![CDATA[Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foleycrafter-bring-silent-videos-to-life-with</guid>
    </item>
    <item>
      <title>Scaling Synthetic Data Creation with 1,000,000,000 Personas</title>
      <link>https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</link>
      <description><![CDATA[We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</guid>
    </item>
    <item>
      <title>Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene</title>
      <link>https://paperswithcode.com/paper/approaching-outside-scaling-unsupervised-3d</link>
      <description><![CDATA[In this paper, we are among the early attempts to integrate LiDAR data with 2D images for unsupervised 3D detection and introduce a new method, dubbed LiDAR-2D Self-paced Learning (LiSe).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/approaching-outside-scaling-unsupervised-3d</guid>
    </item>
    <item>
      <title>CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds</title>
      <link>https://paperswithcode.com/paper/corrnet3d-unsupervised-end-to-end-learning-of</link>
      <description><![CDATA[The symmetric deformer, with an additional regularized loss, transforms the two permuted point clouds to each other to drive the unsupervised learning of the correspondence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/corrnet3d-unsupervised-end-to-end-learning-of</guid>
    </item>
  </channel>
</rss>
