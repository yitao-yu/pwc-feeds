<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 04 Feb 2024 09:11:56 +0000</lastBuildDate>
    <item>
      <title>OLMo: Accelerating the Science of Language Models</title>
      <link>https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</link>
      <description><![CDATA[Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</guid>
    </item>
    <item>
      <title>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</link>
      <description><![CDATA[In this work, we propose a novel training strategy MoE-tuning for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</guid>
    </item>
    <item>
      <title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title>
      <link>https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</link>
      <description><![CDATA[The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</guid>
    </item>
    <item>
      <title>InstantID: Zero-shot Identity-Preserving Generation in Seconds</title>
      <link>https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</link>
      <description><![CDATA[There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</title>
      <link>https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</link>
      <description><![CDATA[To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</guid>
    </item>
    <item>
      <title>High-Quality Image Restoration Following Human Instructions</title>
      <link>https://paperswithcode.com/paper/high-quality-image-restoration-following</link>
      <description><![CDATA[All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-quality-image-restoration-following</guid>
    </item>
    <item>
      <title>Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization</title>
      <link>https://paperswithcode.com/paper/flexibly-scaling-large-language-models</link>
      <description><![CDATA[Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flexibly-scaling-large-language-models</guid>
    </item>
    <item>
      <title>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</title>
      <link>https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</link>
      <description><![CDATA[To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</guid>
    </item>
    <item>
      <title>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</title>
      <link>https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</link>
      <description><![CDATA[Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</guid>
    </item>
    <item>
      <title>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning</title>
      <link>https://paperswithcode.com/paper/animatelcm-accelerating-the-animation-of</link>
      <description><![CDATA[We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatelcm-accelerating-the-animation-of</guid>
    </item>
    <item>
      <title>DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</link>
      <description><![CDATA[The rapid development of large language models has revolutionized code intelligence in software development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</guid>
    </item>
    <item>
      <title>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</title>
      <link>https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</link>
      <description><![CDATA[Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</guid>
    </item>
    <item>
      <title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</title>
      <link>https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</link>
      <description><![CDATA[In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</guid>
    </item>
    <item>
      <title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model</title>
      <link>https://paperswithcode.com/paper/internlm-xcomposer2-mastering-free-form-text</link>
      <description><![CDATA[We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internlm-xcomposer2-mastering-free-form-text</guid>
    </item>
    <item>
      <title>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</title>
      <link>https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</link>
      <description><![CDATA[In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</guid>
    </item>
    <item>
      <title>In-Context Learning for Extreme Multi-Label Classification</title>
      <link>https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</link>
      <description><![CDATA[Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</guid>
    </item>
    <item>
      <title>Proactive Detection of Voice Cloning with Localized Watermarking</title>
      <link>https://paperswithcode.com/paper/proactive-detection-of-voice-cloning-with</link>
      <description><![CDATA[In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/proactive-detection-of-voice-cloning-with</guid>
    </item>
    <item>
      <title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title>
      <link>https://paperswithcode.com/paper/kvquant-towards-10-million-context-length-llm</link>
      <description><![CDATA[LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kvquant-towards-10-million-context-length-llm</guid>
    </item>
    <item>
      <title>Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation</title>
      <link>https://paperswithcode.com/paper/hi-sam-marrying-segment-anything-model-for</link>
      <description><![CDATA[In terms of the AMG mode, Hi-SAM segments text stroke foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hi-sam-marrying-segment-anything-model-for</guid>
    </item>
    <item>
      <title>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook</title>
      <link>https://paperswithcode.com/paper/large-models-for-time-series-and-spatio</link>
      <description><![CDATA[In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-models-for-time-series-and-spatio</guid>
    </item>
  </channel>
</rss>
