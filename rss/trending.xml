<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 25 Oct 2024 21:08:34 +0000</lastBuildDate>
    <item>
      <title>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</title>
      <link>https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</link>
      <description><![CDATA[To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</guid>
    </item>
    <item>
      <title>Data Formulator 2: Iteratively Creating Rich Visualizations with AI</title>
      <link>https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</link>
      <description><![CDATA[To create rich visualizations, data analysts often need to iterate back and forth among data processing and chart specification to achieve their goals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</guid>
    </item>
    <item>
      <title>Allegro: Open the Black Box of Commercial-Level Video Generation Model</title>
      <link>https://paperswithcode.com/paper/allegro-open-the-black-box-of-commercial</link>
      <description><![CDATA[Significant advancements have been made in the field of video generation, with the open-source community contributing a wealth of research papers and tools for training high-quality models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/allegro-open-the-black-box-of-commercial</guid>
    </item>
    <item>
      <title>1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs</title>
      <link>https://paperswithcode.com/paper/1-bit-ai-infra-part-1-1-fast-and-lossless</link>
      <description><![CDATA[Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1. 58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/1-bit-ai-infra-part-1-1-fast-and-lossless</guid>
    </item>
    <item>
      <title>Blendify -- Python rendering framework for Blender</title>
      <link>https://paperswithcode.com/paper/blendify-python-rendering-framework-for</link>
      <description><![CDATA[With the rapid growth of the volume of research fields like computer vision and computer graphics, researchers require effective and user-friendly rendering tools to visualize results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blendify-python-rendering-framework-for</guid>
    </item>
    <item>
      <title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title>
      <link>https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</link>
      <description><![CDATA[It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</guid>
    </item>
    <item>
      <title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/janus-decoupling-visual-encoding-for-unified</link>
      <description><![CDATA[In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/janus-decoupling-visual-encoding-for-unified</guid>
    </item>
    <item>
      <title>Spirit LM: Interleaved Spoken and Written Language Model</title>
      <link>https://paperswithcode.com/paper/spirit-lm-interleaved-spoken-and-written</link>
      <description><![CDATA[Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spirit-lm-interleaved-spoken-and-written</guid>
    </item>
    <item>
      <title>CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos</title>
      <link>https://paperswithcode.com/paper/cotracker3-simpler-and-better-point-tracking</link>
      <description><![CDATA[Most state-of-the-art point trackers are trained on synthetic data due to the difficulty of annotating real videos for this task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cotracker3-simpler-and-better-point-tracking</guid>
    </item>
    <item>
      <title>DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception</title>
      <link>https://paperswithcode.com/paper/doclayout-yolo-enhancing-document-layout</link>
      <description><![CDATA[Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/doclayout-yolo-enhancing-document-layout</guid>
    </item>
    <item>
      <title>Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents</title>
      <link>https://paperswithcode.com/paper/chain-of-ideas-revolutionizing-research-in</link>
      <description><![CDATA[Moreover, our CoI agent is budget-friendly, with a minimum cost of \$0. 50 to generate a candidate idea and its corresponding experimental design.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chain-of-ideas-revolutionizing-research-in</guid>
    </item>
    <item>
      <title>Fast Inference from Transformers via Speculative Decoding</title>
      <link>https://paperswithcode.com/paper/fast-inference-from-transformers-via</link>
      <description><![CDATA[Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-inference-from-transformers-via</guid>
    </item>
    <item>
      <title>DepthSplat: Connecting Gaussian Splatting and Depth</title>
      <link>https://paperswithcode.com/paper/depthsplat-connecting-gaussian-splatting-and</link>
      <description><![CDATA[Gaussian splatting and single/multi-view depth estimation are typically studied in isolation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depthsplat-connecting-gaussian-splatting-and</guid>
    </item>
    <item>
      <title>VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents</title>
      <link>https://paperswithcode.com/paper/visrag-vision-based-retrieval-augmented</link>
      <description><![CDATA[In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visrag-vision-based-retrieval-augmented</guid>
    </item>
    <item>
      <title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title>
      <link>https://paperswithcode.com/paper/sam2long-enhancing-sam-2-for-long-video</link>
      <description><![CDATA[Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sam2long-enhancing-sam-2-for-long-video</guid>
    </item>
    <item>
      <title>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching</title>
      <link>https://paperswithcode.com/paper/f5-tts-a-fairytaler-that-fakes-fluent-and</link>
      <description><![CDATA[This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/f5-tts-a-fairytaler-that-fakes-fluent-and</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement</title>
      <link>https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</link>
      <description><![CDATA[When pretrained on Objects365, D-FINE-L / X attains 57. 1% / 59. 3% AP, surpassing all existing real-time detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</guid>
    </item>
    <item>
      <title>OmniGen: Unified Image Generation</title>
      <link>https://paperswithcode.com/paper/omnigen-unified-image-generation</link>
      <description><![CDATA[In this work, we introduce OmniGen, a new diffusion model for unified image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnigen-unified-image-generation</guid>
    </item>
    <item>
      <title>LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title>
      <link>https://paperswithcode.com/paper/layer-skip-enabling-early-exit-inference-and</link>
      <description><![CDATA[We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layer-skip-enabling-early-exit-inference-and</guid>
    </item>
  </channel>
</rss>
