<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 28 Jan 2025 09:16:04 +0000</lastBuildDate>
    <item>
      <title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title>
      <link>https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</link>
      <description><![CDATA[This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e. g., keyboard and mouse operations).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</guid>
    </item>
    <item>
      <title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/deepseek-r1-incentivizing-reasoning</link>
      <description><![CDATA[We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-r1-incentivizing-reasoning</guid>
    </item>
    <item>
      <title>DeepSeek-V3 Technical Report</title>
      <link>https://paperswithcode.com/paper/deepseek-v3-technical-report</link>
      <description><![CDATA[We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-v3-technical-report</guid>
    </item>
    <item>
      <title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title>
      <link>https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</link>
      <description><![CDATA[This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</guid>
    </item>
    <item>
      <title>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title>
      <link>https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language</link>
      <description><![CDATA[The rapid development of open-source large language models (LLMs) has been truly remarkable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language</guid>
    </item>
    <item>
      <title>Flaming-hot Initiation with Regular Execution Sampling for Large Language Models</title>
      <link>https://paperswithcode.com/paper/flaming-hot-initiation-with-regular-execution</link>
      <description><![CDATA[Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flaming-hot-initiation-with-regular-execution</guid>
    </item>
    <item>
      <title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</link>
      <description><![CDATA[Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</guid>
    </item>
    <item>
      <title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title>
      <link>https://paperswithcode.com/paper/can-we-generate-images-with-cot-let-s-verify</link>
      <description><![CDATA[We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-we-generate-images-with-cot-let-s-verify</guid>
    </item>
    <item>
      <title>PaSa: An LLM Agent for Comprehensive Academic Paper Search</title>
      <link>https://paperswithcode.com/paper/pasa-an-llm-agent-for-comprehensive-academic</link>
      <description><![CDATA[Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37. 78% in recall@20 and 39. 90% in recall@50.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pasa-an-llm-agent-for-comprehensive-academic</guid>
    </item>
    <item>
      <title>DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</link>
      <description><![CDATA[The rapid development of large language models has revolutionized code intelligence in software development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</guid>
    </item>
    <item>
      <title>HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/hdr-gs-efficient-high-dynamic-range-novel</link>
      <description><![CDATA[In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hdr-gs-efficient-high-dynamic-range-novel</guid>
    </item>
    <item>
      <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding</title>
      <link>https://paperswithcode.com/paper/videollama-3-frontier-multimodal-foundation</link>
      <description><![CDATA[The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videollama-3-frontier-multimodal-foundation</guid>
    </item>
    <item>
      <title>Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback</title>
      <link>https://paperswithcode.com/paper/align-anything-training-all-modality-models</link>
      <description><![CDATA[In this work, we make the first attempt to fine-tune all-modality models (i. e. input and output with any modality, also named any-to-any models) using human preference data across all modalities (including text, image, audio, and video), ensuring its behavior aligns with human intentions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/align-anything-training-all-modality-models</guid>
    </item>
    <item>
      <title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title>
      <link>https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</link>
      <description><![CDATA[We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</guid>
    </item>
    <item>
      <title>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</title>
      <link>https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</link>
      <description><![CDATA[The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</guid>
    </item>
    <item>
      <title>DiffuEraser: A Diffusion Model for Video Inpainting</title>
      <link>https://paperswithcode.com/paper/diffueraser-a-diffusion-model-for-video</link>
      <description><![CDATA[Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffueraser-a-diffusion-model-for-video</guid>
    </item>
    <item>
      <title>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title>
      <link>https://paperswithcode.com/paper/deepseekmath-pushing-the-limits-of</link>
      <description><![CDATA[Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseekmath-pushing-the-limits-of</guid>
    </item>
    <item>
      <title>FoundationStereo: Zero-Shot Stereo Matching</title>
      <link>https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</link>
      <description><![CDATA[However, achieving strong zero-shot generalization - a hallmark of foundation models in other computer vision tasks - remains challenging for stereo matching.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</guid>
    </item>
    <item>
      <title>IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems</title>
      <link>https://paperswithcode.com/paper/intellagent-a-multi-agent-framework-for</link>
      <description><![CDATA[IntellAgent represents a paradigm shift in evaluating conversational AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intellagent-a-multi-agent-framework-for</guid>
    </item>
    <item>
      <title>Making Images Real Again: A Comprehensive Survey on Deep Image Composition</title>
      <link>https://paperswithcode.com/paper/making-images-real-again-a-comprehensive</link>
      <description><![CDATA[Image composition task could be decomposed into multiple sub-tasks, in which each sub-task targets at one or more issues.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/making-images-real-again-a-comprehensive</guid>
    </item>
  </channel>
</rss>
