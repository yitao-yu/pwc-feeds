<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 23 Jan 2025 21:08:25 +0000</lastBuildDate>
    <item>
      <title>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</title>
      <link>https://paperswithcode.com/paper/2408-01800</link>
      <description><![CDATA[The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-01800</guid>
    </item>
    <item>
      <title>MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/minirag-towards-extremely-simple-retrieval</link>
      <description><![CDATA[The growing demand for efficient and lightweight Retrieval-Augmented Generation (RAG) systems has highlighted significant challenges when deploying Small Language Models (SLMs) in existing RAG frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minirag-towards-extremely-simple-retrieval</guid>
    </item>
    <item>
      <title>Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</title>
      <link>https://paperswithcode.com/paper/stretching-each-dollar-diffusion-training</link>
      <description><![CDATA[As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stretching-each-dollar-diffusion-training</guid>
    </item>
    <item>
      <title>Monolith: Real Time Recommendation System With Collisionless Embedding Table</title>
      <link>https://paperswithcode.com/paper/monolith-real-time-recommendation-system-with</link>
      <description><![CDATA[In this paper, we present Monolith, a system tailored for online training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monolith-real-time-recommendation-system-with</guid>
    </item>
    <item>
      <title>Tensor Product Attention Is All You Need</title>
      <link>https://paperswithcode.com/paper/tensor-product-attention-is-all-you-need</link>
      <description><![CDATA[Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tensor-product-attention-is-all-you-need</guid>
    </item>
    <item>
      <title>Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos</title>
      <link>https://paperswithcode.com/paper/sa2va-marrying-sam2-with-llava-for-dense</link>
      <description><![CDATA[This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sa2va-marrying-sam2-with-llava-for-dense</guid>
    </item>
    <item>
      <title>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</title>
      <link>https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</link>
      <description><![CDATA[The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</guid>
    </item>
    <item>
      <title>UnCommon Objects in 3D</title>
      <link>https://paperswithcode.com/paper/uncommon-objects-in-3d</link>
      <description><![CDATA[We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uncommon-objects-in-3d</guid>
    </item>
    <item>
      <title>$\text{Transformer}^2$: Self-adaptive LLMs</title>
      <link>https://paperswithcode.com/paper/text-transformer-2-self-adaptive-llms</link>
      <description><![CDATA[Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-transformer-2-self-adaptive-llms</guid>
    </item>
    <item>
      <title>Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks</title>
      <link>https://paperswithcode.com/paper/hallo3-highly-dynamic-and-realistic-portrait</link>
      <description><![CDATA[Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hallo3-highly-dynamic-and-realistic-portrait</guid>
    </item>
    <item>
      <title>Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback</title>
      <link>https://paperswithcode.com/paper/align-anything-training-all-modality-models</link>
      <description><![CDATA[In this work, we make the first attempt to fine-tune all-modality models (i. e. input and output with any modality, also named any-to-any models) using human preference data across all modalities (including text, image, audio, and video), ensuring its behavior aligns with human intentions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/align-anything-training-all-modality-models</guid>
    </item>
    <item>
      <title>Search-o1: Agentic Search-Enhanced Large Reasoning Models</title>
      <link>https://paperswithcode.com/paper/search-o1-agentic-search-enhanced-large</link>
      <description><![CDATA[To address this limitation, we introduce \textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/search-o1-agentic-search-enhanced-large</guid>
    </item>
    <item>
      <title>PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides</title>
      <link>https://paperswithcode.com/paper/pptagent-generating-and-evaluating</link>
      <description><![CDATA[Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pptagent-generating-and-evaluating</guid>
    </item>
    <item>
      <title>LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync</title>
      <link>https://paperswithcode.com/paper/latentsync-audio-conditioned-latent-diffusion</link>
      <description><![CDATA[Since we did not change the overall training framework of SyncNet, our experience can also be applied to other lip sync and audio-driven portrait animation methods that utilize SyncNet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/latentsync-audio-conditioned-latent-diffusion</guid>
    </item>
    <item>
      <title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2409-13731</link>
      <description><![CDATA[The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2409-13731</guid>
    </item>
    <item>
      <title>FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors</title>
      <link>https://paperswithcode.com/paper/framepainter-endowing-interactive-image</link>
      <description><![CDATA[We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \eg, automatically adjust the reflection of the cup.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/framepainter-endowing-interactive-image</guid>
    </item>
    <item>
      <title>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs</title>
      <link>https://paperswithcode.com/paper/openscholar-synthesizing-scientific</link>
      <description><![CDATA[Scientific progress depends on researchers' ability to synthesize the growing body of literature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openscholar-synthesizing-scientific</guid>
    </item>
    <item>
      <title>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</title>
      <link>https://paperswithcode.com/paper/meta-learning-a-real-time-tabular-automl</link>
      <description><![CDATA[We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-learning-a-real-time-tabular-automl</guid>
    </item>
    <item>
      <title>OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer</title>
      <link>https://paperswithcode.com/paper/omagent-a-multi-modal-agent-framework-for</link>
      <description><![CDATA[Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omagent-a-multi-modal-agent-framework-for</guid>
    </item>
    <item>
      <title>SVFR: A Unified Framework for Generalized Video Face Restoration</title>
      <link>https://paperswithcode.com/paper/svfr-a-unified-framework-for-generalized</link>
      <description><![CDATA[In this paper, we propose a novel approach for the Generalized Video Face Restoration (GVFR) task, which integrates video BFR, inpainting, and colorization tasks that we empirically show to benefit each other.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/svfr-a-unified-framework-for-generalized</guid>
    </item>
  </channel>
</rss>
