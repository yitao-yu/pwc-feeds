<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 10 May 2025 21:08:34 +0000</lastBuildDate>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</link>
      <description><![CDATA[Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</guid>
    </item>
    <item>
      <title>PixelHacker: Image Inpainting with Structural and Semantic Consistency</title>
      <link>https://paperswithcode.com/paper/pixelhacker-image-inpainting-with-structural</link>
      <description><![CDATA[Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixelhacker-image-inpainting-with-structural</guid>
    </item>
    <item>
      <title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
      <link>https://paperswithcode.com/paper/voila-voice-language-foundation-models-for</link>
      <description><![CDATA[A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voila-voice-language-foundation-models-for</guid>
    </item>
    <item>
      <title>Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</title>
      <link>https://paperswithcode.com/paper/unified-multimodal-understanding-and</link>
      <description><![CDATA[Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unified-multimodal-understanding-and</guid>
    </item>
    <item>
      <title>LTX-Video: Realtime Video Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</link>
      <description><![CDATA[To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</guid>
    </item>
    <item>
      <title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
      <link>https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</link>
      <description><![CDATA[Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</guid>
    </item>
    <item>
      <title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
      <link>https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</link>
      <description><![CDATA[At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</guid>
    </item>
    <item>
      <title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
      <link>https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</link>
      <description><![CDATA[Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</guid>
    </item>
    <item>
      <title>LiftFeat: 3D Geometry-Aware Local Feature Matching</title>
      <link>https://paperswithcode.com/paper/liftfeat-3d-geometry-aware-local-feature</link>
      <description><![CDATA[We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liftfeat-3d-geometry-aware-local-feature</guid>
    </item>
    <item>
      <title>PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding</title>
      <link>https://paperswithcode.com/paper/perceptionlm-open-access-data-and-models-for</link>
      <description><![CDATA[In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/perceptionlm-open-access-data-and-models-for</guid>
    </item>
    <item>
      <title>LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis</title>
      <link>https://paperswithcode.com/paper/llama-omni2-llm-based-real-time-spoken</link>
      <description><![CDATA[Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-omni2-llm-based-real-time-spoken</guid>
    </item>
    <item>
      <title>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</title>
      <link>https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</link>
      <description><![CDATA[Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</guid>
    </item>
    <item>
      <title>PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides</title>
      <link>https://paperswithcode.com/paper/pptagent-generating-and-evaluating</link>
      <description><![CDATA[Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pptagent-generating-and-evaluating</guid>
    </item>
    <item>
      <title>EdgeTAM: On-Device Track Anything Model</title>
      <link>https://paperswithcode.com/paper/edgetam-on-device-track-anything-model</link>
      <description><![CDATA[Given that video segmentation is a dense prediction task, we find preserving the spatial structure of the memories is essential so that the queries are split into global-level and patch-level groups.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/edgetam-on-device-track-anything-model</guid>
    </item>
    <item>
      <title>BayesFlow: Amortized Bayesian Workflows With Neural Networks</title>
      <link>https://paperswithcode.com/paper/bayesflow-amortized-bayesian-workflows-with</link>
      <description><![CDATA[Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bayesflow-amortized-bayesian-workflows-with</guid>
    </item>
    <item>
      <title>SkyReels-V2: Infinite-length Film Generative Model</title>
      <link>https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</link>
      <description><![CDATA[Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</guid>
    </item>
    <item>
      <title>Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</title>
      <link>https://paperswithcode.com/paper/locate-3d-real-world-object-localization-via</link>
      <description><![CDATA[LOCATE 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/locate-3d-real-world-object-localization-via</guid>
    </item>
    <item>
      <title>D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement</title>
      <link>https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</link>
      <description><![CDATA[When pretrained on Objects365, D-FINE-L / X attains 57. 1% / 59. 3% AP, surpassing all existing real-time detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</guid>
    </item>
    <item>
      <title>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/r1-reward-training-multimodal-reward-model</link>
      <description><![CDATA[Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r1-reward-training-multimodal-reward-model</guid>
    </item>
    <item>
      <title>Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models</title>
      <link>https://paperswithcode.com/paper/attentive-reasoning-queries-a-systematic</link>
      <description><![CDATA[We present Attentive Reasoning Queries (ARQs), a novel structured reasoning approach that significantly improves instruction-following in Large Language Models through domain-specialized reasoning blueprints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/attentive-reasoning-queries-a-systematic</guid>
    </item>
  </channel>
</rss>
