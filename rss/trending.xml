<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 31 May 2025 21:08:56 +0000</lastBuildDate>
    <item>
      <title>Emerging Properties in Unified Multimodal Pretraining</title>
      <link>https://paperswithcode.com/paper/emerging-properties-in-unified-multimodal</link>
      <description><![CDATA[Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/emerging-properties-in-unified-multimodal</guid>
    </item>
    <item>
      <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</link>
      <description><![CDATA[This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</guid>
    </item>
    <item>
      <title>syftr: Pareto-Optimal Generative AI</title>
      <link>https://paperswithcode.com/paper/syftr-pareto-optimal-generative-ai</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) pipelines are central to applying large language models (LLMs) to proprietary or dynamic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/syftr-pareto-optimal-generative-ai</guid>
    </item>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
      <link>https://paperswithcode.com/paper/uncertainty-quantification-for-language</link>
      <description><![CDATA[This approach enables practitioners to optimize the ensemble for a specific use case for improved performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uncertainty-quantification-for-language</guid>
    </item>
    <item>
      <title>AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment</title>
      <link>https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</link>
      <description><![CDATA[In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</guid>
    </item>
    <item>
      <title>Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution</title>
      <link>https://paperswithcode.com/paper/alita-generalist-agent-enabling-scalable</link>
      <description><![CDATA[For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alita-generalist-agent-enabling-scalable</guid>
    </item>
    <item>
      <title>Aligning Anime Video Generation with Human Feedback</title>
      <link>https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</link>
      <description><![CDATA[Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</guid>
    </item>
    <item>
      <title>DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/deepeyes-incentivizing-thinking-with-images</link>
      <description><![CDATA[Large Vision-Language Models (VLMs) have shown strong capabilities in multimodal understanding and reasoning, yet they are primarily constrained by text-based reasoning processes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepeyes-incentivizing-thinking-with-images</guid>
    </item>
    <item>
      <title>MMaDA: Multimodal Large Diffusion Language Models</title>
      <link>https://paperswithcode.com/paper/mmada-multimodal-large-diffusion-language</link>
      <description><![CDATA[We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmada-multimodal-large-diffusion-language</guid>
    </item>
    <item>
      <title>Learning to Reason without External Rewards</title>
      <link>https://paperswithcode.com/paper/learning-to-reason-without-external-rewards</link>
      <description><![CDATA[Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-reason-without-external-rewards</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
    <item>
      <title>qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description><![CDATA[Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions.]]></description>
      <guid isPermaLink="true">https://github.com/microsoft/qlib</guid>
    </item>
    <item>
      <title>OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/opens2v-nexus-a-detailed-benchmark-and</link>
      <description><![CDATA[In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opens2v-nexus-a-detailed-benchmark-and</guid>
    </item>
    <item>
      <title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
      <link>https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</link>
      <description><![CDATA[Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</guid>
    </item>
    <item>
      <title>Training-Free Efficient Video Generation via Dynamic Token Carving</title>
      <link>https://paperswithcode.com/paper/training-free-efficient-video-generation-via</link>
      <description><![CDATA[Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-free-efficient-video-generation-via</guid>
    </item>
    <item>
      <title>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models</title>
      <link>https://paperswithcode.com/paper/audiotrust-benchmarking-the-multifaceted</link>
      <description><![CDATA[We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audiotrust-benchmarking-the-multifaceted</guid>
    </item>
    <item>
      <title>ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels</title>
      <link>https://paperswithcode.com/paper/rocket-exceptionally-fast-and-accurate-time</link>
      <description><![CDATA[Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rocket-exceptionally-fast-and-accurate-time</guid>
    </item>
    <item>
      <title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
      <link>https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</link>
      <description><![CDATA[Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</guid>
    </item>
    <item>
      <title>Phi-4 Technical Report</title>
      <link>https://paperswithcode.com/paper/phi-4-technical-report</link>
      <description><![CDATA[We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/phi-4-technical-report</guid>
    </item>
  </channel>
</rss>
