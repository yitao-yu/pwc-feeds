<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 05 Nov 2022 09:15:10 +0000</lastBuildDate>
    <item>
      <title>Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models</title>
      <link>https://paperswithcode.com/paper/efficient-spatially-sparse-inference-for</link>
      <description><![CDATA[With 1. 2%-area edited regions, our method reduces the computation of DDIM by 7. 5$\times$ and GauGAN by 18$\times$ while preserving the visual fidelity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-spatially-sparse-inference-for</guid>
    </item>
    <item>
      <title>High Fidelity Neural Audio Compression</title>
      <link>https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</link>
      <description><![CDATA[We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</guid>
    </item>
    <item>
      <title>Pop2Piano : Pop Audio-based Piano Cover Generation</title>
      <link>https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</link>
      <description><![CDATA[The piano cover of pop music is widely enjoyed by people.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization</title>
      <link>https://paperswithcode.com/paper/shapo-implicit-representations-for-multi</link>
      <description><![CDATA[A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shapo-implicit-representations-for-multi</guid>
    </item>
    <item>
      <title>Language models enable zero-shot prediction of the effects of mutations on protein function</title>
      <link>https://paperswithcode.com/paper/language-models-enable-zero-shot-prediction</link>
      <description><![CDATA[Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-enable-zero-shot-prediction</guid>
    </item>
    <item>
      <title>Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform</title>
      <link>https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</link>
      <description><![CDATA[We propose a lightweight end-to-end text-to-speech model using multi-band generation and inverse short-time Fourier transform.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>Large Language Models Are Human-Level Prompt Engineers</title>
      <link>https://paperswithcode.com/paper/large-language-models-are-human-level-prompt</link>
      <description><![CDATA[By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-models-are-human-level-prompt</guid>
    </item>
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</link>
      <description><![CDATA[The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</guid>
    </item>
    <item>
      <title>Text-Only Training for Image Captioning using Noise-Injected CLIP</title>
      <link>https://paperswithcode.com/paper/text-only-training-for-image-captioning-using</link>
      <description><![CDATA[We consider the task of image-captioning using only the CLIP model and additional text data at training time, and no additional captioned images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-only-training-for-image-captioning-using</guid>
    </item>
    <item>
      <title>Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective</title>
      <link>https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</link>
      <description><![CDATA[We propose a new paradigm for zero-shot learners that is format agnostic, i. e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</guid>
    </item>
    <item>
      <title>Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation</title>
      <link>https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</link>
      <description><![CDATA[In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</guid>
    </item>
    <item>
      <title>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models</title>
      <link>https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</link>
      <description><![CDATA[We analyze prompts in the dataset and discuss key properties of these prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</guid>
    </item>
    <item>
      <title>UniInst: Unique Representation for End-to-End Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/uniinst-unique-representation-for-end-to-end</link>
      <description><![CDATA[Existing instance segmentation methods have achieved impressive performance but still suffer from a common dilemma: redundant representations (e. g., multiple boxes, grids, and anchor points) are inferred for one instance, which leads to multiple duplicated predictions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uniinst-unique-representation-for-end-to-end</guid>
    </item>
    <item>
      <title>Unsupervised visualization of image datasets using contrastive learning</title>
      <link>https://paperswithcode.com/paper/unsupervised-visualization-of-image-datasets</link>
      <description><![CDATA[This problem can be circumvented by self-supervised approaches based on contrastive learning, such as SimCLR, relying on data augmentation to generate implicit neighbors, but these methods do not produce two-dimensional embeddings suitable for visualization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-visualization-of-image-datasets</guid>
    </item>
    <item>
      <title>A Surprising Thing: The Application of Machine Learning Ensembles and Signal Theory to Predict Earnings Surprises</title>
      <link>https://paperswithcode.com/paper/a-surprising-thing-the-application-of-machine</link>
      <description><![CDATA[Nonlinear classification models can predict future earnings surprises with a high accuracy by using pricing and earnings input data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-surprising-thing-the-application-of-machine</guid>
    </item>
    <item>
      <title>DeepLearningExamples</title>
      <link>https://github.com/NVIDIA/DeepLearningExamples</link>
      <description><![CDATA[Deep Learning Examples]]></description>
      <guid isPermaLink="true">https://github.com/NVIDIA/DeepLearningExamples</guid>
    </item>
    <item>
      <title>WeightedSHAP: analyzing and improving Shapley based feature attributions</title>
      <link>https://paperswithcode.com/paper/weightedshap-analyzing-and-improving-shapley</link>
      <description><![CDATA[On several real-world datasets, we demonstrate that the influential features identified by WeightedSHAP are better able to recapitulate the model's predictions compared to the features identified by the Shapley value.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/weightedshap-analyzing-and-improving-shapley</guid>
    </item>
    <item>
      <title>Focal Modulation Networks</title>
      <link>https://paperswithcode.com/paper/focal-modulation-networks</link>
      <description><![CDATA[For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2. 4, and beats Swin at multi-scale (50. 5 v. s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/focal-modulation-networks</guid>
    </item>
  </channel>
</rss>
