<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 10 Jul 2025 09:21:33 +0000</lastBuildDate>
    <item>
      <title>SymbolicAI: A framework for logic-based approaches combining generative models and solvers</title>
      <link>https://paperswithcode.com/paper/symbolicai-a-framework-for-logic-based</link>
      <description><![CDATA[Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/symbolicai-a-framework-for-logic-based</guid>
    </item>
    <item>
      <title>Do Large Language Models Need a Content Delivery Network?</title>
      <link>https://paperswithcode.com/paper/do-large-language-models-need-a-content</link>
      <description><![CDATA[As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-large-language-models-need-a-content</guid>
    </item>
    <item>
      <title>OmniGen2: Exploration to Advanced Multimodal Generation</title>
      <link>https://paperswithcode.com/paper/omnigen2-exploration-to-advanced-multimodal</link>
      <description><![CDATA[To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnigen2-exploration-to-advanced-multimodal</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</title>
      <link>https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</link>
      <description><![CDATA[Drawing inspiration from the orchestration of diverse specialized agents collaborating to tackle intricate tasks, we propose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that harnesses multi-agent collaboration to bolster the faithfulness and causality of foundation models, involving a set of reasoners and evaluators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</guid>
    </item>
    <item>
      <title>MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</title>
      <link>https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</link>
      <description><![CDATA[To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</guid>
    </item>
    <item>
      <title>AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora</title>
      <link>https://paperswithcode.com/paper/autoschemakg-autonomous-knowledge-graph</link>
      <description><![CDATA[We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoschemakg-autonomous-knowledge-graph</guid>
    </item>
    <item>
      <title>FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution</title>
      <link>https://paperswithcode.com/paper/flashdepth-real-time-streaming-video-depth</link>
      <description><![CDATA[A versatile video depth estimation model should (1) be accurate and consistent across frames, (2) produce high-resolution depth maps, and (3) support real-time streaming.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashdepth-real-time-streaming-video-depth</guid>
    </item>
    <item>
      <title>ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training</title>
      <link>https://paperswithcode.com/paper/ernie-vil-2-0-multi-view-contrastive-learning</link>
      <description><![CDATA[They attempt to learn cross-modal representation using contrastive learning on image-text pairs, however, the built inter-modal correlations only rely on a single view for each modality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ernie-vil-2-0-multi-view-contrastive-learning</guid>
    </item>
    <item>
      <title>Adapting Precomputed Features for Efficient Graph Condensation</title>
      <link>https://paperswithcode.com/paper/adapting-precomputed-features-for-efficient</link>
      <description><![CDATA[To address this, Graph Condensation (GC) methods aim to compress large graphs into smaller, synthetic ones that are more manageable for GNN training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adapting-precomputed-features-for-efficient</guid>
    </item>
    <item>
      <title>Smaller But Better: Unifying Layout Generation with Smaller Large Language Models</title>
      <link>https://paperswithcode.com/paper/smaller-but-better-unifying-layout-generation</link>
      <description><![CDATA[We propose LGGPT, an LLM-based model tailored for unified layout generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/smaller-but-better-unifying-layout-generation</guid>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
      <link>https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</link>
      <description><![CDATA[To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</guid>
    </item>
    <item>
      <title>ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks</title>
      <link>https://paperswithcode.com/paper/itbench-evaluating-ai-agents-across-diverse</link>
      <description><![CDATA[Our results show that agents powered by state-of-the-art models resolve only 13. 8% of SRE scenarios, 25. 2% of CISO scenarios, and 0% of FinOps scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/itbench-evaluating-ai-agents-across-diverse</guid>
    </item>
    <item>
      <title>ManimML: Communicating Machine Learning Architectures with Animation</title>
      <link>https://paperswithcode.com/paper/manimml-communicating-machine-learning</link>
      <description><![CDATA[A user can take a preexisting neural network architecture and easily write a specification for an animation in ManimML, which will then automatically compose animations for different components of the system into a final animation of the entire neural network.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/manimml-communicating-machine-learning</guid>
    </item>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>Mirage: A Multi-Level Superoptimizer for Tensor Programs</title>
      <link>https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</link>
      <description><![CDATA[We introduce Mirage, the first multi-level superoptimizer for tensor programs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</guid>
    </item>
    <item>
      <title>Continuous Thought Machines</title>
      <link>https://paperswithcode.com/paper/continuous-thought-machines</link>
      <description><![CDATA[The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/continuous-thought-machines</guid>
    </item>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment</title>
      <link>https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</link>
      <description><![CDATA[In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</guid>
    </item>
    <item>
      <title>Skill Expansion and Composition in Parameter Space</title>
      <link>https://paperswithcode.com/paper/skill-expansion-and-composition-in-parameter</link>
      <description><![CDATA[In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/skill-expansion-and-composition-in-parameter</guid>
    </item>
  </channel>
</rss>
