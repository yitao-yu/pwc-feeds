<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 16 Jul 2023 21:06:22 +0000</lastBuildDate>
    <item>
      <title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title>
      <link>https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</link>
      <description><![CDATA[With the advance of text-to-image models (e. g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</guid>
    </item>
    <item>
      <title>Generative Pretraining in Multimodality</title>
      <link>https://paperswithcode.com/paper/generative-pretraining-in-multimodality</link>
      <description><![CDATA[We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-pretraining-in-multimodality</guid>
    </item>
    <item>
      <title>Secrets of RLHF in Large Language Models Part I: PPO</title>
      <link>https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</link>
      <description><![CDATA[Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</guid>
    </item>
    <item>
      <title>Semantic-SAM: Segment and Recognize Anything at Any Granularity</title>
      <link>https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</link>
      <description><![CDATA[In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>Focused Transformer: Contrastive Training for Context Scaling</title>
      <link>https://paperswithcode.com/paper/focused-transformer-contrastive-training-for</link>
      <description><![CDATA[This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/focused-transformer-contrastive-training-for</guid>
    </item>
    <item>
      <title>FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing</title>
      <link>https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</link>
      <description><![CDATA[To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</guid>
    </item>
    <item>
      <title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
      <link>https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</link>
      <description><![CDATA[We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</guid>
    </item>
    <item>
      <title>A Survey of Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-large-language-models</link>
      <description><![CDATA[To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-large-language-models</guid>
    </item>
    <item>
      <title>Editing Large Language Models: Problems, Methods, and Opportunities</title>
      <link>https://paperswithcode.com/paper/editing-large-language-models-problems</link>
      <description><![CDATA[Recent advancements in deep learning have precipitated the emergence of large language models (LLMs) which exhibit an impressive aptitude for understanding and producing text akin to human language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/editing-large-language-models-problems</guid>
    </item>
    <item>
      <title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</title>
      <link>https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</link>
      <description><![CDATA[Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</guid>
    </item>
    <item>
      <title>LightGlue: Local Feature Matching at Light Speed</title>
      <link>https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</link>
      <description><![CDATA[We introduce LightGlue, a deep neural network that learns to match local features across images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</guid>
    </item>
    <item>
      <title>ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases</title>
      <link>https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</link>
      <description><![CDATA[Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</guid>
    </item>
    <item>
      <title>MONAI: An open-source framework for deep learning in healthcare</title>
      <link>https://paperswithcode.com/paper/monai-an-open-source-framework-for-deep</link>
      <description><![CDATA[For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e. g. geometry, physiology, physics) of medical data being processed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monai-an-open-source-framework-for-deep</guid>
    </item>
    <item>
      <title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</title>
      <link>https://paperswithcode.com/paper/dragdiffusion-harnessing-diffusion-models-for</link>
      <description><![CDATA[In this work, we extend such an editing framework to diffusion models and propose DragDiffusion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dragdiffusion-harnessing-diffusion-models-for</guid>
    </item>
    <item>
      <title>VampNet: Music Generation via Masked Acoustic Token Modeling</title>
      <link>https://paperswithcode.com/paper/vampnet-music-generation-via-masked-acoustic</link>
      <description><![CDATA[We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vampnet-music-generation-via-masked-acoustic</guid>
    </item>
    <item>
      <title>MMBench: Is Your Multi-modal Model an All-around Player?</title>
      <link>https://paperswithcode.com/paper/mmbench-is-your-multi-modal-model-an-all</link>
      <description><![CDATA[In response to these challenges, we propose MMBench, a novel multi-modality benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmbench-is-your-multi-modal-model-an-all</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
      <link>https://paperswithcode.com/paper/retrieval-augmented-generation-for-knowledge</link>
      <description><![CDATA[Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-augmented-generation-for-knowledge</guid>
    </item>
    <item>
      <title>A Survey on Evaluation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-survey-on-evaluation-of-large-language</link>
      <description><![CDATA[Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-evaluation-of-large-language</guid>
    </item>
  </channel>
</rss>
