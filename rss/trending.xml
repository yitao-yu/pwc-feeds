<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 20 Jun 2023 09:11:42 +0000</lastBuildDate>
    <item>
      <title>FinGPT: Open-Source Financial Large Language Models</title>
      <link>https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</link>
      <description><![CDATA[While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</guid>
    </item>
    <item>
      <title>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title>
      <link>https://paperswithcode.com/paper/self-supervised-learning-from-images-with-a</link>
      <description><![CDATA[This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-supervised-learning-from-images-with-a</guid>
    </item>
    <item>
      <title>Augmenting Language Models with Long-Term Memory</title>
      <link>https://paperswithcode.com/paper/augmenting-language-models-with-long-term</link>
      <description><![CDATA[Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/augmenting-language-models-with-long-term</guid>
    </item>
    <item>
      <title>WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
      <link>https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</link>
      <description><![CDATA[We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</guid>
    </item>
    <item>
      <title>Recognize Anything: A Strong Image Tagging Model</title>
      <link>https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</link>
      <description><![CDATA[We are releasing the RAM at \url{https://recognize-anything. github. io/} to foster the advancements of large models in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</guid>
    </item>
    <item>
      <title>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</title>
      <link>https://paperswithcode.com/paper/wizardcoder-empowering-code-large-language</link>
      <description><![CDATA[Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardcoder-empowering-code-large-language</guid>
    </item>
    <item>
      <title>SqueezeLLM: Dense-and-Sparse Quantization</title>
      <link>https://paperswithcode.com/paper/squeezellm-dense-and-sparse-quantization</link>
      <description><![CDATA[When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2. 1x as compared to the state-of-the-art methods with the same memory requirement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/squeezellm-dense-and-sparse-quantization</guid>
    </item>
    <item>
      <title>Segment Any Point Cloud Sequences by Distilling Vision Foundation Models</title>
      <link>https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by</link>
      <description><![CDATA[Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by</guid>
    </item>
    <item>
      <title>Segment Anything in High Quality</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-high-quality</link>
      <description><![CDATA[HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-high-quality</guid>
    </item>
    <item>
      <title>Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</title>
      <link>https://paperswithcode.com/paper/macaw-llm-multi-modal-language-modeling-with</link>
      <description><![CDATA[Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/macaw-llm-multi-modal-language-modeling-with</guid>
    </item>
    <item>
      <title>DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</title>
      <link>https://paperswithcode.com/paper/dreamsim-learning-new-dimensions-of-human</link>
      <description><![CDATA[Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamsim-learning-new-dimensions-of-human</guid>
    </item>
    <item>
      <title>High-Fidelity Audio Compression with Improved RVQGAN</title>
      <link>https://paperswithcode.com/paper/high-fidelity-audio-compression-with-improved</link>
      <description><![CDATA[Language models have been successfully used to model natural signals, such as images, speech, and music.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-audio-compression-with-improved</guid>
    </item>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>FasterViT: Fast Vision Transformers with Hierarchical Attention</title>
      <link>https://paperswithcode.com/paper/fastervit-fast-vision-transformers-with</link>
      <description><![CDATA[At a high level, global self-attentions enable the efficient cross-window communication at lower costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastervit-fast-vision-transformers-with</guid>
    </item>
    <item>
      <title>Reasoning Implicit Sentiment with Chain-of-Thought Prompting</title>
      <link>https://paperswithcode.com/paper/reasoning-implicit-sentiment-with-chain-of</link>
      <description><![CDATA[While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reasoning-implicit-sentiment-with-chain-of</guid>
    </item>
    <item>
      <title>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
      <link>https://paperswithcode.com/paper/how-far-can-camels-go-exploring-the-state-of</link>
      <description><![CDATA[Our evaluations show that the best model in any given evaluation reaches on average 83% of ChatGPT performance, and 68% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-can-camels-go-exploring-the-state-of</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>TAP-Vid: A Benchmark for Tracking Any Point in a Video</title>
      <link>https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</link>
      <description><![CDATA[Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</guid>
    </item>
    <item>
      <title>Recurrent Vision Transformers for Object Detection with Event Cameras</title>
      <link>https://paperswithcode.com/paper/recurrent-vision-transformers-for-object</link>
      <description><![CDATA[By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/recurrent-vision-transformers-for-object</guid>
    </item>
    <item>
      <title>TART: A plug-and-play Transformer module for task-agnostic reasoning</title>
      <link>https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</link>
      <description><![CDATA[As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</guid>
    </item>
  </channel>
</rss>
