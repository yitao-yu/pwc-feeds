<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 08 May 2023 21:06:13 +0000</lastBuildDate>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection</title>
      <link>https://paperswithcode.com/paper/u-2-net-going-deeper-with-nested-u-structure</link>
      <description><![CDATA[In this paper, we design a simple yet powerful deep network architecture, U$^2$-Net, for salient object detection (SOD).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/u-2-net-going-deeper-with-nested-u-structure</guid>
    </item>
    <item>
      <title>PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
      <link>https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</link>
      <description><![CDATA[Real-world applications have high demands for semantic segmentation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</guid>
    </item>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models</title>
      <link>https://paperswithcode.com/paper/panda-llm-training-data-and-evaluation-for</link>
      <description><![CDATA[This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/panda-llm-training-data-and-evaluation-for</guid>
    </item>
    <item>
      <title>Unlimiformer: Long-Range Transformers with Unlimited Length Input</title>
      <link>https://paperswithcode.com/paper/unlimiformer-long-range-transformers-with</link>
      <description><![CDATA[This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unlimiformer-long-range-transformers-with</guid>
    </item>
    <item>
      <title>Personalize Segment Anything Model with One Shot</title>
      <link>https://paperswithcode.com/paper/personalize-segment-anything-model-with-one</link>
      <description><![CDATA[Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/personalize-segment-anything-model-with-one</guid>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</link>
      <description><![CDATA[We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</guid>
    </item>
    <item>
      <title>A Survey of Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-large-language-models</link>
      <description><![CDATA[To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-large-language-models</guid>
    </item>
    <item>
      <title>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</title>
      <link>https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of</link>
      <description><![CDATA[We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</title>
      <link>https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</link>
      <description><![CDATA[Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github. com/X-PLUG/mPLUG-Owl.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</guid>
    </item>
    <item>
      <title>Track Anything: Segment Anything Meets Videos</title>
      <link>https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</link>
      <description><![CDATA[Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</guid>
    </item>
    <item>
      <title>Learnable latent embeddings for joint behavioral and neural analysis</title>
      <link>https://paperswithcode.com/paper/learnable-latent-embeddings-for-joint</link>
      <description><![CDATA[Mapping behavioral actions to neural activity is a fundamental goal of neuroscience.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learnable-latent-embeddings-for-joint</guid>
    </item>
    <item>
      <title>Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/visual-instruction-tuning</link>
      <description><![CDATA[Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-instruction-tuning</guid>
    </item>
    <item>
      <title>CodeGen2: Lessons for Training LLMs on Programming and Natural Languages</title>
      <link>https://paperswithcode.com/paper/codegen2-lessons-for-training-llms-on</link>
      <description><![CDATA[In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codegen2-lessons-for-training-llms-on</guid>
    </item>
    <item>
      <title>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</title>
      <link>https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</link>
      <description><![CDATA[In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i. e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</guid>
    </item>
    <item>
      <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
      <link>https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</link>
      <description><![CDATA[This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</guid>
    </item>
    <item>
      <title>In-Context Learning Unlocked for Diffusion Models</title>
      <link>https://paperswithcode.com/paper/in-context-learning-unlocked-for-diffusion</link>
      <description><![CDATA[To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-unlocked-for-diffusion</guid>
    </item>
    <item>
      <title>ZipIt! Merging Models from Different Tasks without Training</title>
      <link>https://paperswithcode.com/paper/zipit-merging-models-from-different-tasks</link>
      <description><![CDATA[While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zipit-merging-models-from-different-tasks</guid>
    </item>
  </channel>
</rss>
