<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 03 May 2023 21:06:13 +0000</lastBuildDate>
    <item>
      <title>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</title>
      <link>https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</link>
      <description><![CDATA[In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i. e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>Track Anything: Segment Anything Meets Videos</title>
      <link>https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</link>
      <description><![CDATA[Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</guid>
    </item>
    <item>
      <title>WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
      <link>https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</link>
      <description><![CDATA[By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</guid>
    </item>
    <item>
      <title>VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</title>
      <link>https://paperswithcode.com/paper/videoretalking-audio-based-lip</link>
      <description><![CDATA[Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videoretalking-audio-based-lip</guid>
    </item>
    <item>
      <title>Segment Everything Everywhere All at Once</title>
      <link>https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</link>
      <description><![CDATA[Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</guid>
    </item>
    <item>
      <title>Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model</title>
      <link>https://paperswithcode.com/paper/text-to-audio-generation-using-instruction</link>
      <description><![CDATA[The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-to-audio-generation-using-instruction</guid>
    </item>
    <item>
      <title>LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions</title>
      <link>https://paperswithcode.com/paper/lamini-lm-a-diverse-herd-of-distilled-models</link>
      <description><![CDATA[To this end, we carefully develop a large set of 2. 58M instructions based on both existing and newly-generated instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lamini-lm-a-diverse-herd-of-distilled-models</guid>
    </item>
    <item>
      <title>DataComp: In search of the next generation of multimodal datasets</title>
      <link>https://paperswithcode.com/paper/datacomp-in-search-of-the-next-generation-of</link>
      <description><![CDATA[To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datacomp-in-search-of-the-next-generation-of</guid>
    </item>
    <item>
      <title>Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs</title>
      <link>https://paperswithcode.com/paper/hidet-task-mapping-programming-paradigm-for</link>
      <description><![CDATA[With the proposed paradigm, we implement a deep learning compiler Hidet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hidet-task-mapping-programming-paradigm-for</guid>
    </item>
    <item>
      <title>PMC-LLaMA: Further Finetuning LLaMA on Medical Papers</title>
      <link>https://paperswithcode.com/paper/pmc-llama-further-finetuning-llama-on-medical</link>
      <description><![CDATA[Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding in various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pmc-llama-further-finetuning-llama-on-medical</guid>
    </item>
    <item>
      <title>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</title>
      <link>https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</link>
      <description><![CDATA[Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github. com/X-PLUG/mPLUG-Owl.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</guid>
    </item>
    <item>
      <title>Tool Learning with Foundation Models</title>
      <link>https://paperswithcode.com/paper/tool-learning-with-foundation-models</link>
      <description><![CDATA[Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tool-learning-with-foundation-models</guid>
    </item>
    <item>
      <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
      <link>https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</link>
      <description><![CDATA[This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</guid>
    </item>
    <item>
      <title>DINOv2: Learning Robust Visual Features without Supervision</title>
      <link>https://paperswithcode.com/paper/dinov2-learning-robust-visual-features</link>
      <description><![CDATA[The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dinov2-learning-robust-visual-features</guid>
    </item>
    <item>
      <title>Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</title>
      <link>https://paperswithcode.com/paper/make-it-3d-high-fidelity-3d-creation-from-a</link>
      <description><![CDATA[In this work, we investigate the problem of creating high-fidelity 3D content from only a single image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/make-it-3d-high-fidelity-3d-creation-from-a</guid>
    </item>
    <item>
      <title>JaxPruner: A concise library for sparsity research</title>
      <link>https://paperswithcode.com/paper/jaxpruner-a-concise-library-for-sparsity</link>
      <description><![CDATA[This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/jaxpruner-a-concise-library-for-sparsity</guid>
    </item>
    <item>
      <title>Segment Anything in Medical Images</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-medical-images</link>
      <description><![CDATA[Comprehensive experiments on 21 3D segmentation tasks and 9 2D segmentation tasks demonstrate that MedSAM outperforms the default SAM model with an average Dice Similarity Coefficient (DSC) of 22. 5% and 17. 6% on 3D and 2D segmentation tasks, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-medical-images</guid>
    </item>
    <item>
      <title>Inpaint Anything: Segment Anything Meets Image Inpainting</title>
      <link>https://paperswithcode.com/paper/inpaint-anything-segment-anything-meets-image</link>
      <description><![CDATA[We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/inpaint-anything-segment-anything-meets-image</guid>
    </item>
    <item>
      <title>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency</title>
      <link>https://paperswithcode.com/paper/llm-p-empowering-large-language-models-with</link>
      <description><![CDATA[LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm-p-empowering-large-language-models-with</guid>
    </item>
  </channel>
</rss>
