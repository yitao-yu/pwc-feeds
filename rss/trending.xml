<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 11 Jun 2023 09:10:49 +0000</lastBuildDate>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>Segment Anything in High Quality</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-high-quality</link>
      <description><![CDATA[HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-high-quality</guid>
    </item>
    <item>
      <title>DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement</title>
      <link>https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</link>
      <description><![CDATA[Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</guid>
    </item>
    <item>
      <title>CodeTF: One-stop Transformer Library for State-of-the-art Code LLM</title>
      <link>https://paperswithcode.com/paper/codetf-one-stop-transformer-library-for-state</link>
      <description><![CDATA[In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codetf-one-stop-transformer-library-for-state</guid>
    </item>
    <item>
      <title>Matting Anything</title>
      <link>https://paperswithcode.com/paper/matting-anything</link>
      <description><![CDATA[In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matting-anything</guid>
    </item>
    <item>
      <title>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</title>
      <link>https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</link>
      <description><![CDATA[Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</guid>
    </item>
    <item>
      <title>A Literature Study of Embeddings on Source Code</title>
      <link>https://paperswithcode.com/paper/a-literature-study-of-embeddings-on-source</link>
      <description><![CDATA[In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-literature-study-of-embeddings-on-source</guid>
    </item>
    <item>
      <title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
      <link>https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</link>
      <description><![CDATA[For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</guid>
    </item>
    <item>
      <title>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</title>
      <link>https://paperswithcode.com/paper/rewoo-decoupling-reasoning-from-observations</link>
      <description><![CDATA[Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rewoo-decoupling-reasoning-from-observations</guid>
    </item>
    <item>
      <title>Fine-Tuning Language Models with Just Forward Passes</title>
      <link>https://paperswithcode.com/paper/fine-tuning-language-models-with-just-forward</link>
      <description><![CDATA[Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-tuning-language-models-with-just-forward</guid>
    </item>
    <item>
      <title>White-Box Transformers via Sparse Rate Reduction</title>
      <link>https://paperswithcode.com/paper/white-box-transformers-via-sparse-rate</link>
      <description><![CDATA[Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/white-box-transformers-via-sparse-rate</guid>
    </item>
    <item>
      <title>Gorilla: Large Language Model Connected with Massive APIs</title>
      <link>https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</link>
      <description><![CDATA[Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</guid>
    </item>
    <item>
      <title>AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</title>
      <link>https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</link>
      <description><![CDATA[In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</guid>
    </item>
    <item>
      <title>Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</title>
      <link>https://paperswithcode.com/paper/youku-mplug-a-10-million-large-scale-chinese</link>
      <description><![CDATA[In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/youku-mplug-a-10-million-large-scale-chinese</guid>
    </item>
    <item>
      <title>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</title>
      <link>https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</link>
      <description><![CDATA[Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</guid>
    </item>
    <item>
      <title>Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models</title>
      <link>https://paperswithcode.com/paper/git-theta-a-git-extension-for-collaborative</link>
      <description><![CDATA[Currently, most machine learning models are trained by centralized teams and are rarely updated.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/git-theta-a-git-extension-for-collaborative</guid>
    </item>
    <item>
      <title>InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</title>
      <link>https://paperswithcode.com/paper/instructzero-efficient-instruction</link>
      <description><![CDATA[Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructzero-efficient-instruction</guid>
    </item>
    <item>
      <title>Designing a Better Asymmetric VQGAN for StableDiffusion</title>
      <link>https://paperswithcode.com/paper/designing-a-better-asymmetric-vqgan-for</link>
      <description><![CDATA[The training cost of our asymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion unchanged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/designing-a-better-asymmetric-vqgan-for</guid>
    </item>
    <item>
      <title>SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model</title>
      <link>https://paperswithcode.com/paper/sam3d-zero-shot-3d-object-detection-via</link>
      <description><![CDATA[In the spirit of unleashing the capability of foundation models on vision tasks, the Segment Anything Model (SAM), a vision foundation model for image segmentation, has been proposed recently and presents strong zero-shot ability on many downstream 2D tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sam3d-zero-shot-3d-object-detection-via</guid>
    </item>
    <item>
      <title>Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models</title>
      <link>https://paperswithcode.com/paper/unsupervised-representation-learning-from-pre</link>
      <description><![CDATA[These imply that the gap corresponds to the lost information of the image, and we can reconstruct the image by filling the gap.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-representation-learning-from-pre</guid>
    </item>
  </channel>
</rss>
