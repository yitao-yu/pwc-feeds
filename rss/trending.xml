<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 06 Jul 2023 21:07:19 +0000</lastBuildDate>
    <item>
      <title>ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases</title>
      <link>https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</link>
      <description><![CDATA[Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</guid>
    </item>
    <item>
      <title>DisCo: Disentangled Control for Referring Human Dance Generation in Real World</title>
      <link>https://paperswithcode.com/paper/disco-disentangled-control-for-referring</link>
      <description><![CDATA[Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/disco-disentangled-control-for-referring</guid>
    </item>
    <item>
      <title>StyleDrop: Text-to-Image Generation in Any Style</title>
      <link>https://paperswithcode.com/paper/styledrop-text-to-image-generation-in-any</link>
      <description><![CDATA[Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/styledrop-text-to-image-generation-in-any</guid>
    </item>
    <item>
      <title>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for</link>
      <description><![CDATA[Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for</guid>
    </item>
    <item>
      <title>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset</title>
      <link>https://paperswithcode.com/paper/motion-x-a-large-scale-3d-expressive-whole</link>
      <description><![CDATA[In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/motion-x-a-large-scale-3d-expressive-whole</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>DreamDiffusion: Generating High-Quality Images from Brain EEG Signals</title>
      <link>https://paperswithcode.com/paper/dreamdiffusion-generating-high-quality-images</link>
      <description><![CDATA[This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamdiffusion-generating-high-quality-images</guid>
    </item>
    <item>
      <title>Faster Segment Anything: Towards Lightweight SAM for Mobile Applications</title>
      <link>https://paperswithcode.com/paper/faster-segment-anything-towards-lightweight</link>
      <description><![CDATA[Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/faster-segment-anything-towards-lightweight</guid>
    </item>
    <item>
      <title>End-to-end Autonomous Driving: Challenges and Frontiers</title>
      <link>https://paperswithcode.com/paper/end-to-end-autonomous-driving-challenges-and</link>
      <description><![CDATA[The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/end-to-end-autonomous-driving-challenges-and</guid>
    </item>
    <item>
      <title>Fast Segment Anything</title>
      <link>https://paperswithcode.com/paper/fast-segment-anything</link>
      <description><![CDATA[In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-segment-anything</guid>
    </item>
    <item>
      <title>Segment Anything Meets Point Tracking</title>
      <link>https://paperswithcode.com/paper/segment-anything-meets-point-tracking</link>
      <description><![CDATA[SAM-PT leverages robust and sparse point selection and propagation techniques for mask generation, demonstrating that a SAM-based segmentation tracker can yield strong zero-shot performance across popular video object segmentation benchmarks, including DAVIS, YouTube-VOS, and MOSE.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-meets-point-tracking</guid>
    </item>
    <item>
      <title>A Survey of Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-large-language-models</link>
      <description><![CDATA[To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-large-language-models</guid>
    </item>
    <item>
      <title>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</title>
      <link>https://paperswithcode.com/paper/shikra-unleashing-multimodal-llm-s</link>
      <description><![CDATA[Referential dialogue is a superset of various vision-language (VL) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shikra-unleashing-multimodal-llm-s</guid>
    </item>
    <item>
      <title>DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models</title>
      <link>https://paperswithcode.com/paper/dragondiffusion-enabling-drag-style</link>
      <description><![CDATA[Specifically, we construct classifier guidance based on the strong correspondence of intermediate features in the diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dragondiffusion-enabling-drag-style</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</title>
      <link>https://paperswithcode.com/paper/enhancing-chat-language-models-by-scaling</link>
      <description><![CDATA[Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/enhancing-chat-language-models-by-scaling</guid>
    </item>
    <item>
      <title>Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</title>
      <link>https://paperswithcode.com/paper/drag-your-gan-interactive-point-based</link>
      <description><![CDATA[Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/drag-your-gan-interactive-point-based</guid>
    </item>
    <item>
      <title>Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language</title>
      <link>https://paperswithcode.com/paper/towards-language-models-that-can-see-computer</link>
      <description><![CDATA[We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-language-models-that-can-see-computer</guid>
    </item>
    <item>
      <title>LightGlue: Local Feature Matching at Light Speed</title>
      <link>https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</link>
      <description><![CDATA[We introduce LightGlue, a deep neural network that learns to match local features across images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</guid>
    </item>
    <item>
      <title>Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors</title>
      <link>https://paperswithcode.com/paper/magic123-one-image-to-high-quality-3d-object</link>
      <description><![CDATA[We present Magic123, a two-stage coarse-to-fine approach for high-quality, textured 3D meshes generation from a single unposed image in the wild using both2D and 3D priors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magic123-one-image-to-high-quality-3d-object</guid>
    </item>
  </channel>
</rss>
