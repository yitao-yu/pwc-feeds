<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 05 Aug 2022 21:07:22 +0000</lastBuildDate>
    <item>
      <title>MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training</title>
      <link>https://paperswithcode.com/paper/minvis-a-minimal-video-instance-segmentation</link>
      <description><![CDATA[By only training a query-based image instance segmentation model, MinVIS outperforms the previous best result on the challenging Occluded VIS dataset by over 10% AP.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minvis-a-minimal-video-instance-segmentation</guid>
    </item>
    <item>
      <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
      <link>https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</link>
      <description><![CDATA[This paper deals with the problem of audio source separation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</guid>
    </item>
    <item>
      <title>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</title>
      <link>https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</link>
      <description><![CDATA[Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</guid>
    </item>
    <item>
      <title>GAUDI: A Neural Architect for Immersive 3D Scene Generation</title>
      <link>https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</link>
      <description><![CDATA[We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</guid>
    </item>
    <item>
      <title>Neural Density-Distance Fields</title>
      <link>https://paperswithcode.com/paper/neural-density-distance-fields</link>
      <description><![CDATA[However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-density-distance-fields</guid>
    </item>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>YOLO-FaceV2: A Scale and Occlusion Aware Face Detector</title>
      <link>https://paperswithcode.com/paper/yolo-facev2-a-scale-and-occlusion-aware-face</link>
      <description><![CDATA[In this paper, we propose a real-time face detector based on the one-stage detector YOLOv5, named YOLO-FaceV2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-facev2-a-scale-and-occlusion-aware-face</guid>
    </item>
    <item>
      <title>Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer</title>
      <link>https://paperswithcode.com/paper/safety-enhanced-autonomous-driving-using-1</link>
      <description><![CDATA[Large-scale deployment of autonomous vehicles has been continually delayed due to safety concerns.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/safety-enhanced-autonomous-driving-using-1</guid>
    </item>
    <item>
      <title>A Conversational Paradigm for Program Synthesis</title>
      <link>https://paperswithcode.com/paper/a-conversational-paradigm-for-program</link>
      <description><![CDATA[We train a family of large language models, called CodeGen, on natural language and programming language data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-conversational-paradigm-for-program</guid>
    </item>
    <item>
      <title>Text2LIVE: Text-Driven Layered Image and Video Editing</title>
      <link>https://paperswithcode.com/paper/text2live-text-driven-layered-image-and-video</link>
      <description><![CDATA[Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e. g., object's texture) or augment the scene with visual effects (e. g., smoke, fire) in a semantically meaningful manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2live-text-driven-layered-image-and-video</guid>
    </item>
    <item>
      <title>Theseus: A Library for Differentiable Nonlinear Optimization</title>
      <link>https://paperswithcode.com/paper/theseus-a-library-for-differentiable</link>
      <description><![CDATA[We present Theseus, an efficient application-agnostic open source library for differentiable nonlinear least squares (DNLS) optimization built on PyTorch, providing a common framework for end-to-end structured learning in robotics and vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/theseus-a-library-for-differentiable</guid>
    </item>
    <item>
      <title>OCR-free Document Understanding Transformer</title>
      <link>https://paperswithcode.com/paper/donut-document-understanding-transformer</link>
      <description><![CDATA[Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/donut-document-understanding-transformer</guid>
    </item>
    <item>
      <title>REALY: Rethinking the Evaluation of 3D Face Reconstruction</title>
      <link>https://paperswithcode.com/paper/realy-rethinking-the-evaluation-of-3d-face</link>
      <description><![CDATA[The evaluation of 3D face reconstruction results typically relies on a rigid shape alignment between the estimated 3D model and the ground-truth scan.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/realy-rethinking-the-evaluation-of-3d-face</guid>
    </item>
    <item>
      <title>Towards Real-World Blind Face Restoration with Generative Facial Prior</title>
      <link>https://paperswithcode.com/paper/towards-real-world-blind-face-restoration</link>
      <description><![CDATA[Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-real-world-blind-face-restoration</guid>
    </item>
    <item>
      <title>Per-Clip Video Object Segmentation</title>
      <link>https://paperswithcode.com/paper/per-clip-video-object-segmentation-1</link>
      <description><![CDATA[In this per-clip inference scheme, we update the memory with an interval and simultaneously process a set of consecutive frames (i. e. clip) between the memory updates.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/per-clip-video-object-segmentation-1</guid>
    </item>
    <item>
      <title>Accurate Polygonal Mapping of Buildings in Satellite Imagery</title>
      <link>https://paperswithcode.com/paper/accurate-polygonal-mapping-of-buildings-in</link>
      <description><![CDATA[We addressed such an issue by exploiting the hierarchical supervision (of bottom-level vertices, mid-level line segments and the high-level regional masks) and proposed a novel interaction mechanism of feature embedding sourced from different levels of supervision signals to obtain reversible building masks for polygonal mapping of buildings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/accurate-polygonal-mapping-of-buildings-in</guid>
    </item>
    <item>
      <title>The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large Web Corpus</title>
      <link>https://paperswithcode.com/paper/the-web-is-your-oyster-knowledge-intensive</link>
      <description><![CDATA[In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-web-is-your-oyster-knowledge-intensive</guid>
    </item>
    <item>
      <title>DETRs with Hybrid Matching</title>
      <link>https://paperswithcode.com/paper/detrs-with-hybrid-matching</link>
      <description><![CDATA[This end-to-end signature is important for the versatility of DETR, and it has been generalized to a wide range of visual problems, including instance/semantic segmentation, human pose estimation, and point cloud/multi-view-images based detection, etc.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/detrs-with-hybrid-matching</guid>
    </item>
    <item>
      <title>NAFSSR: Stereo Image Super-Resolution Using NAFNet</title>
      <link>https://paperswithcode.com/paper/nafssr-stereo-image-super-resolution-using</link>
      <description><![CDATA[This paper inherits a strong and simple image restoration model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nafssr-stereo-image-super-resolution-using</guid>
    </item>
    <item>
      <title>DCT-Net: Domain-Calibrated Translation for Portrait Stylization</title>
      <link>https://paperswithcode.com/paper/dct-net-domain-calibrated-translation-for</link>
      <description><![CDATA[This paper introduces DCT-Net, a novel image translation architecture for few-shot portrait stylization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dct-net-domain-calibrated-translation-for</guid>
    </item>
  </channel>
</rss>
