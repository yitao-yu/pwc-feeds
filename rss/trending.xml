<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 15 May 2025 21:10:22 +0000</lastBuildDate>
    <item>
      <title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
      <link>https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</link>
      <description><![CDATA[At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</guid>
    </item>
    <item>
      <title>Continuous Thought Machines</title>
      <link>https://paperswithcode.com/paper/continuous-thought-machines</link>
      <description><![CDATA[The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/continuous-thought-machines</guid>
    </item>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</link>
      <description><![CDATA[Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</guid>
    </item>
    <item>
      <title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
      <link>https://paperswithcode.com/paper/generating-physically-stable-and-buildable</link>
      <description><![CDATA[Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generating-physically-stable-and-buildable</guid>
    </item>
    <item>
      <title>Flow-GRPO: Training Flow Matching Models via Online RL</title>
      <link>https://paperswithcode.com/paper/flow-grpo-training-flow-matching-models-via</link>
      <description><![CDATA[We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flow-grpo-training-flow-matching-models-via</guid>
    </item>
    <item>
      <title>LTX-Video: Realtime Video Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</link>
      <description><![CDATA[To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</guid>
    </item>
    <item>
      <title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
      <link>https://paperswithcode.com/paper/perception-reason-think-and-plan-a-survey-on</link>
      <description><![CDATA[Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/perception-reason-think-and-plan-a-survey-on</guid>
    </item>
    <item>
      <title>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</title>
      <link>https://paperswithcode.com/paper/univla-learning-to-act-anywhere-with-task</link>
      <description><![CDATA[Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/univla-learning-to-act-anywhere-with-task</guid>
    </item>
    <item>
      <title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
      <link>https://paperswithcode.com/paper/openhelix-a-short-survey-empirical-analysis</link>
      <description><![CDATA[Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openhelix-a-short-survey-empirical-analysis</guid>
    </item>
    <item>
      <title>Unified Continuous Generative Models</title>
      <link>https://paperswithcode.com/paper/unified-continuous-generative-models</link>
      <description><![CDATA[We introduce a unified framework for training, sampling, and analyzing these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unified-continuous-generative-models</guid>
    </item>
    <item>
      <title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
      <link>https://paperswithcode.com/paper/paper2code-automating-code-generation-from</link>
      <description><![CDATA[Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paper2code-automating-code-generation-from</guid>
    </item>
    <item>
      <title>3D Scene Generation: A Survey</title>
      <link>https://paperswithcode.com/paper/3d-scene-generation-a-survey</link>
      <description><![CDATA[Recent advances in deep generative models (e. g., GANs, diffusion models) and 3D representations (e. g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-scene-generation-a-survey</guid>
    </item>
    <item>
      <title>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</title>
      <link>https://paperswithcode.com/paper/nexus-gen-a-unified-model-for-image</link>
      <description><![CDATA[To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nexus-gen-a-unified-model-for-image</guid>
    </item>
    <item>
      <title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
      <link>https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</link>
      <description><![CDATA[Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</guid>
    </item>
    <item>
      <title>Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</title>
      <link>https://paperswithcode.com/paper/unified-multimodal-understanding-and</link>
      <description><![CDATA[Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unified-multimodal-understanding-and</guid>
    </item>
    <item>
      <title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
      <link>https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</link>
      <description><![CDATA[Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</guid>
    </item>
    <item>
      <title>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</title>
      <link>https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</link>
      <description><![CDATA[Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</guid>
    </item>
    <item>
      <title>Deformable Beta Splatting</title>
      <link>https://paperswithcode.com/paper/deformable-beta-splatting</link>
      <description><![CDATA[Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1. 5x faster than 3DGS-MCMC, highlighting the superior performance of DBS for real-time radiance field rendering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deformable-beta-splatting</guid>
    </item>
    <item>
      <title>ZClip: Adaptive Spike Mitigation for LLM Pre-Training</title>
      <link>https://paperswithcode.com/paper/zclip-adaptive-spike-mitigation-for-llm-pre</link>
      <description><![CDATA[Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zclip-adaptive-spike-mitigation-for-llm-pre</guid>
    </item>
    <item>
      <title>Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</title>
      <link>https://paperswithcode.com/paper/voila-voice-language-foundation-models-for</link>
      <description><![CDATA[A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voila-voice-language-foundation-models-for</guid>
    </item>
  </channel>
</rss>
