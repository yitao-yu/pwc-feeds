<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 25 Jun 2024 09:14:22 +0000</lastBuildDate>
    <item>
      <title>Depth Anything V2</title>
      <link>https://paperswithcode.com/paper/depth-anything-v2</link>
      <description><![CDATA[This work presents Depth Anything V2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-v2</guid>
    </item>
    <item>
      <title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title>
      <link>https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</link>
      <description><![CDATA[Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</guid>
    </item>
    <item>
      <title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title>
      <link>https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</link>
      <description><![CDATA[In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</guid>
    </item>
    <item>
      <title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</link>
      <description><![CDATA[Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</guid>
    </item>
    <item>
      <title>StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning</title>
      <link>https://paperswithcode.com/paper/streamspeech-simultaneous-speech-to-speech</link>
      <description><![CDATA[Simultaneous speech-to-speech translation (Simul-S2ST, a. k. a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamspeech-simultaneous-speech-to-speech</guid>
    </item>
    <item>
      <title>Meta Learning Text-to-Speech Synthesis in over 7000 Languages</title>
      <link>https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</link>
      <description><![CDATA[In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</guid>
    </item>
    <item>
      <title>ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code</title>
      <link>https://paperswithcode.com/paper/ml-bench-large-language-models-leverage-open</link>
      <description><![CDATA[Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e. g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ml-bench-large-language-models-leverage-open</guid>
    </item>
    <item>
      <title>Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?</title>
      <link>https://paperswithcode.com/paper/can-long-context-language-models-subsume</link>
      <description><![CDATA[Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-long-context-language-models-subsume</guid>
    </item>
    <item>
      <title>TextGrad: Automatic "Differentiation" via Text</title>
      <link>https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</link>
      <description><![CDATA[Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</guid>
    </item>
    <item>
      <title>Structure-Aware Sparse-View X-ray 3D Reconstruction</title>
      <link>https://paperswithcode.com/paper/structure-aware-sparse-view-x-ray-3d</link>
      <description><![CDATA[In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structure-aware-sparse-view-x-ray-3d</guid>
    </item>
    <item>
      <title>CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making</title>
      <link>https://paperswithcode.com/paper/cleandiffuser-an-easy-to-use-modularized</link>
      <description><![CDATA[By revisiting the roles of DMs in the decision-making domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cleandiffuser-an-easy-to-use-modularized</guid>
    </item>
    <item>
      <title>Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title>
      <link>https://paperswithcode.com/paper/accessing-gpt-4-level-mathematical-olympiad</link>
      <description><![CDATA[This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/accessing-gpt-4-level-mathematical-olympiad</guid>
    </item>
    <item>
      <title>A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-multimodal-guided-image-editing</link>
      <description><![CDATA[Image editing aims to edit the given synthetic or real image to meet the specific requirements from users.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-multimodal-guided-image-editing</guid>
    </item>
    <item>
      <title>Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</link>
      <description><![CDATA[Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</guid>
    </item>
    <item>
      <title>MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens</title>
      <link>https://paperswithcode.com/paper/mint-1t-scaling-open-source-multimodal-data</link>
      <description><![CDATA[Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mint-1t-scaling-open-source-multimodal-data</guid>
    </item>
    <item>
      <title>Proactive Detection of Voice Cloning with Localized Watermarking</title>
      <link>https://paperswithcode.com/paper/proactive-detection-of-voice-cloning-with</link>
      <description><![CDATA[In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/proactive-detection-of-voice-cloning-with</guid>
    </item>
    <item>
      <title>TroL: Traversal of Layers for Large Language and Vision Models</title>
      <link>https://paperswithcode.com/paper/trol-traversal-of-layers-for-large-language</link>
      <description><![CDATA[Large language and vision models (LLVMs) have been driven by the generalization power of large language models (LLMs) and the advent of visual instruction tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/trol-traversal-of-layers-for-large-language</guid>
    </item>
    <item>
      <title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title>
      <link>https://paperswithcode.com/paper/videollama-2-advancing-spatial-temporal</link>
      <description><![CDATA[In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videollama-2-advancing-spatial-temporal</guid>
    </item>
    <item>
      <title>Matching Anything by Segmenting Anything</title>
      <link>https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</link>
      <description><![CDATA[The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</guid>
    </item>
    <item>
      <title>Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/bench2drive-towards-multi-ability</link>
      <description><![CDATA[In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bench2drive-towards-multi-ability</guid>
    </item>
  </channel>
</rss>
