<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 19 Jul 2023 09:11:59 +0000</lastBuildDate>
    <item>
      <title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title>
      <link>https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</link>
      <description><![CDATA[With the advance of text-to-image models (e. g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</guid>
    </item>
    <item>
      <title>Copy Is All You Need</title>
      <link>https://paperswithcode.com/paper/copy-is-all-you-need</link>
      <description><![CDATA[The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/copy-is-all-you-need</guid>
    </item>
    <item>
      <title>Semantic-SAM: Segment and Recognize Anything at Any Granularity</title>
      <link>https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</link>
      <description><![CDATA[In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</guid>
    </item>
    <item>
      <title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
      <link>https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</link>
      <description><![CDATA[We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</guid>
    </item>
    <item>
      <title>Generative Pretraining in Multimodality</title>
      <link>https://paperswithcode.com/paper/generative-pretraining-in-multimodality</link>
      <description><![CDATA[We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-pretraining-in-multimodality</guid>
    </item>
    <item>
      <title>Secrets of RLHF in Large Language Models Part I: PPO</title>
      <link>https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</link>
      <description><![CDATA[Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</guid>
    </item>
    <item>
      <title>FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing</title>
      <link>https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</link>
      <description><![CDATA[To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>MONAI: An open-source framework for deep learning in healthcare</title>
      <link>https://paperswithcode.com/paper/monai-an-open-source-framework-for-deep</link>
      <description><![CDATA[For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e. g. geometry, physiology, physics) of medical data being processed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monai-an-open-source-framework-for-deep</guid>
    </item>
    <item>
      <title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</title>
      <link>https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</link>
      <description><![CDATA[Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</guid>
    </item>
    <item>
      <title>MedLSAM: Localize and Segment Anything Model for 3D Medical Images</title>
      <link>https://paperswithcode.com/paper/medlsam-localize-and-segment-anything-model</link>
      <description><![CDATA[Nevertheless, both the original SAM and its medical adaptations necessitate slice-by-slice annotations, which directly increase the annotation workload with the size of the dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medlsam-localize-and-segment-anything-model</guid>
    </item>
    <item>
      <title>Zero-1-to-3: Zero-shot One Image to 3D Object</title>
      <link>https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</link>
      <description><![CDATA[We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
      <link>https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</link>
      <description><![CDATA[In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</guid>
    </item>
    <item>
      <title>MMBench: Is Your Multi-modal Model an All-around Player?</title>
      <link>https://paperswithcode.com/paper/mmbench-is-your-multi-modal-model-an-all</link>
      <description><![CDATA[In response to these challenges, we propose MMBench, a novel multi-modality benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmbench-is-your-multi-modal-model-an-all</guid>
    </item>
    <item>
      <title>MIS-FM: 3D Medical Image Segmentation using Foundation Models Pretrained on a Large-Scale Unannotated Dataset</title>
      <link>https://paperswithcode.com/paper/mis-fm-3d-medical-image-segmentation-using</link>
      <description><![CDATA[The proposed model was pretrained with 110k unannotated 3D CT volumes, and experiments with different downstream segmentation targets including head and neck organs, thoracic/abdominal organs showed that our pretrained model largely outperformed training from scratch and several state-of-the-art self-supervised training methods and segmentation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mis-fm-3d-medical-image-segmentation-using</guid>
    </item>
    <item>
      <title>ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases</title>
      <link>https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</link>
      <description><![CDATA[Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</guid>
    </item>
    <item>
      <title>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</title>
      <link>https://paperswithcode.com/paper/animate-a-story-storytelling-with-retrieval</link>
      <description><![CDATA[For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animate-a-story-storytelling-with-retrieval</guid>
    </item>
    <item>
      <title>Vision-Language Models for Vision Tasks: A Survey</title>
      <link>https://paperswithcode.com/paper/vision-language-models-for-vision-tasks-a</link>
      <description><![CDATA[Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-language-models-for-vision-tasks-a</guid>
    </item>
    <item>
      <title>OpenGSL: A Comprehensive Benchmark for Graph Structure Learning</title>
      <link>https://paperswithcode.com/paper/opengsl-a-comprehensive-benchmark-for-graph</link>
      <description><![CDATA[Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opengsl-a-comprehensive-benchmark-for-graph</guid>
    </item>
  </channel>
</rss>
