<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 31 Dec 2024 09:15:30 +0000</lastBuildDate>
    <item>
      <title>DeepSeek-V3 Technical Report</title>
      <link>https://paperswithcode.com/paper/deepseek-v3-technical-report</link>
      <description><![CDATA[We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-v3-technical-report</guid>
    </item>
    <item>
      <title>HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</title>
      <link>https://paperswithcode.com/paper/huatuogpt-o1-towards-medical-complex</link>
      <description><![CDATA[To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuogpt-o1-towards-medical-complex</guid>
    </item>
    <item>
      <title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2409-13731</link>
      <description><![CDATA[The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2409-13731</guid>
    </item>
    <item>
      <title>CogAgent: A Visual Language Model for GUI Agents</title>
      <link>https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui</link>
      <description><![CDATA[People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e. g., computer or smartphone screens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui</guid>
    </item>
    <item>
      <title>PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</title>
      <link>https://paperswithcode.com/paper/pulse-self-supervised-photo-upsampling-via</link>
      <description><![CDATA[We present an algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pulse-self-supervised-photo-upsampling-via</guid>
    </item>
    <item>
      <title>Automating the Search for Artificial Life with Foundation Models</title>
      <link>https://paperswithcode.com/paper/automating-the-search-for-artificial-life</link>
      <description><![CDATA[With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automating-the-search-for-artificial-life</guid>
    </item>
    <item>
      <title>MINIMA: Modality Invariant Image Matching</title>
      <link>https://paperswithcode.com/paper/minima-modality-invariant-image-matching</link>
      <description><![CDATA[Under this setting, the matching labels and rich diversity of the RGB dataset are well inherited by the generated multimodal data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minima-modality-invariant-image-matching</guid>
    </item>
    <item>
      <title>VidTwin: Video VAE with Decoupled Structure and Dynamics</title>
      <link>https://paperswithcode.com/paper/vidtwin-video-vae-with-decoupled-structure</link>
      <description><![CDATA[Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vidtwin-video-vae-with-decoupled-structure</guid>
    </item>
    <item>
      <title>Large Concept Models: Language Modeling in a Sentence Representation Space</title>
      <link>https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</link>
      <description><![CDATA[In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</guid>
    </item>
    <item>
      <title>SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific Large Language Model</title>
      <link>https://paperswithcode.com/paper/semikong-curating-training-and-evaluating-a</link>
      <description><![CDATA[Large Language Models (LLMs) have demonstrated the potential to address some issues within the semiconductor industry.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semikong-curating-training-and-evaluating-a</guid>
    </item>
    <item>
      <title>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</title>
      <link>https://paperswithcode.com/paper/task-preference-optimization-improving</link>
      <description><![CDATA[Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/task-preference-optimization-improving</guid>
    </item>
    <item>
      <title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title>
      <link>https://paperswithcode.com/paper/ditctrl-exploring-attention-control-in-multi</link>
      <description><![CDATA[Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ditctrl-exploring-attention-control-in-multi</guid>
    </item>
    <item>
      <title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title>
      <link>https://paperswithcode.com/paper/drt-o1-optimized-deep-reasoning-translation</link>
      <description><![CDATA[Using Qwen2. 5 and LLama-3. 1 as the backbones, DRT-o1 models can learn the thought process during machine translation, and outperform vanilla LLMs as well as existing O1-like LLMs, showing their effectiveness The project is available at https://github. com/krystalan/DRT-o1]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/drt-o1-optimized-deep-reasoning-translation</guid>
    </item>
    <item>
      <title>Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly</title>
      <link>https://paperswithcode.com/paper/exploring-what-why-and-how-a-multifaceted</link>
      <description><![CDATA[Specifically, each instance of our ECVA involves three sets of human annotations to indicate "what", "why" and "how" of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-what-why-and-how-a-multifaceted</guid>
    </item>
    <item>
      <title>StoryWeaver: A Unified World Model for Knowledge-Enhanced Story Character Customization</title>
      <link>https://paperswithcode.com/paper/storyweaver-a-unified-world-model-for</link>
      <description><![CDATA[Story visualization has gained increasing attention in artificial intelligence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/storyweaver-a-unified-world-model-for</guid>
    </item>
    <item>
      <title>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search</title>
      <link>https://paperswithcode.com/paper/mulberry-empowering-mllm-with-o1-like</link>
      <description><![CDATA[Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mulberry-empowering-mllm-with-o1-like</guid>
    </item>
    <item>
      <title>Monolith: Real Time Recommendation System With Collisionless Embedding Table</title>
      <link>https://paperswithcode.com/paper/monolith-real-time-recommendation-system-with</link>
      <description><![CDATA[In this paper, we present Monolith, a system tailored for online training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monolith-real-time-recommendation-system-with</guid>
    </item>
    <item>
      <title>Conformal prediction under ambiguous ground truth</title>
      <link>https://paperswithcode.com/paper/conformal-prediction-under-ambiguous-ground</link>
      <description><![CDATA[However, in many real-world scenarios, the labels $Y_1,..., Y_n$ are obtained by aggregating expert opinions using a voting procedure, resulting in a one-hot distribution $\mathbb{P}_{vote}^{Y|X}$.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/conformal-prediction-under-ambiguous-ground</guid>
    </item>
    <item>
      <title>OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/openemma-open-source-multimodal-model-for-end</link>
      <description><![CDATA[Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openemma-open-source-multimodal-model-for-end</guid>
    </item>
    <item>
      <title>Byte Latent Transformer: Patches Scale Better Than Tokens</title>
      <link>https://paperswithcode.com/paper/byte-latent-transformer-patches-scale-better</link>
      <description><![CDATA[We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/byte-latent-transformer-patches-scale-better</guid>
    </item>
  </channel>
</rss>
