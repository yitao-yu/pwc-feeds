<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 26 Nov 2024 09:17:36 +0000</lastBuildDate>
    <item>
      <title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title>
      <link>https://paperswithcode.com/paper/samurai-adapting-segment-anything-model-for-1</link>
      <description><![CDATA[The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/samurai-adapting-segment-anything-model-for-1</guid>
    </item>
    <item>
      <title>EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation</title>
      <link>https://paperswithcode.com/paper/echomimicv2-towards-striking-simplified-and</link>
      <description><![CDATA[Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/echomimicv2-towards-striking-simplified-and</guid>
    </item>
    <item>
      <title>JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation</title>
      <link>https://paperswithcode.com/paper/joyvasa-portrait-and-animal-image-animation</link>
      <description><![CDATA[Specifically, in the first stage, we introduce a decoupled facial representation framework that separates dynamic facial expressions from static 3D facial representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/joyvasa-portrait-and-animal-image-animation</guid>
    </item>
    <item>
      <title>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</title>
      <link>https://paperswithcode.com/paper/dino-x-a-unified-vision-model-for-open-world</link>
      <description><![CDATA[DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1. 5 to pursue an object-level representation for open-world object understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dino-x-a-unified-vision-model-for-open-world</guid>
    </item>
    <item>
      <title>The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use</title>
      <link>https://paperswithcode.com/paper/the-dawn-of-gui-agent-a-preliminary-case</link>
      <description><![CDATA[The recently released model, Claude 3. 5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-dawn-of-gui-agent-a-preliminary-case</guid>
    </item>
    <item>
      <title>Learning to Fly in Seconds</title>
      <link>https://paperswithcode.com/paper/learning-to-fly-in-seconds</link>
      <description><![CDATA[Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-fly-in-seconds</guid>
    </item>
    <item>
      <title>Multimodal Autoregressive Pre-training of Large Vision Encoders</title>
      <link>https://paperswithcode.com/paper/multimodal-autoregressive-pre-training-of</link>
      <description><![CDATA[We introduce a novel method for pre-training of large-scale vision encoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-autoregressive-pre-training-of</guid>
    </item>
    <item>
      <title>When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training</title>
      <link>https://paperswithcode.com/paper/when-precision-meets-position-bfloat16-breaks</link>
      <description><![CDATA[To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/when-precision-meets-position-bfloat16-breaks</guid>
    </item>
    <item>
      <title>Tora: Trajectory-oriented Diffusion Transformer for Video Generation</title>
      <link>https://paperswithcode.com/paper/tora-trajectory-oriented-diffusion</link>
      <description><![CDATA[The TE encodes arbitrary trajectories into hierarchical spacetime motion patches with a 3D video compression network.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tora-trajectory-oriented-diffusion</guid>
    </item>
    <item>
      <title>Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback</title>
      <link>https://paperswithcode.com/paper/unpacking-dpo-and-ppo-disentangling-best</link>
      <description><![CDATA[High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unpacking-dpo-and-ppo-disentangling-best</guid>
    </item>
    <item>
      <title>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs</title>
      <link>https://paperswithcode.com/paper/openscholar-synthesizing-scientific</link>
      <description><![CDATA[Scientific progress depends on researchers' ability to synthesize the growing body of literature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openscholar-synthesizing-scientific</guid>
    </item>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</link>
      <description><![CDATA[While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</guid>
    </item>
    <item>
      <title>Multi-Programming Language Sandbox for LLMs</title>
      <link>https://paperswithcode.com/paper/multi-programming-language-sandbox-for-llms</link>
      <description><![CDATA[We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox designed to provide unified and comprehensive feedback from compiler and analysis tools for Large Language Models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-programming-language-sandbox-for-llms</guid>
    </item>
    <item>
      <title>FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations</title>
      <link>https://paperswithcode.com/paper/flipsketch-flipping-static-drawings-to-text</link>
      <description><![CDATA[Sketch animations offer a powerful medium for visual storytelling, from simple flip-book doodles to professional studio productions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flipsketch-flipping-static-drawings-to-text</guid>
    </item>
    <item>
      <title>CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</title>
      <link>https://paperswithcode.com/paper/crisperwhisper-accurate-timestamps-on</link>
      <description><![CDATA[We demonstrate that carefully adjusting the tokenizer of the Whisper speech recognition model significantly improves the precision of word-level timestamps when applying dynamic time warping to the decoder's cross-attention scores.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/crisperwhisper-accurate-timestamps-on</guid>
    </item>
    <item>
      <title>WhisperNER: Unified Open Named Entity and Speech Recognition</title>
      <link>https://paperswithcode.com/paper/whisperner-unified-open-named-entity-and</link>
      <description><![CDATA[In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/whisperner-unified-open-named-entity-and</guid>
    </item>
    <item>
      <title>REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using Extremely Compressed Motion Latents</title>
      <link>https://paperswithcode.com/paper/reducio-generating-1024-times-1024-video</link>
      <description><![CDATA[Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reducio-generating-1024-times-1024-video</guid>
    </item>
    <item>
      <title>Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline</title>
      <link>https://paperswithcode.com/paper/interactive-medical-image-segmentation-a</link>
      <description><![CDATA[To facilitate research on foundational models in medical computer vision, we release the IMed-361M and model at https://github. com/uni-medical/IMIS-Bench.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/interactive-medical-image-segmentation-a</guid>
    </item>
    <item>
      <title>Qwen2.5-Coder Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-5-coder-technical-report</link>
      <description><![CDATA[In this report, we introduce the Qwen2. 5-Coder series, a significant upgrade from its predecessor, CodeQwen1. 5.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-5-coder-technical-report</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
  </channel>
</rss>
