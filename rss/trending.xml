<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 25 Mar 2023 21:05:42 +0000</lastBuildDate>
    <item>
      <title>Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</title>
      <link>https://paperswithcode.com/paper/text2video-zero-text-to-image-diffusion</link>
      <description><![CDATA[Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2video-zero-text-to-image-diffusion</guid>
    </item>
    <item>
      <title>Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/text2room-extracting-textured-3d-meshes-from</link>
      <description><![CDATA[We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2room-extracting-textured-3d-meshes-from</guid>
    </item>
    <item>
      <title>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</title>
      <link>https://paperswithcode.com/paper/smoothquant-accurate-and-efficient-post</link>
      <description><![CDATA[We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/smoothquant-accurate-and-efficient-post</guid>
    </item>
    <item>
      <title>ReVersion: Diffusion-Based Relation Inversion from Images</title>
      <link>https://paperswithcode.com/paper/reversion-diffusion-based-relation-inversion</link>
      <description><![CDATA[Specifically, we propose a novel relation-steering contrastive learning scheme to impose two critical properties of the relation prompt: 1) The relation prompt should capture the interaction between objects, enforced by the preposition prior.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reversion-diffusion-based-relation-inversion</guid>
    </item>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</link>
      <description><![CDATA[We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</guid>
    </item>
    <item>
      <title>Zero-1-to-3: Zero-shot One Image to 3D Object</title>
      <link>https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</link>
      <description><![CDATA[We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</guid>
    </item>
    <item>
      <title>VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation</title>
      <link>https://paperswithcode.com/paper/decomposed-diffusion-models-for-high-quality</link>
      <description><![CDATA[A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decomposed-diffusion-models-for-high-quality</guid>
    </item>
    <item>
      <title>VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</title>
      <link>https://paperswithcode.com/paper/voxelnext-fully-sparse-voxelnet-for-3d-object-1</link>
      <description><![CDATA[Our core insight is to predict objects directly based on sparse voxel features, without relying on hand-crafted proxies.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voxelnext-fully-sparse-voxelnet-for-3d-object-1</guid>
    </item>
    <item>
      <title>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</title>
      <link>https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</link>
      <description><![CDATA[To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</guid>
    </item>
    <item>
      <title>A Dynamic Multi-Scale Voxel Flow Network for Video Prediction</title>
      <link>https://paperswithcode.com/paper/a-dynamic-multi-scale-voxel-flow-network-for</link>
      <description><![CDATA[The performance of video prediction has been greatly boosted by advanced deep neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-dynamic-multi-scale-voxel-flow-network-for</guid>
    </item>
    <item>
      <title>SHERF: Generalizable Human NeRF from a Single Image</title>
      <link>https://paperswithcode.com/paper/sherf-generalizable-human-nerf-from-a-single</link>
      <description><![CDATA[To this end, we propose a bank of 3D-aware hierarchical features, including global, point-level, and pixel-aligned features, to facilitate informative encoding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sherf-generalizable-human-nerf-from-a-single</guid>
    </item>
    <item>
      <title>NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping</title>
      <link>https://paperswithcode.com/paper/nerf-loam-neural-implicit-representation-for</link>
      <description><![CDATA[To bridge this gap, in this paper, we propose a novel NeRF-based LiDAR odometry and mapping approach, NeRF-LOAM, consisting of three modules neural odometry, neural mapping, and mesh reconstruction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nerf-loam-neural-implicit-representation-for</guid>
    </item>
    <item>
      <title>SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</title>
      <link>https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</link>
      <description><![CDATA[We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</guid>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</link>
      <description><![CDATA[We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</guid>
    </item>
    <item>
      <title>Learning Context-aware Classifier for Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/learning-context-aware-classifier-for</link>
      <description><![CDATA[Semantic segmentation is still a challenging task for parsing diverse contexts in different scenes, thus the fixed classifier might not be able to well address varying feature distributions during testing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-context-aware-classifier-for</guid>
    </item>
    <item>
      <title>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot</title>
      <link>https://paperswithcode.com/paper/massive-language-models-can-be-accurately</link>
      <description><![CDATA[We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/massive-language-models-can-be-accurately</guid>
    </item>
    <item>
      <title>Neural Preset for Color Style Transfer</title>
      <link>https://paperswithcode.com/paper/neural-preset-for-color-style-transfer</link>
      <description><![CDATA[In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-preset-for-color-style-transfer</guid>
    </item>
    <item>
      <title>FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</title>
      <link>https://paperswithcode.com/paper/fatezero-fusing-attentions-for-zero-shot-text</link>
      <description><![CDATA[We also have a better zero-shot shape-aware editing ability based on the text-to-video model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fatezero-fusing-attentions-for-zero-shot-text</guid>
    </item>
    <item>
      <title>Planning-oriented Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/goal-oriented-autonomous-driving</link>
      <description><![CDATA[Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/goal-oriented-autonomous-driving</guid>
    </item>
    <item>
      <title>Ablating Concepts in Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/ablating-concepts-in-text-to-image-diffusion</link>
      <description><![CDATA[To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i. e., preventing the generation of a target concept.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ablating-concepts-in-text-to-image-diffusion</guid>
    </item>
  </channel>
</rss>
