<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 23 Nov 2023 09:12:30 +0000</lastBuildDate>
    <item>
      <title>StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models</title>
      <link>https://paperswithcode.com/paper/styletts-2-towards-human-level-text-to-speech</link>
      <description><![CDATA[In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/styletts-2-towards-human-level-text-to-speech</guid>
    </item>
    <item>
      <title>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</title>
      <link>https://paperswithcode.com/paper/video-llava-learning-united-visual-1</link>
      <description><![CDATA[In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-llava-learning-united-visual-1</guid>
    </item>
    <item>
      <title>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</title>
      <link>https://paperswithcode.com/paper/luciddreamer-towards-high-fidelity-text-to-3d</link>
      <description><![CDATA[The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/luciddreamer-towards-high-fidelity-text-to-3d</guid>
    </item>
    <item>
      <title>GraphCast: Learning skillful medium-range global weather forecasting</title>
      <link>https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</link>
      <description><![CDATA[Global medium-range weather forecasting is critical to decision-making across many social and economic domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</guid>
    </item>
    <item>
      <title>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents</title>
      <link>https://paperswithcode.com/paper/igniting-language-intelligence-the-hitchhiker</link>
      <description><![CDATA[Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/igniting-language-intelligence-the-hitchhiker</guid>
    </item>
    <item>
      <title>JaxMARL: Multi-Agent RL Environments in JAX</title>
      <link>https://paperswithcode.com/paper/jaxmarl-multi-agent-rl-environments-in-jax</link>
      <description><![CDATA[This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/jaxmarl-multi-agent-rl-environments-in-jax</guid>
    </item>
    <item>
      <title>Stella Nera: Achieving 161 TOp/s/W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication</title>
      <link>https://paperswithcode.com/paper/stella-nera-achieving-161-top-s-w-with</link>
      <description><![CDATA[From classical HPC to deep learning, MatMul is at the heart of today's computing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stella-nera-achieving-161-top-s-w-with</guid>
    </item>
    <item>
      <title>Mustango: Toward Controllable Text-to-Music Generation</title>
      <link>https://paperswithcode.com/paper/mustango-toward-controllable-text-to-music</link>
      <description><![CDATA[With recent advancements in text-to-audio and text-to-music based on latent diffusion models, the quality of generated content has been reaching new heights.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mustango-toward-controllable-text-to-music</guid>
    </item>
    <item>
      <title>A Survey on Language Models for Code</title>
      <link>https://paperswithcode.com/paper/a-survey-on-language-models-for-code</link>
      <description><![CDATA[In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 150+ datasets, and 550 related works.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-language-models-for-code</guid>
    </item>
    <item>
      <title>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</title>
      <link>https://paperswithcode.com/paper/black-box-prompt-optimization-aligning-large</link>
      <description><![CDATA[However, these models are often not well aligned with human intents, which calls for additional treatments on them, that is, the alignment problem.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/black-box-prompt-optimization-aligning-large</guid>
    </item>
    <item>
      <title>Plum: Prompt Learning using Metaheuristic</title>
      <link>https://paperswithcode.com/paper/plum-prompt-learning-using-metaheuristic</link>
      <description><![CDATA[Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/plum-prompt-learning-using-metaheuristic</guid>
    </item>
    <item>
      <title>Large-Scale Intelligent Microservices</title>
      <link>https://paperswithcode.com/paper/large-scale-intelligent-microservices</link>
      <description><![CDATA[Deploying Machine Learning (ML) algorithms within databases is a challenge due to the varied computational footprints of modern ML algorithms and the myriad of database technologies each with its own restrictive syntax.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-scale-intelligent-microservices</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</title>
      <link>https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora</link>
      <description><![CDATA[To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora</guid>
    </item>
    <item>
      <title>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</title>
      <link>https://paperswithcode.com/paper/lcm-lora-a-universal-stable-diffusion</link>
      <description><![CDATA[Latent Consistency Models (LCMs) have achieved impressive performance in accelerating text-to-image generative tasks, producing high-quality images with minimal inference steps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lcm-lora-a-universal-stable-diffusion</guid>
    </item>
    <item>
      <title>Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling</title>
      <link>https://paperswithcode.com/paper/distil-whisper-robust-knowledge-distillation</link>
      <description><![CDATA[As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/distil-whisper-robust-knowledge-distillation</guid>
    </item>
    <item>
      <title>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</title>
      <link>https://paperswithcode.com/paper/monkey-image-resolution-and-text-label-are</link>
      <description><![CDATA[Large Multimodal Models have demonstrated impressive capabilities in understanding general vision-language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monkey-image-resolution-and-text-label-are</guid>
    </item>
    <item>
      <title>VideoCon: Robust Video-Language Alignment via Contrast Captions</title>
      <link>https://paperswithcode.com/paper/videocon-robust-video-language-alignment-via</link>
      <description><![CDATA[Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videocon-robust-video-language-alignment-via</guid>
    </item>
    <item>
      <title>MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning</title>
      <link>https://paperswithcode.com/paper/medagents-large-language-models-as</link>
      <description><![CDATA[Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medagents-large-language-models-as</guid>
    </item>
    <item>
      <title>Zephyr: Direct Distillation of LM Alignment</title>
      <link>https://paperswithcode.com/paper/zephyr-direct-distillation-of-lm-alignment</link>
      <description><![CDATA[Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zephyr-direct-distillation-of-lm-alignment</guid>
    </item>
  </channel>
</rss>
