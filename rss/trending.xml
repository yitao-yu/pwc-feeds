<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 06 Oct 2022 09:24:24 +0000</lastBuildDate>
    <item>
      <title>Protein structure generation via folding diffusion</title>
      <link>https://paperswithcode.com/paper/protein-structure-generation-via-folding</link>
      <description><![CDATA[The ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/protein-structure-generation-via-folding</guid>
    </item>
    <item>
      <title>Human Motion Diffusion Model</title>
      <link>https://paperswithcode.com/paper/human-motion-diffusion-model</link>
      <description><![CDATA[In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-motion-diffusion-model</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images</title>
      <link>https://paperswithcode.com/paper/get3d-a-generative-model-of-high-quality-3d</link>
      <description><![CDATA[As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/get3d-a-generative-model-of-high-quality-3d</guid>
    </item>
    <item>
      <title>Deep Neural Networks to Detect Weeds from Crops in Agricultural Environments in Real-Time: A Review</title>
      <link>https://paperswithcode.com/paper/deep-neural-networks-to-detect-weeds-from</link>
      <description><![CDATA[Machine vision has wide applications in agriculture, including the detection of weeds and pests in crops.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-neural-networks-to-detect-weeds-from</guid>
    </item>
    <item>
      <title>TabDDPM: Modelling Tabular Data with Diffusion Models</title>
      <link>https://paperswithcode.com/paper/tabddpm-modelling-tabular-data-with-diffusion</link>
      <description><![CDATA[Denoising diffusion probabilistic models are currently becoming the leading paradigm of generative modeling for many important data modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tabddpm-modelling-tabular-data-with-diffusion</guid>
    </item>
    <item>
      <title>Offline Reinforcement Learning with Implicit Q-Learning</title>
      <link>https://paperswithcode.com/paper/offline-reinforcement-learning-with-implicit</link>
      <description><![CDATA[The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/offline-reinforcement-learning-with-implicit</guid>
    </item>
    <item>
      <title>Min-Max Similarity: A Contrastive Semi-Supervised Deep Learning Network for Surgical Tools Segmentation</title>
      <link>https://paperswithcode.com/paper/min-max-similarity-a-contrastive-learning</link>
      <description><![CDATA[To address this issue, we proposed a semi-supervised segmentation network based on contrastive learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/min-max-similarity-a-contrastive-learning</guid>
    </item>
    <item>
      <title>SoundStream: An End-to-End Neural Audio Codec</title>
      <link>https://paperswithcode.com/paper/soundstream-an-end-to-end-neural-audio-codec</link>
      <description><![CDATA[We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soundstream-an-end-to-end-neural-audio-codec</guid>
    </item>
    <item>
      <title>LAVIS: A Library for Language-Vision Intelligence</title>
      <link>https://paperswithcode.com/paper/lavis-a-library-for-language-vision</link>
      <description><![CDATA[We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lavis-a-library-for-language-vision</guid>
    </item>
    <item>
      <title>Dilated Neighborhood Attention Transformer</title>
      <link>https://paperswithcode.com/paper/dilated-neighborhood-attention-transformer</link>
      <description><![CDATA[These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dilated-neighborhood-attention-transformer</guid>
    </item>
    <item>
      <title>SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model</title>
      <link>https://paperswithcode.com/paper/speechclip-integrating-speech-with-pre</link>
      <description><![CDATA[Data-driven speech processing models usually perform well with a large amount of text supervision, but collecting transcribed speech data is costly.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/speechclip-integrating-speech-with-pre</guid>
    </item>
    <item>
      <title>IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis</title>
      <link>https://paperswithcode.com/paper/intrinsicnerf-learning-intrinsic-neural</link>
      <description><![CDATA[Given that intrinsic decomposition is a fundamentally ambiguous and under-constrained inverse problem, we propose a novel distance-aware point sampling and adaptive reflectance iterative clustering optimization method that enables IntrinsicNeRF with traditional intrinsic decomposition constraints to be trained in an unsupervised manner, resulting in temporally consistent intrinsic decomposition results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intrinsicnerf-learning-intrinsic-neural</guid>
    </item>
    <item>
      <title>rl</title>
      <link>https://github.com/facebookresearch/rl</link>
      <description><![CDATA[A modular, primitive-first, python-first PyTorch library for Reinforcement Learning.]]></description>
      <guid isPermaLink="true">https://github.com/facebookresearch/rl</guid>
    </item>
    <item>
      <title>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</title>
      <link>https://paperswithcode.com/paper/vip-towards-universal-visual-reward-and</link>
      <description><![CDATA[Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vip-towards-universal-visual-reward-and</guid>
    </item>
    <item>
      <title>Efficient Few-Shot Learning Without Prompts</title>
      <link>https://paperswithcode.com/paper/efficient-few-shot-learning-without-prompts</link>
      <description><![CDATA[This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-few-shot-learning-without-prompts</guid>
    </item>
    <item>
      <title>VToonify: Controllable High-Resolution Portrait Video Style Transfer</title>
      <link>https://paperswithcode.com/paper/vtoonify-controllable-high-resolution</link>
      <description><![CDATA[Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vtoonify-controllable-high-resolution</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale</guid>
    </item>
    <item>
      <title>High-Resolution Image Synthesis with Latent Diffusion Models</title>
      <link>https://paperswithcode.com/paper/high-resolution-image-synthesis-with-latent</link>
      <description><![CDATA[By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-resolution-image-synthesis-with-latent</guid>
    </item>
    <item>
      <title>PyEPO: A PyTorch-based End-to-End Predict-then-Optimize Library for Linear and Integer Programming</title>
      <link>https://paperswithcode.com/paper/pyepo-a-pytorch-based-end-to-end-predict-then</link>
      <description><![CDATA[It provides two base algorithms: the first is based on the convex surrogate loss function from the seminal work of Elmachtoub & Grigas (2021), and the second is based on the differentiable black-box solver approach of Vlastelica et al. (2019).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pyepo-a-pytorch-based-end-to-end-predict-then</guid>
    </item>
  </channel>
</rss>
