<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 12 Jun 2024 09:14:34 +0000</lastBuildDate>
    <item>
      <title>Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</title>
      <link>https://paperswithcode.com/paper/autoregressive-model-beats-diffusion-llama</link>
      <description><![CDATA[(3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoregressive-model-beats-diffusion-llama</guid>
    </item>
    <item>
      <title>Scalable MatMul-free Language Modeling</title>
      <link>https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</link>
      <description><![CDATA[Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2. 7B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</guid>
    </item>
    <item>
      <title>"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</title>
      <link>https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating</link>
      <description><![CDATA[We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating</guid>
    </item>
    <item>
      <title>Matching Anything by Segmenting Anything</title>
      <link>https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</link>
      <description><![CDATA[The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</guid>
    </item>
    <item>
      <title>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</title>
      <link>https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</link>
      <description><![CDATA[Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</guid>
    </item>
    <item>
      <title>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</title>
      <link>https://paperswithcode.com/paper/buffer-of-thoughts-thought-augmented</link>
      <description><![CDATA[We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/buffer-of-thoughts-thought-augmented</guid>
    </item>
    <item>
      <title>Blind Image Restoration via Fast Diffusion Inversion</title>
      <link>https://paperswithcode.com/paper/blind-image-restoration-via-fast-diffusion-2</link>
      <description><![CDATA[This is ultimately equivalent to casting the IR task as an optimization problem in the space of the input noise.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blind-image-restoration-via-fast-diffusion-2</guid>
    </item>
    <item>
      <title>StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning</title>
      <link>https://paperswithcode.com/paper/streamspeech-simultaneous-speech-to-speech</link>
      <description><![CDATA[Simultaneous speech-to-speech translation (Simul-S2ST, a. k. a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamspeech-simultaneous-speech-to-speech</guid>
    </item>
    <item>
      <title>Vision-LSTM: xLSTM as Generic Vision Backbone</title>
      <link>https://paperswithcode.com/paper/vision-lstm-xlstm-as-generic-vision-backbone</link>
      <description><![CDATA[Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-lstm-xlstm-as-generic-vision-backbone</guid>
    </item>
    <item>
      <title>Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis</title>
      <link>https://paperswithcode.com/paper/lighting-every-darkness-with-3dgs-fast</link>
      <description><![CDATA[Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lighting-every-darkness-with-3dgs-fast</guid>
    </item>
    <item>
      <title>Multi-Head RAG: Solving Multi-Aspect Problems with LLMs</title>
      <link>https://paperswithcode.com/paper/multi-head-rag-solving-multi-aspect-problems</link>
      <description><![CDATA[Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-head-rag-solving-multi-aspect-problems</guid>
    </item>
    <item>
      <title>Mathematical Supplement for the $\texttt{gsplat}$ Library</title>
      <link>https://paperswithcode.com/paper/mathematical-supplement-for-the-texttt-gsplat</link>
      <description><![CDATA[This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mathematical-supplement-for-the-texttt-gsplat</guid>
    </item>
    <item>
      <title>Fast Timing-Conditioned Latent Audio Diffusion</title>
      <link>https://paperswithcode.com/paper/fast-timing-conditioned-latent-audio</link>
      <description><![CDATA[Generating long-form 44. 1kHz stereo audio from text prompts can be computationally demanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-timing-conditioned-latent-audio</guid>
    </item>
    <item>
      <title>Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness</title>
      <link>https://paperswithcode.com/paper/less-is-more-removing-text-regions-improves</link>
      <description><![CDATA[In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/less-is-more-removing-text-regions-improves</guid>
    </item>
    <item>
      <title>Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training</title>
      <link>https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy</link>
      <description><![CDATA[In this work, we revisit the settings by adopting step time as a more accurate measure of model complexity, and by determining the total compute budget under the Chinchilla compute-optimal settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy</guid>
    </item>
    <item>
      <title>Scaling and evaluating sparse autoencoders</title>
      <link>https://paperswithcode.com/paper/scaling-and-evaluating-sparse-autoencoders</link>
      <description><![CDATA[Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-and-evaluating-sparse-autoencoders</guid>
    </item>
    <item>
      <title>Seed-TTS: A Family of High-Quality Versatile Speech Generation Models</title>
      <link>https://paperswithcode.com/paper/seed-tts-a-family-of-high-quality-versatile</link>
      <description><![CDATA[Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-tts-a-family-of-high-quality-versatile</guid>
    </item>
    <item>
      <title>AgentGym: Evolving Large Language Model-based Agents across Diverse Environments</title>
      <link>https://paperswithcode.com/paper/agentgym-evolving-large-language-model-based</link>
      <description><![CDATA[Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentgym-evolving-large-language-model-based</guid>
    </item>
    <item>
      <title>Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning</title>
      <link>https://paperswithcode.com/paper/husky-a-unified-open-source-language-agent</link>
      <description><![CDATA[Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/husky-a-unified-open-source-language-agent</guid>
    </item>
    <item>
      <title>Recurrent Context Compression: Efficiently Expanding the Context Window of LLM</title>
      <link>https://paperswithcode.com/paper/recurrent-context-compression-efficiently</link>
      <description><![CDATA[To extend the context length of Transformer-based large language models (LLMs) and improve comprehension capabilities, we often face limitations due to computational resources and bounded memory storage capacity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/recurrent-context-compression-efficiently</guid>
    </item>
  </channel>
</rss>
