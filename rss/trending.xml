<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 08 Jul 2022 09:16:23 +0000</lastBuildDate>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>Pen and Paper Exercises in Machine Learning</title>
      <link>https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</link>
      <description><![CDATA[This is a collection of (mostly) pen-and-paper exercises in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</guid>
    </item>
    <item>
      <title>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</title>
      <link>https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</link>
      <description><![CDATA[Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</guid>
    </item>
    <item>
      <title>Speech Denoising in the Waveform Domain with Self-Attention</title>
      <link>https://paperswithcode.com/paper/speech-denoising-in-the-waveform-domain-with</link>
      <description><![CDATA[In this work, we present CleanUNet, a causal speech denoising model on the raw waveform.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/speech-denoising-in-the-waveform-domain-with</guid>
    </item>
    <item>
      <title>Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation</title>
      <link>https://paperswithcode.com/paper/efficient-spatial-temporal-information-fusion</link>
      <description><![CDATA[We also use a point refinement module via 3D sparse convolution to fuse the information from both LiDAR range image and point cloud representations and reduce the artifacts on the borders of the objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-spatial-temporal-information-fusion</guid>
    </item>
    <item>
      <title>DCT-Net: Domain-Calibrated Translation for Portrait Stylization</title>
      <link>https://paperswithcode.com/paper/dct-net-domain-calibrated-translation-for</link>
      <description><![CDATA[This paper introduces DCT-Net, a novel image translation architecture for few-shot portrait stylization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dct-net-domain-calibrated-translation-for</guid>
    </item>
    <item>
      <title>CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/coderl-mastering-code-generation-through</link>
      <description><![CDATA[To address the limitations, we propose "CodeRL", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/coderl-mastering-code-generation-through</guid>
    </item>
    <item>
      <title>AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture</title>
      <link>https://paperswithcode.com/paper/avatarcap-animatable-avatar-conditioned</link>
      <description><![CDATA[Then given a monocular RGB video of this subject, our method integrates information from both the image observation and the avatar prior, and accordingly recon-structs high-fidelity 3D textured models with dynamic details regardless of the visibility.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/avatarcap-animatable-avatar-conditioned</guid>
    </item>
    <item>
      <title>DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech</title>
      <link>https://paperswithcode.com/paper/dailytalk-spoken-dialogue-dataset-for</link>
      <description><![CDATA[We sampled, modified, and recorded 2, 541 dialogues from the open-domain dialogue dataset DailyDialog which are adequately long to represent context of each dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dailytalk-spoken-dialogue-dataset-for</guid>
    </item>
    <item>
      <title>Text2Human: Text-Driven Controllable Human Image Generation</title>
      <link>https://paperswithcode.com/paper/text2human-text-driven-controllable-human</link>
      <description><![CDATA[In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2human-text-driven-controllable-human</guid>
    </item>
    <item>
      <title>BoT-SORT: Robust Associations Multi-Pedestrian Tracking</title>
      <link>https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</link>
      <description><![CDATA[The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</guid>
    </item>
    <item>
      <title>MLGO: a Machine Learning Guided Compiler Optimizations Framework</title>
      <link>https://paperswithcode.com/paper/mlgo-a-machine-learning-guided-compiler</link>
      <description><![CDATA[Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mlgo-a-machine-learning-guided-compiler</guid>
    </item>
    <item>
      <title>Disentangling Random and Cyclic Effects in Time-Lapse Sequences</title>
      <link>https://paperswithcode.com/paper/disentangling-random-and-cyclic-effects-in</link>
      <description><![CDATA[We introduce the problem of disentangling time-lapse sequences in a way that allows separate, after-the-fact control of overall trends, cyclic effects, and random effects in the images, and describe a technique based on data-driven generative models that achieves this goal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/disentangling-random-and-cyclic-effects-in</guid>
    </item>
    <item>
      <title>Back to MLP: A Simple Baseline for Human Motion Prediction</title>
      <link>https://paperswithcode.com/paper/back-to-mlp-a-simple-baseline-for-human</link>
      <description><![CDATA[This paper tackles the problem of human motion prediction, consisting in forecasting future body poses from historically observed sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/back-to-mlp-a-simple-baseline-for-human</guid>
    </item>
    <item>
      <title>Towards a General Purpose CNN for Long Range Dependencies in $N$D</title>
      <link>https://paperswithcode.com/paper/towards-a-general-purpose-cnn-for-long-range</link>
      <description><![CDATA[The use of Convolutional Neural Networks (CNNs) is widespread in Deep Learning due to a range of desirable model properties which result in an efficient and effective machine learning framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-a-general-purpose-cnn-for-long-range</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/point-to-voxel-knowledge-distillation-for-1</link>
      <description><![CDATA[This article addresses the problem of distilling knowledge from a large teacher model to a slim student network for LiDAR semantic segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/point-to-voxel-knowledge-distillation-for-1</guid>
    </item>
    <item>
      <title>Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity</title>
      <link>https://paperswithcode.com/paper/accelerating-sparse-dnn-models-without</link>
      <description><![CDATA[Network pruning can reduce the high computation cost of deep neural network (DNN) models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/accelerating-sparse-dnn-models-without</guid>
    </item>
    <item>
      <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</link>
      <description><![CDATA[We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</guid>
    </item>
    <item>
      <title>Global Context Vision Transformers</title>
      <link>https://paperswithcode.com/paper/global-context-vision-transformers</link>
      <description><![CDATA[We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/global-context-vision-transformers</guid>
    </item>
  </channel>
</rss>
