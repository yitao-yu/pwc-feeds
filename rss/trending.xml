<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 27 Feb 2025 09:17:07 +0000</lastBuildDate>
    <item>
      <title>Fractal Generative Models</title>
      <link>https://paperswithcode.com/paper/fractal-generative-models</link>
      <description><![CDATA[In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fractal-generative-models</guid>
    </item>
    <item>
      <title>Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction</title>
      <link>https://paperswithcode.com/paper/step-audio-unified-understanding-and</link>
      <description><![CDATA[Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/step-audio-unified-understanding-and</guid>
    </item>
    <item>
      <title>Slamming: Training a Speech Language Model on One GPU in a Day</title>
      <link>https://paperswithcode.com/paper/slamming-training-a-speech-language-model-on</link>
      <description><![CDATA[We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slamming-training-a-speech-language-model-on</guid>
    </item>
    <item>
      <title>R1-OnevisionAn Open-Source Multimodal Large Language Model Capable of Deep Reasoning</title>
      <link>https://paperswithcode.com/paper/r1-onevision-an-open-source-multimodal-large</link>
      <description><![CDATA[R1-OneVision is a versatile multimodal reasoning large model, designed to tackle complex visual reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r1-onevision-an-open-source-multimodal-large</guid>
    </item>
    <item>
      <title>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</title>
      <link>https://paperswithcode.com/paper/step-video-t2v-technical-report-the-practice</link>
      <description><![CDATA[We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/step-video-t2v-technical-report-the-practice</guid>
    </item>
    <item>
      <title>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</title>
      <link>https://paperswithcode.com/paper/diception-a-generalist-diffusion-model-for</link>
      <description><![CDATA[We achieve results on par with SAM-vit-h using only 0. 06% of their data (e. g., 600K vs. 1B pixel-level annotated images).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diception-a-generalist-diffusion-model-for</guid>
    </item>
    <item>
      <title>Magma: A Foundation Model for Multimodal AI Agents</title>
      <link>https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</link>
      <description><![CDATA[We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</guid>
    </item>
    <item>
      <title>OmniParser for Pure Vision Based GUI Agent</title>
      <link>https://paperswithcode.com/paper/omniparser-for-pure-vision-based-gui-agent</link>
      <description><![CDATA[The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omniparser-for-pure-vision-based-gui-agent</guid>
    </item>
    <item>
      <title>MoBA: Mixture of Block Attention for Long-Context LLMs</title>
      <link>https://paperswithcode.com/paper/moba-mixture-of-block-attention-for-long</link>
      <description><![CDATA[Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moba-mixture-of-block-attention-for-long</guid>
    </item>
    <item>
      <title>Craw4LLM: Efficient Web Crawling for LLM Pretraining</title>
      <link>https://paperswithcode.com/paper/craw4llm-efficient-web-crawling-for-llm</link>
      <description><![CDATA[Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/craw4llm-efficient-web-crawling-for-llm</guid>
    </item>
    <item>
      <title>PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</title>
      <link>https://paperswithcode.com/paper/photodoodle-learning-artistic-image-editing</link>
      <description><![CDATA[Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photodoodle-learning-artistic-image-editing</guid>
    </item>
    <item>
      <title>Flaming-hot Initiation with Regular Execution Sampling for Large Language Models</title>
      <link>https://paperswithcode.com/paper/flaming-hot-initiation-with-regular-execution</link>
      <description><![CDATA[Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flaming-hot-initiation-with-regular-execution</guid>
    </item>
    <item>
      <title>VaViM and VaVAM: Autonomous Driving through Video Generative Modeling</title>
      <link>https://paperswithcode.com/paper/vavim-and-vavam-autonomous-driving-through</link>
      <description><![CDATA[We explore the potential of large-scale generative video models for autonomous driving, introducing an open-source auto-regressive video model (VaViM) and its companion video-action model (VaVAM) to investigate how video pre-training transfers to real-world driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vavim-and-vavam-autonomous-driving-through</guid>
    </item>
    <item>
      <title>PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation</title>
      <link>https://paperswithcode.com/paper/pike-rag-specialized-knowledge-and-rationale</link>
      <description><![CDATA[Despite notable advancements in Retrieval-Augmented Generation (RAG) systems that expand large language model (LLM) capabilities through external retrieval, these systems often struggle to meet the complex and diverse needs of real-world industrial applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pike-rag-specialized-knowledge-and-rationale</guid>
    </item>
    <item>
      <title>HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</title>
      <link>https://paperswithcode.com/paper/healthgpt-a-medical-large-vision-language</link>
      <description><![CDATA[To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/healthgpt-a-medical-large-vision-language</guid>
    </item>
    <item>
      <title>Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs</title>
      <link>https://paperswithcode.com/paper/towards-economical-inference-enabling</link>
      <description><![CDATA[For example, the KV cache size of Llama2-7B is reduced by 92. 19%, with only a 0. 5% drop in LongBench performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-economical-inference-enabling</guid>
    </item>
    <item>
      <title>SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation</title>
      <link>https://paperswithcode.com/paper/songgen-a-single-stage-auto-regressive</link>
      <description><![CDATA[To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/songgen-a-single-stage-auto-regressive</guid>
    </item>
    <item>
      <title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title>
      <link>https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</link>
      <description><![CDATA[We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</guid>
    </item>
    <item>
      <title>Optimizing Model Selection for Compound AI Systems</title>
      <link>https://paperswithcode.com/paper/optimizing-model-selection-for-compound-ai</link>
      <description><![CDATA[We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/optimizing-model-selection-for-compound-ai</guid>
    </item>
    <item>
      <title>Hawk: Learning to Understand Open-World Video Anomalies</title>
      <link>https://paperswithcode.com/paper/hawk-learning-to-understand-open-world-video</link>
      <description><![CDATA[Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hawk-learning-to-understand-open-world-video</guid>
    </item>
  </channel>
</rss>
