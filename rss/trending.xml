<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 16 Dec 2022 09:12:06 +0000</lastBuildDate>
    <item>
      <title>RT-1: Robotics Transformer for Real-World Control at Scale</title>
      <link>https://paperswithcode.com/paper/rt-1-robotics-transformer-for-real-world</link>
      <description><![CDATA[By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rt-1-robotics-transformer-for-real-world</guid>
    </item>
    <item>
      <title>4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions</title>
      <link>https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields</link>
      <description><![CDATA[In this paper, we present a novel and effective framework, named 4K-NeRF, to pursue high fidelity view synthesis on the challenging scenarios of ultra high resolutions, building on the methodology of neural radiance fields (NeRF).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields</guid>
    </item>
    <item>
      <title>NMS Strikes Back</title>
      <link>https://paperswithcode.com/paper/nms-strikes-back</link>
      <description><![CDATA[Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50. 2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone, outperforming all existing traditional or transformer-based detectors in this setting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nms-strikes-back</guid>
    </item>
    <item>
      <title>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</title>
      <link>https://paperswithcode.com/paper/training-free-structured-diffusion-guidance</link>
      <description><![CDATA[In this work, we improve the compositional skills of T2I models, specifically more accurate attribute binding and better image compositions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-free-structured-diffusion-guidance</guid>
    </item>
    <item>
      <title>Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal</title>
      <link>https://paperswithcode.com/paper/reasoning-over-different-types-of-knowledge</link>
      <description><![CDATA[The early works in this domain mainly focus on static KGR and tend to directly apply general knowledge graph embedding models to the reasoning task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reasoning-over-different-types-of-knowledge</guid>
    </item>
    <item>
      <title>DifFace: Blind Face Restoration with Diffused Error Contraction</title>
      <link>https://paperswithcode.com/paper/difface-blind-face-restoration-with-diffused</link>
      <description><![CDATA[Moreover, the transition distribution can contract the error of the restoration backbone and thus makes our method more robust to unknown degradations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/difface-blind-face-restoration-with-diffused</guid>
    </item>
    <item>
      <title>What do Vision Transformers Learn? A Visual Exploration</title>
      <link>https://paperswithcode.com/paper/what-do-vision-transformers-learn-a-visual</link>
      <description><![CDATA[In addition, we show that ViTs maintain spatial information in all layers except the final layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-do-vision-transformers-learn-a-visual</guid>
    </item>
    <item>
      <title>GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation</title>
      <link>https://paperswithcode.com/paper/gpvit-a-high-resolution-non-hierarchical</link>
      <description><![CDATA[In each GP Block, features are first grouped together by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped features; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpvit-a-high-resolution-non-hierarchical</guid>
    </item>
    <item>
      <title>Programming Is Hard -- Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation</title>
      <link>https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</link>
      <description><![CDATA[The introductory programming sequence has been the focus of much research in computing education.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</guid>
    </item>
    <item>
      <title>ECON: Explicit Clothed humans Obtained from Normals</title>
      <link>https://paperswithcode.com/paper/econ-explicit-clothed-humans-obtained-from</link>
      <description><![CDATA[The combination of artist-curated scans, and deep implicit functions (IF), is enabling the creation of detailed, clothed, 3D humans from images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/econ-explicit-clothed-humans-obtained-from</guid>
    </item>
    <item>
      <title>The Stable Artist: Steering Semantics in Diffusion Latent Space</title>
      <link>https://paperswithcode.com/paper/the-stable-artist-steering-semantics-in</link>
      <description><![CDATA[Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-stable-artist-steering-semantics-in</guid>
    </item>
    <item>
      <title>CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet</title>
      <link>https://paperswithcode.com/paper/clip-itself-is-a-strong-fine-tuner-achieving</link>
      <description><![CDATA[Recent studies have shown that CLIP has achieved remarkable success in performing zero-shot inference while its fine-tuning performance is not satisfactory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/clip-itself-is-a-strong-fine-tuner-achieving</guid>
    </item>
    <item>
      <title>Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</title>
      <link>https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</link>
      <description><![CDATA[To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</guid>
    </item>
    <item>
      <title>Learning Video Representations from Large Language Models</title>
      <link>https://paperswithcode.com/paper/learning-video-representations-from-large</link>
      <description><![CDATA[We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-video-representations-from-large</guid>
    </item>
    <item>
      <title>DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization</title>
      <link>https://paperswithcode.com/paper/da-wand-distortion-aware-selection-using</link>
      <description><![CDATA[We present a neural technique for learning to select a local sub-region around a point which can be used for mesh parameterization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/da-wand-distortion-aware-selection-using</guid>
    </item>
    <item>
      <title>Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders</title>
      <link>https://paperswithcode.com/paper/learning-3d-representations-from-2d-pre</link>
      <description><![CDATA[Pre-training by numerous image data has become de-facto for robust 2D representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-3d-representations-from-2d-pre</guid>
    </item>
    <item>
      <title>GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech</title>
      <link>https://paperswithcode.com/paper/generspeech-towards-style-transfer-for</link>
      <description><![CDATA[Style transfer for out-of-domain (OOD) speech synthesis aims to generate speech samples with unseen style (e. g., speaker identity, emotion, and prosody) derived from an acoustic reference, while facing the following challenges: 1) The highly dynamic style features in expressive voice are difficult to model and transfer; and 2) the TTS models should be robust enough to handle diverse OOD conditions that differ from the source data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generspeech-towards-style-transfer-for</guid>
    </item>
    <item>
      <title>ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer</title>
      <link>https://paperswithcode.com/paper/aspanformer-detector-free-image-matching-with</link>
      <description><![CDATA[Generating robust and reliable correspondences across images is a fundamental task for a diversity of applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aspanformer-detector-free-image-matching-with</guid>
    </item>
    <item>
      <title>Structured Prompting: Scaling In-Context Learning to 1,000 Examples</title>
      <link>https://paperswithcode.com/paper/structured-prompting-scaling-in-context</link>
      <description><![CDATA[Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structured-prompting-scaling-in-context</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
  </channel>
</rss>
