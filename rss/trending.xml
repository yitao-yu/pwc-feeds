<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 06 Jan 2024 09:11:10 +0000</lastBuildDate>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>AnyText: Multilingual Visual Text Generation And Editing</title>
      <link>https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</link>
      <description><![CDATA[Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</guid>
    </item>
    <item>
      <title>Fast Inference of Mixture-of-Experts Language Models with Offloading</title>
      <link>https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</link>
      <description><![CDATA[In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</guid>
    </item>
    <item>
      <title>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</title>
      <link>https://paperswithcode.com/paper/tinygpt-v-efficient-multimodal-large-language</link>
      <description><![CDATA[In the era of advanced multimodel learning, multimodal large language models (MLLMs) such as GPT-4V have made remarkable strides towards bridging language and visual elements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinygpt-v-efficient-multimodal-large-language</guid>
    </item>
    <item>
      <title>GPT-4V(ision) is a Generalist Web Agent, if Grounded</title>
      <link>https://paperswithcode.com/paper/gpt-4v-ision-is-a-generalist-web-agent-if</link>
      <description><![CDATA[The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-4v-ision-is-a-generalist-web-agent-if</guid>
    </item>
    <item>
      <title>EasyVolcap: Accelerating Neural Volumetric Video Research</title>
      <link>https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</link>
      <description><![CDATA[Volumetric video is a technology that digitally records dynamic events such as artistic performances, sporting events, and remote conversations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</guid>
    </item>
    <item>
      <title>MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices</title>
      <link>https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong</link>
      <description><![CDATA[We present MobileVLM, a competent multimodal vision language model (MMVLM) targeted to run on mobile devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong</guid>
    </item>
    <item>
      <title>Video Understanding with Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/video-understanding-with-large-language</link>
      <description><![CDATA[With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-understanding-with-large-language</guid>
    </item>
    <item>
      <title>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</title>
      <link>https://paperswithcode.com/paper/atom-low-bit-quantization-for-efficient-and</link>
      <description><![CDATA[To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/atom-low-bit-quantization-for-efficient-and</guid>
    </item>
    <item>
      <title>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</title>
      <link>https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for</link>
      <description><![CDATA[This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for</guid>
    </item>
    <item>
      <title>HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting</title>
      <link>https://paperswithcode.com/paper/handrefiner-refining-malformed-hands-in</link>
      <description><![CDATA[Given a generated failed image due to malformed hands, we utilize ControlNet modules to re-inject such correct hand information.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/handrefiner-refining-malformed-hands-in</guid>
    </item>
    <item>
      <title>NEFTune: Noisy Embeddings Improve Instruction Finetuning</title>
      <link>https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</link>
      <description><![CDATA[We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</guid>
    </item>
    <item>
      <title>Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation</title>
      <link>https://paperswithcode.com/paper/auffusion-leveraging-the-power-of-diffusion</link>
      <description><![CDATA[Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/auffusion-leveraging-the-power-of-diffusion</guid>
    </item>
    <item>
      <title>TIES-Merging: Resolving Interference When Merging Models</title>
      <link>https://paperswithcode.com/paper/ties-merging-resolving-interference-when</link>
      <description><![CDATA[To address this, we propose our method, TRIM, ELECT SIGN & MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ties-merging-resolving-interference-when</guid>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <link>https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</link>
      <description><![CDATA[We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</guid>
    </item>
    <item>
      <title>Large Language Models for Generative Information Extraction: A Survey</title>
      <link>https://paperswithcode.com/paper/large-language-models-for-generative-1</link>
      <description><![CDATA[Information extraction (IE) aims to extract structural knowledge (such as entities, relations, and events) from plain natural language texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-models-for-generative-1</guid>
    </item>
    <item>
      <title>KwaiAgents: Generalized Information-seeking Agent System with Large Language Models</title>
      <link>https://paperswithcode.com/paper/kwaiagents-generalized-information-seeking</link>
      <description><![CDATA[Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kwaiagents-generalized-information-seeking</guid>
    </item>
    <item>
      <title>Learning Vision from Models Rivals Learning Vision from Data</title>
      <link>https://paperswithcode.com/paper/learning-vision-from-models-rivals-learning</link>
      <description><![CDATA[We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-vision-from-models-rivals-learning</guid>
    </item>
    <item>
      <title>WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia</title>
      <link>https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</link>
      <description><![CDATA[WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</guid>
    </item>
    <item>
      <title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</title>
      <link>https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</link>
      <description><![CDATA[To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</guid>
    </item>
  </channel>
</rss>
