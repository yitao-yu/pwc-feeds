<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 02 Oct 2023 21:06:27 +0000</lastBuildDate>
    <item>
      <title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title>
      <link>https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</link>
      <description><![CDATA[In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</guid>
    </item>
    <item>
      <title>Demystifying CLIP Data</title>
      <link>https://paperswithcode.com/paper/demystifying-clip-data</link>
      <description><![CDATA[We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demystifying-clip-data</guid>
    </item>
    <item>
      <title>Text-to-3D using Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/text-to-3d-using-gaussian-splatting</link>
      <description><![CDATA[In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-to-3d-using-gaussian-splatting</guid>
    </item>
    <item>
      <title>ProPainter: Improving Propagation and Transformer for Video Inpainting</title>
      <link>https://paperswithcode.com/paper/propainter-improving-propagation-and</link>
      <description><![CDATA[We also propose a mask-guided sparse video Transformer, which achieves high efficiency by discarding unnecessary and redundant tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/propainter-improving-propagation-and</guid>
    </item>
    <item>
      <title>Deep Geometrized Cartoon Line Inbetweening</title>
      <link>https://paperswithcode.com/paper/deep-geometrized-cartoon-line-inbetweening-1</link>
      <description><![CDATA[To preserve the precision and detail of the line drawings, we propose a new approach, AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-geometrized-cartoon-line-inbetweening-1</guid>
    </item>
    <item>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
      <link>https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</link>
      <description><![CDATA[Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</guid>
    </item>
    <item>
      <title>Communicative Agents for Software Development</title>
      <link>https://paperswithcode.com/paper/communicative-agents-for-software-development</link>
      <description><![CDATA[At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/communicative-agents-for-software-development</guid>
    </item>
    <item>
      <title>QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/qa-lora-quantization-aware-low-rank</link>
      <description><![CDATA[Recently years have witnessed a rapid development of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qa-lora-quantization-aware-low-rank</guid>
    </item>
    <item>
      <title>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</title>
      <link>https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long</link>
      <description><![CDATA[LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long</guid>
    </item>
    <item>
      <title>NExT-GPT: Any-to-Any Multimodal LLM</title>
      <link>https://paperswithcode.com/paper/next-gpt-any-to-any-multimodal-llm</link>
      <description><![CDATA[While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/next-gpt-any-to-any-multimodal-llm</guid>
    </item>
    <item>
      <title>ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs</title>
      <link>https://paperswithcode.com/paper/reconcile-round-table-conference-improves</link>
      <description><![CDATA[We also experiment with GPT-4 itself as one of the agents in ReConcile and demonstrate that its initial performance also improves by absolute 10. 0% through discussion and feedback from other agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reconcile-round-table-conference-improves</guid>
    </item>
    <item>
      <title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</title>
      <link>https://paperswithcode.com/paper/internlm-xcomposer-a-vision-language-large</link>
      <description><![CDATA[We propose InternLM-XComposer, a vision-language large model that enables advanced image-text comprehension and composition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internlm-xcomposer-a-vision-language-large</guid>
    </item>
    <item>
      <title>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</title>
      <link>https://paperswithcode.com/paper/demonstrate-search-predict-composing</link>
      <description><![CDATA[Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demonstrate-search-predict-composing</guid>
    </item>
    <item>
      <title>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</link>
      <description><![CDATA[In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</guid>
    </item>
    <item>
      <title>What Makes Good In-Context Examples for GPT-$3$?</title>
      <link>https://paperswithcode.com/paper/what-makes-good-in-context-examples-for-gpt-3</link>
      <description><![CDATA[Inspired by the recent success of leveraging a retrieval module to augment large-scale neural network models, we propose to retrieve examples that are semantically-similar to a test sample to formulate its corresponding prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-makes-good-in-context-examples-for-gpt-3</guid>
    </item>
    <item>
      <title>Agents: An Open-source Framework for Autonomous Language Agents</title>
      <link>https://paperswithcode.com/paper/agents-an-open-source-framework-for</link>
      <description><![CDATA[Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agents-an-open-source-framework-for</guid>
    </item>
    <item>
      <title>Boolformer: Symbolic Regression of Logic Functions with Transformers</title>
      <link>https://paperswithcode.com/paper/boolformer-symbolic-regression-of-logic</link>
      <description><![CDATA[In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/boolformer-symbolic-regression-of-logic</guid>
    </item>
    <item>
      <title>Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline</title>
      <link>https://paperswithcode.com/paper/towards-explainable-3d-grounded-visual</link>
      <description><![CDATA[We also propose a new 3D VQA framework to effectively predict the completely visually grounded and explainable answer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-explainable-3d-grounded-visual</guid>
    </item>
    <item>
      <title>Simple Online and Realtime Tracking</title>
      <link>https://paperswithcode.com/paper/simple-online-and-realtime-tracking</link>
      <description><![CDATA[This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-online-and-realtime-tracking</guid>
    </item>
    <item>
      <title>NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions</title>
      <link>https://paperswithcode.com/paper/neurbf-a-neural-fields-representation-with-1</link>
      <description><![CDATA[The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neurbf-a-neural-fields-representation-with-1</guid>
    </item>
  </channel>
</rss>
