<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 13 Jun 2023 21:06:12 +0000</lastBuildDate>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</title>
      <link>https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</link>
      <description><![CDATA[In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</guid>
    </item>
    <item>
      <title>Matting Anything</title>
      <link>https://paperswithcode.com/paper/matting-anything</link>
      <description><![CDATA[In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matting-anything</guid>
    </item>
    <item>
      <title>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</title>
      <link>https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</link>
      <description><![CDATA[Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</guid>
    </item>
    <item>
      <title>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</title>
      <link>https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</link>
      <description><![CDATA[Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</guid>
    </item>
    <item>
      <title>A Literature Study of Embeddings on Source Code</title>
      <link>https://paperswithcode.com/paper/a-literature-study-of-embeddings-on-source</link>
      <description><![CDATA[In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-literature-study-of-embeddings-on-source</guid>
    </item>
    <item>
      <title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</link>
      <description><![CDATA[We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</guid>
    </item>
    <item>
      <title>Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance</title>
      <link>https://paperswithcode.com/paper/chain-of-thought-hub-a-continuous-effort-to</link>
      <description><![CDATA[As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chain-of-thought-hub-a-continuous-effort-to</guid>
    </item>
    <item>
      <title>White-Box Transformers via Sparse Rate Reduction</title>
      <link>https://paperswithcode.com/paper/white-box-transformers-via-sparse-rate</link>
      <description><![CDATA[Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/white-box-transformers-via-sparse-rate</guid>
    </item>
    <item>
      <title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
      <link>https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</link>
      <description><![CDATA[For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</guid>
    </item>
    <item>
      <title>DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement</title>
      <link>https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</link>
      <description><![CDATA[Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</guid>
    </item>
    <item>
      <title>Segment Anything in High Quality</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-high-quality</link>
      <description><![CDATA[HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-high-quality</guid>
    </item>
    <item>
      <title>Matte Anything: Interactive Natural Image Matting with Segment Anything Models</title>
      <link>https://paperswithcode.com/paper/matte-anything-interactive-natural-image</link>
      <description><![CDATA[We leverage task-specific vision models to enhance the performance of natural image matting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matte-anything-interactive-natural-image</guid>
    </item>
    <item>
      <title>GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective</title>
      <link>https://paperswithcode.com/paper/glue-x-evaluating-natural-language</link>
      <description><![CDATA[Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glue-x-evaluating-natural-language</guid>
    </item>
    <item>
      <title>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</title>
      <link>https://paperswithcode.com/paper/rewoo-decoupling-reasoning-from-observations</link>
      <description><![CDATA[Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rewoo-decoupling-reasoning-from-observations</guid>
    </item>
    <item>
      <title>Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</title>
      <link>https://paperswithcode.com/paper/youku-mplug-a-10-million-large-scale-chinese</link>
      <description><![CDATA[In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/youku-mplug-a-10-million-large-scale-chinese</guid>
    </item>
    <item>
      <title>CodeTF: One-stop Transformer Library for State-of-the-art Code LLM</title>
      <link>https://paperswithcode.com/paper/codetf-one-stop-transformer-library-for-state</link>
      <description><![CDATA[In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codetf-one-stop-transformer-library-for-state</guid>
    </item>
    <item>
      <title>HUMAP: Hierarchical Uniform Manifold Approximation and Projection</title>
      <link>https://paperswithcode.com/paper/humap-hierarchical-uniform-manifold</link>
      <description><![CDATA[Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humap-hierarchical-uniform-manifold</guid>
    </item>
    <item>
      <title>Designing a Better Asymmetric VQGAN for StableDiffusion</title>
      <link>https://paperswithcode.com/paper/designing-a-better-asymmetric-vqgan-for</link>
      <description><![CDATA[The training cost of our asymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion unchanged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/designing-a-better-asymmetric-vqgan-for</guid>
    </item>
  </channel>
</rss>
