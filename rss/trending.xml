<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 01 Nov 2022 09:18:16 +0000</lastBuildDate>
    <item>
      <title>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models</title>
      <link>https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</link>
      <description><![CDATA[We analyze prompts in the dataset and discuss key properties of these prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</guid>
    </item>
    <item>
      <title>Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform</title>
      <link>https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</link>
      <description><![CDATA[We propose a lightweight end-to-end text-to-speech model using multi-band generation and inverse short-time Fourier transform.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</guid>
    </item>
    <item>
      <title>High Fidelity Neural Audio Compression</title>
      <link>https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</link>
      <description><![CDATA[We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</guid>
    </item>
    <item>
      <title>Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation</title>
      <link>https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</link>
      <description><![CDATA[In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Poisson Flow Generative Models</title>
      <link>https://paperswithcode.com/paper/poisson-flow-generative-models</link>
      <description><![CDATA[We interpret the data points as electrical charges on the $z=0$ hyperplane in a space augmented with an additional dimension $z$, generating a high-dimensional electric field (the gradient of the solution to Poisson equation).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/poisson-flow-generative-models</guid>
    </item>
    <item>
      <title>MetaFormer Baselines for Vision</title>
      <link>https://paperswithcode.com/paper/metaformer-baselines-for-vision</link>
      <description><![CDATA[By simply applying depthwise separable convolutions as token mixer in the bottom stages and vanilla self-attention in the top stages, the resulting model CAFormer sets a new record on ImageNet-1K: it achieves an accuracy of 85. 5% at 224x224 resolution, under normal supervised training without external data or distillation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metaformer-baselines-for-vision</guid>
    </item>
    <item>
      <title>What Makes Convolutional Models Great on Long Sequence Modeling?</title>
      <link>https://paperswithcode.com/paper/what-makes-convolutional-models-great-on-long</link>
      <description><![CDATA[We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-makes-convolutional-models-great-on-long</guid>
    </item>
    <item>
      <title>Referring Image Matting</title>
      <link>https://paperswithcode.com/paper/referring-image-matting</link>
      <description><![CDATA[Image matting refers to extracting the accurate foregrounds in the image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/referring-image-matting</guid>
    </item>
    <item>
      <title>Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations</title>
      <link>https://paperswithcode.com/paper/towards-high-quality-neural-tts-for-low</link>
      <description><![CDATA[Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-high-quality-neural-tts-for-low</guid>
    </item>
    <item>
      <title>Human Motion Diffusion Model</title>
      <link>https://paperswithcode.com/paper/human-motion-diffusion-model</link>
      <description><![CDATA[In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-motion-diffusion-model</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>VSA: Learning Varied-Size Window Attention in Vision Transformers</title>
      <link>https://paperswithcode.com/paper/vsa-learning-varied-size-window-attention-in</link>
      <description><![CDATA[Attention within windows has been widely explored in vision transformers to balance the performance, computation complexity, and memory footprint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vsa-learning-varied-size-window-attention-in</guid>
    </item>
    <item>
      <title>Rethinking Portrait Matting with Privacy Preserving</title>
      <link>https://paperswithcode.com/paper/rethinking-portrait-matting-with-privacy</link>
      <description><![CDATA[We systematically evaluate both trimap-free and trimap-based matting methods on P3M-10k and find that existing matting methods show different generalization abilities under the privacy preserving training setting, i. e., training only on face-blurred images while testing on arbitrary images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-portrait-matting-with-privacy</guid>
    </item>
    <item>
      <title>Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models</title>
      <link>https://paperswithcode.com/paper/adan-adaptive-nesterov-momentum-algorithm-for</link>
      <description><![CDATA[Then Adan adopts NME to estimate the first- and second-order moments of the gradient in adaptive gradient algorithms for convergence acceleration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adan-adaptive-nesterov-momentum-algorithm-for</guid>
    </item>
    <item>
      <title>Structure-based Drug Design with Equivariant Diffusion Models</title>
      <link>https://paperswithcode.com/paper/structure-based-drug-design-with-equivariant</link>
      <description><![CDATA[In this paper, we formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an E(3)-equivariant 3D-conditional diffusion model that generates novel ligands conditioned on protein pockets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structure-based-drug-design-with-equivariant</guid>
    </item>
    <item>
      <title>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</title>
      <link>https://paperswithcode.com/paper/meta-learning-a-real-time-tabular-automl</link>
      <description><![CDATA[We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-learning-a-real-time-tabular-automl</guid>
    </item>
    <item>
      <title>ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond</title>
      <link>https://paperswithcode.com/paper/vitaev2-vision-transformer-advanced-by</link>
      <description><![CDATA[Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vitaev2-vision-transformer-advanced-by</guid>
    </item>
    <item>
      <title>Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model</title>
      <link>https://paperswithcode.com/paper/advancing-plain-vision-transformer-towards</link>
      <description><![CDATA[In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models customized for RS tasks and explore how such large models perform.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advancing-plain-vision-transformer-towards</guid>
    </item>
    <item>
      <title>FEAR: Fast, Efficient, Accurate and Robust Visual Tracker</title>
      <link>https://paperswithcode.com/paper/fear-fast-efficient-accurate-and-robust</link>
      <description><![CDATA[In addition, we expand the definition of the model efficiency by introducing FEAR benchmark that assesses energy consumption and execution speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fear-fast-efficient-accurate-and-robust</guid>
    </item>
  </channel>
</rss>
