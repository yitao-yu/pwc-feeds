<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 11 Dec 2022 09:12:09 +0000</lastBuildDate>
    <item>
      <title>Learning Video Representations from Large Language Models</title>
      <link>https://paperswithcode.com/paper/learning-video-representations-from-large</link>
      <description><![CDATA[We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-video-representations-from-large</guid>
    </item>
    <item>
      <title>DAMO-YOLO : A Report on Real-Time Object Detection Design</title>
      <link>https://paperswithcode.com/paper/damo-yolo-a-report-on-real-time-object</link>
      <description><![CDATA[In this report, we present a fast and accurate object detection method dubbed DAMO-YOLO, which achieves higher performance than the state-of-the-art YOLO series.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/damo-yolo-a-report-on-real-time-object</guid>
    </item>
    <item>
      <title>Images Speak in Images: A Generalist Painter for In-Context Visual Learning</title>
      <link>https://paperswithcode.com/paper/images-speak-in-images-a-generalist-painter</link>
      <description><![CDATA[In this work, we present Painter, a generalist model which addresses these obstacles with an "image"-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/images-speak-in-images-a-generalist-painter</guid>
    </item>
    <item>
      <title>DiffusionInst: Diffusion Model for Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/diffusioninst-diffusion-model-for-instance</link>
      <description><![CDATA[This paper proposes DiffusionInst, a novel framework that represents instances as instance-aware filters and formulates instance segmentation as a noise-to-filter denoising process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusioninst-diffusion-model-for-instance</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</guid>
    </item>
    <item>
      <title>ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency</title>
      <link>https://paperswithcode.com/paper/ace-cooperative-multi-agent-q-learning-with</link>
      <description><![CDATA[In the learning phase, each agent minimizes the TD error that is dependent on how the subsequent agents have reacted to their chosen action.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ace-cooperative-multi-agent-q-learning-with</guid>
    </item>
    <item>
      <title>EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</title>
      <link>https://paperswithcode.com/paper/eva-exploring-the-limits-of-masked-visual</link>
      <description><![CDATA[We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eva-exploring-the-limits-of-masked-visual</guid>
    </item>
    <item>
      <title>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</title>
      <link>https://paperswithcode.com/paper/simvtp-simple-video-text-pre-training-with</link>
      <description><![CDATA[This paper presents SimVTP: a Simple Video-Text Pretraining framework via masked autoencoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simvtp-simple-video-text-pre-training-with</guid>
    </item>
    <item>
      <title>UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/unetr-delving-into-efficient-and-accurate-3d</link>
      <description><![CDATA[Owing to the success of transformer models, recent works study their applicability in 3D medical segmentation tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unetr-delving-into-efficient-and-accurate-3d</guid>
    </item>
    <item>
      <title>Fine-Tuning Language Models from Human Preferences</title>
      <link>https://paperswithcode.com/paper/fine-tuning-language-models-from-human</link>
      <description><![CDATA[Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-tuning-language-models-from-human</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
    <item>
      <title>Melody transcription via generative pre-training</title>
      <link>https://paperswithcode.com/paper/melody-transcription-via-generative-pre</link>
      <description><![CDATA[The combination of generative pre-training and a new dataset for this task results in $77$% stronger performance on melody transcription relative to the strongest available baseline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/melody-transcription-via-generative-pre</guid>
    </item>
    <item>
      <title>OpenFE: Automated Feature Generation beyond Expert-level Performance</title>
      <link>https://paperswithcode.com/paper/openfe-automated-feature-generation-beyond</link>
      <description><![CDATA[The major challenge in automated feature generation is to efficiently and accurately identify useful features from a vast pool of candidate features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openfe-automated-feature-generation-beyond</guid>
    </item>
    <item>
      <title>Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model</title>
      <link>https://paperswithcode.com/paper/zero-shot-image-restoration-using-denoising</link>
      <description><![CDATA[Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-image-restoration-using-denoising</guid>
    </item>
    <item>
      <title>Training language models to follow instructions with human feedback</title>
      <link>https://paperswithcode.com/paper/training-language-models-to-follow</link>
      <description><![CDATA[In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-language-models-to-follow</guid>
    </item>
    <item>
      <title>Programming Is Hard -- Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation</title>
      <link>https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</link>
      <description><![CDATA[The introductory programming sequence has been the focus of much research in computing education.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</guid>
    </item>
    <item>
      <title>InternVideo: General Video Foundation Models via Generative and Discriminative Learning</title>
      <link>https://paperswithcode.com/paper/internvideo-general-video-foundation-models</link>
      <description><![CDATA[Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internvideo-general-video-foundation-models</guid>
    </item>
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</link>
      <description><![CDATA[The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</guid>
    </item>
    <item>
      <title>Unifying Vision, Text, and Layout for Universal Document Processing</title>
      <link>https://paperswithcode.com/paper/unifying-vision-text-and-layout-for-universal</link>
      <description><![CDATA[UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unifying-vision-text-and-layout-for-universal</guid>
    </item>
    <item>
      <title>D$^2$LV: A Data-Driven and Local-Verification Approach for Image Copy Detection</title>
      <link>https://paperswithcode.com/paper/d-2lv-a-data-driven-and-local-verification</link>
      <description><![CDATA[In this paper, a data-driven and local-verification (D$^2$LV) approach is proposed to compete for Image Similarity Challenge: Matching Track at NeurIPS'21.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-2lv-a-data-driven-and-local-verification</guid>
    </item>
  </channel>
</rss>
