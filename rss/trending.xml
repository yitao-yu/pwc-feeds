<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 06 Jul 2022 09:16:01 +0000</lastBuildDate>
    <item>
      <title>Pen and Paper Exercises in Machine Learning</title>
      <link>https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</link>
      <description><![CDATA[This is a collection of (mostly) pen-and-paper exercises in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</guid>
    </item>
    <item>
      <title>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</title>
      <link>https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</link>
      <description><![CDATA[Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</guid>
    </item>
    <item>
      <title>DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech</title>
      <link>https://paperswithcode.com/paper/dailytalk-spoken-dialogue-dataset-for</link>
      <description><![CDATA[We sampled, modified, and recorded 2, 541 dialogues from the open-domain dialogue dataset DailyDialog which are adequately long to represent context of each dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dailytalk-spoken-dialogue-dataset-for</guid>
    </item>
    <item>
      <title>Back to MLP: A Simple Baseline for Human Motion Prediction</title>
      <link>https://paperswithcode.com/paper/back-to-mlp-a-simple-baseline-for-human</link>
      <description><![CDATA[This paper tackles the problem of human motion prediction, consisting in forecasting future body poses from historically observed sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/back-to-mlp-a-simple-baseline-for-human</guid>
    </item>
    <item>
      <title>LViT: Language meets Vision Transformer in Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/lvit-language-meets-vision-transformer-in</link>
      <description><![CDATA[In our model, medical text annotation is introduced to compensate for the quality deficiency in image data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lvit-language-meets-vision-transformer-in</guid>
    </item>
    <item>
      <title>Disentangling Random and Cyclic Effects in Time-Lapse Sequences</title>
      <link>https://paperswithcode.com/paper/disentangling-random-and-cyclic-effects-in</link>
      <description><![CDATA[We introduce the problem of disentangling time-lapse sequences in a way that allows separate, after-the-fact control of overall trends, cyclic effects, and random effects in the images, and describe a technique based on data-driven generative models that achieves this goal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/disentangling-random-and-cyclic-effects-in</guid>
    </item>
    <item>
      <title>Text2Human: Text-Driven Controllable Human Image Generation</title>
      <link>https://paperswithcode.com/paper/text2human-text-driven-controllable-human</link>
      <description><![CDATA[In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2human-text-driven-controllable-human</guid>
    </item>
    <item>
      <title>BoT-SORT: Robust Associations Multi-Pedestrian Tracking</title>
      <link>https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</link>
      <description><![CDATA[The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</guid>
    </item>
    <item>
      <title>Forecasting Future World Events with Neural Networks</title>
      <link>https://paperswithcode.com/paper/forecasting-future-world-events-with-neural</link>
      <description><![CDATA[We test language models on our forecasting task and find that performance is far below a human expert baseline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/forecasting-future-world-events-with-neural</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</title>
      <link>https://paperswithcode.com/paper/edgenext-efficiently-amalgamated-cnn</link>
      <description><![CDATA[Our EdgeNeXt model with 1. 3M parameters achieves 71. 2\% top-1 accuracy on ImageNet-1K, outperforming MobileViT with an absolute gain of 2. 2\% with 28\% reduction in FLOPs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/edgenext-efficiently-amalgamated-cnn</guid>
    </item>
    <item>
      <title>Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity</title>
      <link>https://paperswithcode.com/paper/accelerating-sparse-dnn-models-without</link>
      <description><![CDATA[Network pruning can reduce the high computation cost of deep neural network (DNN) models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/accelerating-sparse-dnn-models-without</guid>
    </item>
    <item>
      <title>TSM: Temporal Shift Module for Efficient Video Understanding</title>
      <link>https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video</link>
      <description><![CDATA[The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video</guid>
    </item>
    <item>
      <title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre</link>
      <description><![CDATA[Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre</guid>
    </item>
    <item>
      <title>Denoised MDPs: Learning World Models Better Than the World Itself</title>
      <link>https://paperswithcode.com/paper/denoised-mdps-learning-world-models-better</link>
      <description><![CDATA[The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/denoised-mdps-learning-world-models-better</guid>
    </item>
    <item>
      <title>LaMDA: Language Models for Dialog Applications</title>
      <link>https://paperswithcode.com/paper/lamda-language-models-for-dialog-applications</link>
      <description><![CDATA[We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lamda-language-models-for-dialog-applications</guid>
    </item>
    <item>
      <title>NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates</title>
      <link>https://paperswithcode.com/paper/nu-wave-2-a-general-neural-audio-upsampling</link>
      <description><![CDATA[Conventionally, audio super-resolution models fixed the initial and the target sampling rates, which necessitate the model to be trained for each pair of sampling rates.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nu-wave-2-a-general-neural-audio-upsampling</guid>
    </item>
    <item>
      <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</link>
      <description><![CDATA[We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</guid>
    </item>
    <item>
      <title>PolarFormer: Multi-camera 3D Object Detection with Polar Transformers</title>
      <link>https://paperswithcode.com/paper/polarformer-multi-camera-3d-object-detection</link>
      <description><![CDATA[3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/polarformer-multi-camera-3d-object-detection</guid>
    </item>
    <item>
      <title>Avocodo: Generative Adversarial Network for Artifact-free Vocoder</title>
      <link>https://paperswithcode.com/paper/avocodo-generative-adversarial-network-for</link>
      <description><![CDATA[Therefore, in this paper, we investigate the relationship between these artifacts and GAN-based neural vocoders and propose a GAN-based neural vocoder, called Avocodo, that allows the synthesis of high-fidelity speech with reduced artifacts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/avocodo-generative-adversarial-network-for</guid>
    </item>
  </channel>
</rss>
