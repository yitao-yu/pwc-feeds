<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 17 Nov 2024 09:15:41 +0000</lastBuildDate>
    <item>
      <title>Qwen2.5-Coder Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-5-coder-technical-report</link>
      <description><![CDATA[In this report, we introduce the Qwen2. 5-Coder series, a significant upgrade from its predecessor, CodeQwen1. 5.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-5-coder-technical-report</guid>
    </item>
    <item>
      <title>Watermark Anything with Localized Messages</title>
      <link>https://paperswithcode.com/paper/watermark-anything-with-localized-messages</link>
      <description><![CDATA[Image watermarking methods are not tailored to handle small watermarked areas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/watermark-anything-with-localized-messages</guid>
    </item>
    <item>
      <title>Docling Technical Report</title>
      <link>https://paperswithcode.com/paper/docling-technical-report</link>
      <description><![CDATA[This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/docling-technical-report</guid>
    </item>
    <item>
      <title>MinerU: An Open-Source Solution for Precise Document Content Extraction</title>
      <link>https://paperswithcode.com/paper/mineru-an-open-source-solution-for-precise</link>
      <description><![CDATA[Document content analysis has been a crucial research area in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mineru-an-open-source-solution-for-precise</guid>
    </item>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</link>
      <description><![CDATA[While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</guid>
    </item>
    <item>
      <title>Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement</title>
      <link>https://paperswithcode.com/paper/lingma-swe-gpt-an-open-development-process</link>
      <description><![CDATA[The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30. 20% of the GitHub issues, marking a significant improvement in automatic issue resolution (22. 76% relative improvement compared to Llama 3. 1 405B), approaching the performance of closed-source models (31. 80\% issues of GPT-4o resolved).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lingma-swe-gpt-an-open-development-process</guid>
    </item>
    <item>
      <title>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</title>
      <link>https://paperswithcode.com/paper/llm2clip-powerful-language-model-unlock</link>
      <description><![CDATA[In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm2clip-powerful-language-model-unlock</guid>
    </item>
    <item>
      <title>OmniGen: Unified Image Generation</title>
      <link>https://paperswithcode.com/paper/omnigen-unified-image-generation</link>
      <description><![CDATA[In this work, we introduce OmniGen, a new diffusion model for unified image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnigen-unified-image-generation</guid>
    </item>
    <item>
      <title>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</title>
      <link>https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</link>
      <description><![CDATA[To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</guid>
    </item>
    <item>
      <title>Autoregressive Models in Vision: A Survey</title>
      <link>https://paperswithcode.com/paper/autoregressive-models-in-vision-a-survey</link>
      <description><![CDATA[Autoregressive modeling has been a huge success in the field of natural language processing (NLP).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoregressive-models-in-vision-a-survey</guid>
    </item>
    <item>
      <title>Scaling Mesh Generation via Compressive Tokenization</title>
      <link>https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</link>
      <description><![CDATA[We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</guid>
    </item>
    <item>
      <title>TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks</title>
      <link>https://paperswithcode.com/paper/transformerranker-a-tool-for-efficiently</link>
      <description><![CDATA[Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transformerranker-a-tool-for-efficiently</guid>
    </item>
    <item>
      <title>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title>
      <link>https://paperswithcode.com/paper/svdqunat-absorbing-outliers-by-low-rank</link>
      <description><![CDATA[To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/svdqunat-absorbing-outliers-by-low-rank</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>SplatFormer: Point Transformer for Robust 3D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</link>
      <description><![CDATA[To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</guid>
    </item>
    <item>
      <title>EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture</title>
      <link>https://paperswithcode.com/paper/easyanimate-a-high-performance-long-video</link>
      <description><![CDATA[The motion module can be adapted to various DiT baseline methods to generate video with different styles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyanimate-a-high-performance-long-video</guid>
    </item>
    <item>
      <title>optillm</title>
      <link>https://github.com/codelion/optillm</link>
      <description><![CDATA[Optimizing inference proxy for LLMs]]></description>
      <guid isPermaLink="true">https://github.com/codelion/optillm</guid>
    </item>
    <item>
      <title>TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</title>
      <link>https://paperswithcode.com/paper/tokenformer-rethinking-transformer-scaling</link>
      <description><![CDATA[By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tokenformer-rethinking-transformer-scaling</guid>
    </item>
    <item>
      <title>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</title>
      <link>https://paperswithcode.com/paper/stylesinger-2-zero-shot-singing-voice</link>
      <description><![CDATA[To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stylesinger-2-zero-shot-singing-voice</guid>
    </item>
    <item>
      <title>MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation</title>
      <link>https://paperswithcode.com/paper/mba-slam-motion-blur-aware-dense-visual-slam</link>
      <description><![CDATA[In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mba-slam-motion-blur-aware-dense-visual-slam</guid>
    </item>
  </channel>
</rss>
