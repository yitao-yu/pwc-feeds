<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 14 Apr 2024 11:18:22 +0000</lastBuildDate>
    <item>
      <title>AutoCodeRover: Autonomous Program Improvement</title>
      <link>https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</link>
      <description><![CDATA[Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</guid>
    </item>
    <item>
      <title>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</title>
      <link>https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</link>
      <description><![CDATA[We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</guid>
    </item>
    <item>
      <title>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</title>
      <link>https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</link>
      <description><![CDATA[Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</title>
      <link>https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</link>
      <description><![CDATA[The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</guid>
    </item>
    <item>
      <title>InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</link>
      <description><![CDATA[Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</guid>
    </item>
    <item>
      <title>AIOS: LLM Agent Operating System</title>
      <link>https://paperswithcode.com/paper/llm-agent-operating-system</link>
      <description><![CDATA[Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system "with soul" -- an important step towards AGI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm-agent-operating-system</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>Rho-1: Not All Tokens Are What You Need</title>
      <link>https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</link>
      <description><![CDATA[After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40. 6% and 51. 8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</guid>
    </item>
    <item>
      <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
      <link>https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</link>
      <description><![CDATA[We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</guid>
    </item>
    <item>
      <title>OmniFusion Technical Report</title>
      <link>https://paperswithcode.com/paper/omnifusion-technical-report</link>
      <description><![CDATA[We propose an \textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnifusion-technical-report</guid>
    </item>
    <item>
      <title>Hash3D: Training-free Acceleration for 3D Generation</title>
      <link>https://paperswithcode.com/paper/hash3d-training-free-acceleration-for-3d</link>
      <description><![CDATA[The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hash3d-training-free-acceleration-for-3d</guid>
    </item>
    <item>
      <title>Video-Based Human Pose Regression via Decoupled Space-Time Aggregation</title>
      <link>https://paperswithcode.com/paper/video-based-human-pose-regression-via</link>
      <description><![CDATA[In light of this, we propose a novel Decoupled Space-Time Aggregation network (DSTA) to separately capture the spatial contexts between adjacent joints and the temporal cues of each individual joint, thereby avoiding the conflation of spatiotemporal dimensions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-based-human-pose-regression-via</guid>
    </item>
    <item>
      <title>HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach</title>
      <link>https://paperswithcode.com/paper/hairfastgan-realistic-and-robust-hair</link>
      <description><![CDATA[Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hairfastgan-realistic-and-robust-hair</guid>
    </item>
    <item>
      <title>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</title>
      <link>https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</link>
      <description><![CDATA[In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</guid>
    </item>
    <item>
      <title>ReFT: Representation Finetuning for Language Models</title>
      <link>https://paperswithcode.com/paper/reft-representation-finetuning-for-language</link>
      <description><![CDATA[LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reft-representation-finetuning-for-language</guid>
    </item>
    <item>
      <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
      <link>https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</link>
      <description><![CDATA[To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</guid>
    </item>
    <item>
      <title>Policy-Guided Diffusion</title>
      <link>https://paperswithcode.com/paper/policy-guided-diffusion</link>
      <description><![CDATA[Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/policy-guided-diffusion</guid>
    </item>
    <item>
      <title>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/cross-attention-makes-inference-cumbersome-in</link>
      <description><![CDATA[This study explores the role of cross-attention during inference in text-conditional diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cross-attention-makes-inference-cumbersome-in</guid>
    </item>
    <item>
      <title>Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</title>
      <link>https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</link>
      <description><![CDATA[To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</guid>
    </item>
  </channel>
</rss>
