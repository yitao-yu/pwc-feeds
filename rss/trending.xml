<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 26 Feb 2023 09:12:45 +0000</lastBuildDate>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language</link>
      <description><![CDATA[We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language</guid>
    </item>
    <item>
      <title>Multimodal Chain-of-Thought Reasoning in Language Models</title>
      <link>https://paperswithcode.com/paper/multimodal-chain-of-thought-reasoning-in</link>
      <description><![CDATA[Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-chain-of-thought-reasoning-in</guid>
    </item>
    <item>
      <title>Adding Conditional Control to Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/adding-conditional-control-to-text-to-image</link>
      <description><![CDATA[Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adding-conditional-control-to-text-to-image</guid>
    </item>
    <item>
      <title>Composer: Creative and Controllable Image Synthesis with Composable Conditions</title>
      <link>https://paperswithcode.com/paper/composer-creative-and-controllable-image</link>
      <description><![CDATA[Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/composer-creative-and-controllable-image</guid>
    </item>
    <item>
      <title>BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</title>
      <link>https://paperswithcode.com/paper/biogpt-generative-pre-trained-transformer-for</link>
      <description><![CDATA[Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/biogpt-generative-pre-trained-transformer-for</guid>
    </item>
    <item>
      <title>VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion</title>
      <link>https://paperswithcode.com/paper/voxformer-sparse-voxel-transformer-for-camera</link>
      <description><![CDATA[To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voxformer-sparse-voxel-transformer-for-camera</guid>
    </item>
    <item>
      <title>3D-aware Conditional Image Synthesis</title>
      <link>https://paperswithcode.com/paper/3d-aware-conditional-image-synthesis</link>
      <description><![CDATA[We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-aware-conditional-image-synthesis</guid>
    </item>
    <item>
      <title>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers</title>
      <link>https://paperswithcode.com/paper/why-can-gpt-learn-in-context-language-models</link>
      <description><![CDATA[In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/why-can-gpt-learn-in-context-language-models</guid>
    </item>
    <item>
      <title>AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</title>
      <link>https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</link>
      <description><![CDATA[In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</guid>
    </item>
    <item>
      <title>Colossal-Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models</title>
      <link>https://paperswithcode.com/paper/map-memory-aware-automated-intra-op-parallel</link>
      <description><![CDATA[To address these challenges, we introduce a system that can jointly optimize distributed execution and gradient checkpointing plans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/map-memory-aware-automated-intra-op-parallel</guid>
    </item>
    <item>
      <title>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering</title>
      <link>https://paperswithcode.com/paper/learn-to-explain-multimodal-reasoning-via</link>
      <description><![CDATA[We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learn-to-explain-multimodal-reasoning-via</guid>
    </item>
    <item>
      <title>T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/t2i-adapter-learning-adapters-to-dig-out-more</link>
      <description><![CDATA[The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t2i-adapter-learning-adapters-to-dig-out-more</guid>
    </item>
    <item>
      <title>Symbolic Discovery of Optimization Algorithms</title>
      <link>https://paperswithcode.com/paper/symbolic-discovery-of-optimization-algorithms</link>
      <description><![CDATA[We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/symbolic-discovery-of-optimization-algorithms</guid>
    </item>
    <item>
      <title>Learning Visual Representations via Language-Guided Sampling</title>
      <link>https://paperswithcode.com/paper/learning-visual-representations-via-language</link>
      <description><![CDATA[Although an object may appear in numerous contexts, we often describe it in a limited number of ways.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-visual-representations-via-language</guid>
    </item>
    <item>
      <title>Mastering Diverse Domains through World Models</title>
      <link>https://paperswithcode.com/paper/mastering-diverse-domains-through-world</link>
      <description><![CDATA[General intelligence requires solving tasks across many domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mastering-diverse-domains-through-world</guid>
    </item>
    <item>
      <title>MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</title>
      <link>https://paperswithcode.com/paper/multidiffusion-fusing-diffusion-paths-for</link>
      <description><![CDATA[In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multidiffusion-fusing-diffusion-paths-for</guid>
    </item>
    <item>
      <title>MarioGPT: Open-Ended Text2Level Generation through Large Language Models</title>
      <link>https://paperswithcode.com/paper/mariogpt-open-ended-text2level-generation</link>
      <description><![CDATA[Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mariogpt-open-ended-text2level-generation</guid>
    </item>
    <item>
      <title>HTNet: Human Topology Aware Network for 3D Human Pose Estimation</title>
      <link>https://paperswithcode.com/paper/htnet-human-topology-aware-network-for-3d</link>
      <description><![CDATA[3D human pose estimation errors would propagate along the human body topology and accumulate at the end joints of limbs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/htnet-human-topology-aware-network-for-3d</guid>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</link>
      <description><![CDATA[We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</guid>
    </item>
    <item>
      <title>Fine-Tuning Language Models from Human Preferences</title>
      <link>https://paperswithcode.com/paper/fine-tuning-language-models-from-human</link>
      <description><![CDATA[Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-tuning-language-models-from-human</guid>
    </item>
  </channel>
</rss>
