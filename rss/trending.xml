<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 25 Aug 2024 21:07:07 +0000</lastBuildDate>
    <item>
      <title>Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/show-o-one-single-transformer-to-unify</link>
      <description><![CDATA[We present a unified transformer, i. e., Show-o, that unifies multimodal understanding and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-o-one-single-transformer-to-unify</guid>
    </item>
    <item>
      <title>Automated Design of Agentic Systems</title>
      <link>https://paperswithcode.com/paper/automated-design-of-agentic-systems</link>
      <description><![CDATA[Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e. g. Chain-of-Thought, Self-Reflection, Toolformer).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automated-design-of-agentic-systems</guid>
    </item>
    <item>
      <title>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title>
      <link>https://paperswithcode.com/paper/longwriter-unleashing-10000-word-generation</link>
      <description><![CDATA[By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10, 000 words while maintaining output quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longwriter-unleashing-10000-word-generation</guid>
    </item>
    <item>
      <title>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</title>
      <link>https://paperswithcode.com/paper/the-ai-scientist-towards-fully-automated-open</link>
      <description><![CDATA[This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-ai-scientist-towards-fully-automated-open</guid>
    </item>
    <item>
      <title>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</title>
      <link>https://paperswithcode.com/paper/longvila-scaling-long-context-visual-language</link>
      <description><![CDATA[We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longvila-scaling-long-context-visual-language</guid>
    </item>
    <item>
      <title>GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/gaussianocc-fully-self-supervised-and</link>
      <description><![CDATA[We propose Gaussian Splatting from Voxel space (GSV) to leverage the fast rendering properties of Gaussian splatting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaussianocc-fully-self-supervised-and</guid>
    </item>
    <item>
      <title>FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance</title>
      <link>https://paperswithcode.com/paper/fancyvideo-towards-dynamic-and-consistent</link>
      <description><![CDATA[Then, TAR refines the correlation matrix between cross-frame textual conditions and latent features along the time dimension.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fancyvideo-towards-dynamic-and-consistent</guid>
    </item>
    <item>
      <title>OpenResearcher: Unleashing AI for Accelerated Scientific Research</title>
      <link>https://paperswithcode.com/paper/openresearcher-unleashing-ai-for-accelerated</link>
      <description><![CDATA[The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openresearcher-unleashing-ai-for-accelerated</guid>
    </item>
    <item>
      <title>Bilateral Reference for High-Resolution Dichotomous Image Segmentation</title>
      <link>https://paperswithcode.com/paper/bilateral-reference-for-high-resolution</link>
      <description><![CDATA[It comprises two essential components: the localization module (LM) and the reconstruction module (RM) with our proposed bilateral reference (BiRef).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bilateral-reference-for-high-resolution</guid>
    </item>
    <item>
      <title>Scalable Autoregressive Image Generation with Mamba</title>
      <link>https://paperswithcode.com/paper/scalable-autoregressive-image-generation-with</link>
      <description><![CDATA[On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2. 21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-autoregressive-image-generation-with</guid>
    </item>
    <item>
      <title>ControlNeXt: Powerful and Efficient Control for Image and Video Generation</title>
      <link>https://paperswithcode.com/paper/controlnext-powerful-and-efficient-control</link>
      <description><![CDATA[In this paper, we propose ControlNeXt: a powerful and efficient method for controllable image and video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controlnext-powerful-and-efficient-control</guid>
    </item>
    <item>
      <title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title>
      <link>https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</link>
      <description><![CDATA[Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</guid>
    </item>
    <item>
      <title>Text-Driven Image Editing via Learnable Regions</title>
      <link>https://paperswithcode.com/paper/text-driven-image-editing-via-learnable</link>
      <description><![CDATA[Language has emerged as a natural interface for image editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-driven-image-editing-via-learnable</guid>
    </item>
    <item>
      <title>UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling</title>
      <link>https://paperswithcode.com/paper/unibench-visual-reasoning-requires-rethinking</link>
      <description><![CDATA[To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unibench-visual-reasoning-requires-rethinking</guid>
    </item>
    <item>
      <title>RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/ragchecker-a-fine-grained-framework-for</link>
      <description><![CDATA[Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ragchecker-a-fine-grained-framework-for</guid>
    </item>
    <item>
      <title>RAGLAB: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/raglab-a-modular-and-research-oriented</link>
      <description><![CDATA[Leveraging RAGLAB, we conduct a fair comparison of 6 RAG algorithms across 10 benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/raglab-a-modular-and-research-oriented</guid>
    </item>
    <item>
      <title>Controllable Text Generation for Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/controllable-text-generation-for-large</link>
      <description><![CDATA[This paper systematically reviews the latest advancements in CTG for LLMs, offering a comprehensive definition of its core concepts and clarifying the requirements for control conditions and text quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controllable-text-generation-for-large</guid>
    </item>
    <item>
      <title>MixTex: Unambiguous Recognition Should Not Rely Solely on Real Data</title>
      <link>https://paperswithcode.com/paper/unambiguous-recognition-should-not-rely</link>
      <description><![CDATA[This paper introduces MixTex, an end-to-end LaTeX OCR model designed for low-bias multilingual recognition, along with its novel data collection method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unambiguous-recognition-should-not-rely</guid>
    </item>
    <item>
      <title>Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs</title>
      <link>https://paperswithcode.com/paper/2408-03010</link>
      <description><![CDATA[Recent advancements in Large Language Models (LLMs) have showcased their proficiency in answering natural language queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-03010</guid>
    </item>
    <item>
      <title>A Survey of Embodied Learning for Object-Centric Robotic Manipulation</title>
      <link>https://paperswithcode.com/paper/a-survey-of-embodied-learning-for-object</link>
      <description><![CDATA[Embodied learning for object-centric robotic manipulation is a rapidly developing and challenging area in embodied AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-embodied-learning-for-object</guid>
    </item>
  </channel>
</rss>
