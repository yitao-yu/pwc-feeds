<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 16 Jun 2023 09:11:44 +0000</lastBuildDate>
    <item>
      <title>FinGPT: Open-Source Financial Large Language Models</title>
      <link>https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</link>
      <description><![CDATA[While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</guid>
    </item>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</title>
      <link>https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</link>
      <description><![CDATA[In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</guid>
    </item>
    <item>
      <title>WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
      <link>https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</link>
      <description><![CDATA[We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</guid>
    </item>
    <item>
      <title>Augmenting Language Models with Long-Term Memory</title>
      <link>https://paperswithcode.com/paper/augmenting-language-models-with-long-term</link>
      <description><![CDATA[Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/augmenting-language-models-with-long-term</guid>
    </item>
    <item>
      <title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</link>
      <description><![CDATA[We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</guid>
    </item>
    <item>
      <title>High-Fidelity Audio Compression with Improved RVQGAN</title>
      <link>https://paperswithcode.com/paper/high-fidelity-audio-compression-with-improved</link>
      <description><![CDATA[Language models have been successfully used to model natural signals, such as images, speech, and music.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-audio-compression-with-improved</guid>
    </item>
    <item>
      <title>TART: A plug-and-play Transformer module for task-agnostic reasoning</title>
      <link>https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</link>
      <description><![CDATA[As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</guid>
    </item>
    <item>
      <title>Recognize Anything: A Strong Image Tagging Model</title>
      <link>https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</link>
      <description><![CDATA[We are releasing the RAM at \url{https://recognize-anything. github. io/} to foster the advancements of large models in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</guid>
    </item>
    <item>
      <title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
      <link>https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</link>
      <description><![CDATA[For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</guid>
    </item>
    <item>
      <title>Segment Anything in High Quality</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-high-quality</link>
      <description><![CDATA[HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-high-quality</guid>
    </item>
    <item>
      <title>FasterViT: Fast Vision Transformers with Hierarchical Attention</title>
      <link>https://paperswithcode.com/paper/fastervit-fast-vision-transformers-with</link>
      <description><![CDATA[At a high level, global self-attentions enable the efficient cross-window communication at lower costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastervit-fast-vision-transformers-with</guid>
    </item>
    <item>
      <title>Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow</title>
      <link>https://paperswithcode.com/paper/data-copilot-bridging-billions-of-data-and</link>
      <description><![CDATA[Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-copilot-bridging-billions-of-data-and</guid>
    </item>
    <item>
      <title>Matting Anything</title>
      <link>https://paperswithcode.com/paper/matting-anything</link>
      <description><![CDATA[In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matting-anything</guid>
    </item>
    <item>
      <title>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</title>
      <link>https://paperswithcode.com/paper/how-far-can-camels-go-exploring-the-state-of</link>
      <description><![CDATA[Our evaluations show that the best model in any given evaluation reaches on average 83% of ChatGPT performance, and 68% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-can-camels-go-exploring-the-state-of</guid>
    </item>
    <item>
      <title>WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
      <link>https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</link>
      <description><![CDATA[In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</guid>
    </item>
    <item>
      <title>Gorilla: Large Language Model Connected with Massive APIs</title>
      <link>https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</link>
      <description><![CDATA[Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</guid>
    </item>
    <item>
      <title>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</title>
      <link>https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</link>
      <description><![CDATA[Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</guid>
    </item>
    <item>
      <title>Aggregated Contextual Transformations for High-Resolution Image Inpainting</title>
      <link>https://paperswithcode.com/paper/aggregated-contextual-transformations-for</link>
      <description><![CDATA[For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aggregated-contextual-transformations-for</guid>
    </item>
    <item>
      <title>Matte Anything: Interactive Natural Image Matting with Segment Anything Models</title>
      <link>https://paperswithcode.com/paper/matte-anything-interactive-natural-image</link>
      <description><![CDATA[We leverage task-specific vision models to enhance the performance of natural image matting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matte-anything-interactive-natural-image</guid>
    </item>
  </channel>
</rss>
