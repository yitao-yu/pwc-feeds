<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 29 Aug 2023 21:06:02 +0000</lastBuildDate>
    <item>
      <title>Code Llama: Open Foundation Models for Code</title>
      <link>https://paperswithcode.com/paper/code-llama-open-foundation-models-for-code</link>
      <description><![CDATA[We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/code-llama-open-foundation-models-for-code</guid>
    </item>
    <item>
      <title>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</title>
      <link>https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</link>
      <description><![CDATA[We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</guid>
    </item>
    <item>
      <title>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</title>
      <link>https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</link>
      <description><![CDATA[We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</guid>
    </item>
    <item>
      <title>Prompt2Model: Generating Deployable Models from Natural Language Instructions</title>
      <link>https://paperswithcode.com/paper/prompt2model-generating-deployable-models</link>
      <description><![CDATA[In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompt2model-generating-deployable-models</guid>
    </item>
    <item>
      <title>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</title>
      <link>https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</link>
      <description><![CDATA[What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</guid>
    </item>
    <item>
      <title>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</title>
      <link>https://paperswithcode.com/paper/omniquant-omnidirectionally-calibrated</link>
      <description><![CDATA[To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omniquant-omnidirectionally-calibrated</guid>
    </item>
    <item>
      <title>A Survey on Large Language Model based Autonomous Agents</title>
      <link>https://paperswithcode.com/paper/a-survey-on-large-language-model-based</link>
      <description><![CDATA[Based on the previous studies, we also present several challenges and future directions in this field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-large-language-model-based</guid>
    </item>
    <item>
      <title>Dense Text-to-Image Generation with Attention Modulation</title>
      <link>https://paperswithcode.com/paper/dense-text-to-image-generation-with-attention</link>
      <description><![CDATA[To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dense-text-to-image-generation-with-attention</guid>
    </item>
    <item>
      <title>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</title>
      <link>https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</link>
      <description><![CDATA[In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</guid>
    </item>
    <item>
      <title>Relighting Neural Radiance Fields with Shadow and Highlight Hints</title>
      <link>https://paperswithcode.com/paper/relighting-neural-radiance-fields-with-shadow</link>
      <description><![CDATA[This paper presents a novel neural implicit radiance representation for free viewpoint relighting from a small set of unstructured photographs of an object lit by a moving point light source different from the view position.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/relighting-neural-radiance-fields-with-shadow</guid>
    </item>
    <item>
      <title>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</title>
      <link>https://paperswithcode.com/paper/wizardmath-empowering-mathematical-reasoning</link>
      <description><![CDATA[Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardmath-empowering-mathematical-reasoning</guid>
    </item>
    <item>
      <title>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</title>
      <link>https://paperswithcode.com/paper/scenimefy-learning-to-craft-anime-scene-via</link>
      <description><![CDATA[The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scenimefy-learning-to-craft-anime-scene-via</guid>
    </item>
    <item>
      <title>DiffusionTrack: Diffusion Model For Multi-Object Tracking</title>
      <link>https://paperswithcode.com/paper/diffusiontrack-diffusion-model-for-multi</link>
      <description><![CDATA[In inference, the model refines a set of paired randomly generated boxes to the detection and tracking results in a flexible one-step or multi-step denoising diffusion process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusiontrack-diffusion-model-for-multi</guid>
    </item>
    <item>
      <title>CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</title>
      <link>https://paperswithcode.com/paper/codef-content-deformation-fields-for</link>
      <description><![CDATA[We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i. e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e. g., the object shape) from the video. With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found at https://qiuyu96. github. io/CoDeF/.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codef-content-deformation-fields-for</guid>
    </item>
    <item>
      <title>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</title>
      <link>https://paperswithcode.com/paper/demonstrate-search-predict-composing</link>
      <description><![CDATA[Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demonstrate-search-predict-composing</guid>
    </item>
    <item>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
      <link>https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</link>
      <description><![CDATA[Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</guid>
    </item>
    <item>
      <title>TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition</title>
      <link>https://paperswithcode.com/paper/tf-icon-diffusion-based-training-free-cross</link>
      <description><![CDATA[In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tf-icon-diffusion-based-training-free-cross</guid>
    </item>
    <item>
      <title>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</title>
      <link>https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</link>
      <description><![CDATA[Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</guid>
    </item>
    <item>
      <title>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</title>
      <link>https://paperswithcode.com/paper/audioldm-2-learning-holistic-audio-generation</link>
      <description><![CDATA[Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audioldm-2-learning-holistic-audio-generation</guid>
    </item>
    <item>
      <title>SoTaNa: The Open-Source Software Development Assistant</title>
      <link>https://paperswithcode.com/paper/sotana-the-open-source-software-development</link>
      <description><![CDATA[To meet the demands of this dynamic field, there is a growing need for an effective software development assistant.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sotana-the-open-source-software-development</guid>
    </item>
  </channel>
</rss>
