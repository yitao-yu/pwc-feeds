<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 03 Apr 2024 09:12:50 +0000</lastBuildDate>
    <item>
      <title>VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</title>
      <link>https://paperswithcode.com/paper/voicecraft-zero-shot-speech-editing-and-text</link>
      <description><![CDATA[We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voicecraft-zero-shot-speech-editing-and-text</guid>
    </item>
    <item>
      <title>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</title>
      <link>https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</link>
      <description><![CDATA[In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</guid>
    </item>
    <item>
      <title>AIOS: LLM Agent Operating System</title>
      <link>https://paperswithcode.com/paper/llm-agent-operating-system</link>
      <description><![CDATA[Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system "with soul" -- an important step towards AGI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm-agent-operating-system</guid>
    </item>
    <item>
      <title>BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion</title>
      <link>https://paperswithcode.com/paper/brushnet-a-plug-and-play-image-inpainting</link>
      <description><![CDATA[Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of diffusion models (DMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/brushnet-a-plug-and-play-image-inpainting</guid>
    </item>
    <item>
      <title>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</title>
      <link>https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</link>
      <description><![CDATA[We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i. e., high-resolution visual tokens, high-quality data, and VLM-guided generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</guid>
    </item>
    <item>
      <title>Long-form factuality in large language models</title>
      <link>https://paperswithcode.com/paper/long-form-factuality-in-large-language-models</link>
      <description><![CDATA[Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/long-form-factuality-in-large-language-models</guid>
    </item>
    <item>
      <title>UniDepth: Universal Monocular Metric Depth Estimation</title>
      <link>https://paperswithcode.com/paper/unidepth-universal-monocular-metric-depth</link>
      <description><![CDATA[However, the remarkable accuracy of recent MMDE methods is confined to their training domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unidepth-universal-monocular-metric-depth</guid>
    </item>
    <item>
      <title>Arc2Face: A Foundation Model of Human Faces</title>
      <link>https://paperswithcode.com/paper/arc2face-a-foundation-model-of-human-faces</link>
      <description><![CDATA[This paper presents Arc2Face, an identity-conditioned face foundation model, which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of face similarity than existing models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/arc2face-a-foundation-model-of-human-faces</guid>
    </item>
    <item>
      <title>RSMamba: Remote Sensing Image Classification with State Space Model</title>
      <link>https://paperswithcode.com/paper/rsmamba-remote-sensing-image-classification</link>
      <description><![CDATA[Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rsmamba-remote-sensing-image-classification</guid>
    </item>
    <item>
      <title>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</title>
      <link>https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</link>
      <description><![CDATA[We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0. 1s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</guid>
    </item>
    <item>
      <title>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</title>
      <link>https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</link>
      <description><![CDATA[Sora is the first large-scale generalist video generation model that garnered significant attention across society.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</guid>
    </item>
    <item>
      <title>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</title>
      <link>https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</link>
      <description><![CDATA[Recognizing the complementary strengths and weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes both prompts within a single model through contrastive learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</guid>
    </item>
    <item>
      <title>SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</title>
      <link>https://paperswithcode.com/paper/sdxs-real-time-one-step-latent-diffusion</link>
      <description><![CDATA[Recent advancements in diffusion models have positioned them at the forefront of image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sdxs-real-time-one-step-latent-diffusion</guid>
    </item>
    <item>
      <title>One-Step Image Translation with Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</link>
      <description><![CDATA[In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</guid>
    </item>
    <item>
      <title>Adaptive Super Resolution For One-Shot Talking-Head Generation</title>
      <link>https://paperswithcode.com/paper/adaptive-super-resolution-for-one-shot</link>
      <description><![CDATA[In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptive-super-resolution-for-one-shot</guid>
    </item>
    <item>
      <title>GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond</title>
      <link>https://paperswithcode.com/paper/gaustudio-a-modular-framework-for-3d-gaussian</link>
      <description><![CDATA[We present GauStudio, a novel modular framework for modeling 3D Gaussian Splatting (3DGS) to provide standardized, plug-and-play components for users to easily customize and implement a 3DGS pipeline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaustudio-a-modular-framework-for-3d-gaussian</guid>
    </item>
    <item>
      <title>LITA: Language Instructed Temporal-Localization Assistant</title>
      <link>https://paperswithcode.com/paper/lita-language-instructed-temporal</link>
      <description><![CDATA[In addition to leveraging existing video datasets with timestamps, we propose a new task, Reasoning Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for learning and evaluating this task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lita-language-instructed-temporal</guid>
    </item>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity Talking Head Generation</title>
      <link>https://paperswithcode.com/paper/moditalker-motion-disentangled-diffusion</link>
      <description><![CDATA[AToM excels in capturing subtle lip movements by leveraging an audio attention mechanism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moditalker-motion-disentangled-diffusion</guid>
    </item>
    <item>
      <title>Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians</title>
      <link>https://paperswithcode.com/paper/octree-gs-towards-consistent-real-time</link>
      <description><![CDATA[The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/octree-gs-towards-consistent-real-time</guid>
    </item>
  </channel>
</rss>
