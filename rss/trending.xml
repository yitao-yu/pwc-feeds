<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 27 Jan 2025 21:08:51 +0000</lastBuildDate>
    <item>
      <title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title>
      <link>https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</link>
      <description><![CDATA[This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</guid>
    </item>
    <item>
      <title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title>
      <link>https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</link>
      <description><![CDATA[This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e. g., keyboard and mouse operations).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</guid>
    </item>
    <item>
      <title>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</title>
      <link>https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</link>
      <description><![CDATA[The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/go-with-the-flow-motion-controllable-video</guid>
    </item>
    <item>
      <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding</title>
      <link>https://paperswithcode.com/paper/videollama-3-frontier-multimodal-foundation</link>
      <description><![CDATA[The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videollama-3-frontier-multimodal-foundation</guid>
    </item>
    <item>
      <title>DeepSeek-V3 Technical Report</title>
      <link>https://paperswithcode.com/paper/deepseek-v3-technical-report</link>
      <description><![CDATA[We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-v3-technical-report</guid>
    </item>
    <item>
      <title>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</title>
      <link>https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language</link>
      <description><![CDATA[The rapid development of open-source large language models (LLMs) has been truly remarkable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language</guid>
    </item>
    <item>
      <title>IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems</title>
      <link>https://paperswithcode.com/paper/intellagent-a-multi-agent-framework-for</link>
      <description><![CDATA[IntellAgent represents a paradigm shift in evaluating conversational AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intellagent-a-multi-agent-framework-for</guid>
    </item>
    <item>
      <title>PaSa: An LLM Agent for Comprehensive Academic Paper Search</title>
      <link>https://paperswithcode.com/paper/pasa-an-llm-agent-for-comprehensive-academic</link>
      <description><![CDATA[Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37. 78% in recall@20 and 39. 90% in recall@50.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pasa-an-llm-agent-for-comprehensive-academic</guid>
    </item>
    <item>
      <title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title>
      <link>https://paperswithcode.com/paper/can-we-generate-images-with-cot-let-s-verify</link>
      <description><![CDATA[We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-we-generate-images-with-cot-let-s-verify</guid>
    </item>
    <item>
      <title>DiffuEraser: A Diffusion Model for Video Inpainting</title>
      <link>https://paperswithcode.com/paper/diffueraser-a-diffusion-model-for-video</link>
      <description><![CDATA[Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffueraser-a-diffusion-model-for-video</guid>
    </item>
    <item>
      <title>FoundationStereo: Zero-Shot Stereo Matching</title>
      <link>https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</link>
      <description><![CDATA[However, achieving strong zero-shot generalization - a hallmark of foundation models in other computer vision tasks - remains challenging for stereo matching.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</guid>
    </item>
    <item>
      <title>X-Dyna: Expressive Dynamic Human Image Animation</title>
      <link>https://paperswithcode.com/paper/x-dyna-expressive-dynamic-human-image</link>
      <description><![CDATA[At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-dyna-expressive-dynamic-human-image</guid>
    </item>
    <item>
      <title>Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization</title>
      <link>https://paperswithcode.com/paper/unfolding-the-headline-iterative-self</link>
      <description><![CDATA[In the fast-changing realm of information, the capacity to construct coherent timelines from extensive event-related content has become increasingly significant and challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unfolding-the-headline-iterative-self</guid>
    </item>
    <item>
      <title>Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback</title>
      <link>https://paperswithcode.com/paper/align-anything-training-all-modality-models</link>
      <description><![CDATA[In this work, we make the first attempt to fine-tune all-modality models (i. e. input and output with any modality, also named any-to-any models) using human preference data across all modalities (including text, image, audio, and video), ensuring its behavior aligns with human intentions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/align-anything-training-all-modality-models</guid>
    </item>
    <item>
      <title>HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/hdr-gs-efficient-high-dynamic-range-novel</link>
      <description><![CDATA[In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hdr-gs-efficient-high-dynamic-range-novel</guid>
    </item>
    <item>
      <title>CameraHMR: Aligning People with Perspective</title>
      <link>https://paperswithcode.com/paper/camerahmr-aligning-people-with-perspective</link>
      <description><![CDATA[We use the estimated intrinsics to enhance the 4D-Humans dataset by incorporating a full perspective camera model during SMPLify fitting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/camerahmr-aligning-people-with-perspective</guid>
    </item>
    <item>
      <title>Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG</title>
      <link>https://paperswithcode.com/paper/agentic-retrieval-augmented-generation-a</link>
      <description><![CDATA[Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentic-retrieval-augmented-generation-a</guid>
    </item>
    <item>
      <title>WebWalker: Benchmarking LLMs in Web Traversal</title>
      <link>https://paperswithcode.com/paper/webwalker-benchmarking-llms-in-web-traversal</link>
      <description><![CDATA[Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webwalker-benchmarking-llms-in-web-traversal</guid>
    </item>
    <item>
      <title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</link>
      <description><![CDATA[Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</guid>
    </item>
    <item>
      <title>Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks</title>
      <link>https://paperswithcode.com/paper/hallo3-highly-dynamic-and-realistic-portrait</link>
      <description><![CDATA[Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hallo3-highly-dynamic-and-realistic-portrait</guid>
    </item>
  </channel>
</rss>
