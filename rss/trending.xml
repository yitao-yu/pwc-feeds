<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 28 Apr 2025 21:09:01 +0000</lastBuildDate>
    <item>
      <title>Packing Input Frame Context in Next-Frame Prediction Models for Video Generation</title>
      <link>https://paperswithcode.com/paper/packing-input-frame-context-in-next-frame</link>
      <description><![CDATA[We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/packing-input-frame-context-in-next-frame</guid>
    </item>
    <item>
      <title>InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework</title>
      <link>https://paperswithcode.com/paper/instantcharacter-personalize-any-characters</link>
      <description><![CDATA[Third, to effectively train the framework, we construct a large-scale character dataset containing 10-million-level samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantcharacter-personalize-any-characters</guid>
    </item>
    <item>
      <title>Reinforcement Learning from Human Feedback</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-from-human-feedback-4</link>
      <description><![CDATA[Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-from-human-feedback-4</guid>
    </item>
    <item>
      <title>UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer</title>
      <link>https://paperswithcode.com/paper/unianimate-dit-human-image-animation-with</link>
      <description><![CDATA[Furthermore, we adopt a simple concatenation operation to integrate the reference appearance into the model and incorporate the pose information of the reference image for enhanced pose alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unianimate-dit-human-image-animation-with</guid>
    </item>
    <item>
      <title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title>
      <link>https://paperswithcode.com/paper/prima-cpp-speeding-up-70b-scale-llm-inference</link>
      <description><![CDATA[Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prima-cpp-speeding-up-70b-scale-llm-inference</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
    <item>
      <title>Bitnet.cpp: Efficient Edge Inference for Ternary LLMs</title>
      <link>https://paperswithcode.com/paper/bitnet-cpp-efficient-edge-inference-for</link>
      <description><![CDATA[The advent of 1-bit large language models (LLMs), led by BitNet b1. 58, has spurred interest in ternary LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bitnet-cpp-efficient-edge-inference-for</guid>
    </item>
    <item>
      <title>LettuceDetect: A Hallucination Detection Framework for RAG Applications</title>
      <link>https://paperswithcode.com/paper/lettucedetect-a-hallucination-detection</link>
      <description><![CDATA[Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lettucedetect-a-hallucination-detection</guid>
    </item>
    <item>
      <title>BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence</title>
      <link>https://paperswithcode.com/paper/bip3d-bridging-2d-images-and-3d-perception</link>
      <description><![CDATA[In embodied intelligence systems, a key component is 3D perception algorithm, which enables agents to understand their surrounding environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bip3d-bridging-2d-images-and-3d-perception</guid>
    </item>
    <item>
      <title>Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion</title>
      <link>https://paperswithcode.com/paper/advanced-video-inpainting-using-optical-flow</link>
      <description><![CDATA[Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advanced-video-inpainting-using-optical-flow</guid>
    </item>
    <item>
      <title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title>
      <link>https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</link>
      <description><![CDATA[This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e. g., keyboard and mouse operations).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ui-tars-pioneering-automated-gui-interaction</guid>
    </item>
    <item>
      <title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
      <link>https://paperswithcode.com/paper/the-ai-scientist-v2-workshop-level-automated</link>
      <description><![CDATA[AI is increasingly playing a pivotal role in transforming how scientific discoveries are made.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-ai-scientist-v2-workshop-level-automated</guid>
    </item>
    <item>
      <title>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</title>
      <link>https://paperswithcode.com/paper/locomujoco-a-comprehensive-imitation-learning</link>
      <description><![CDATA[Imitation Learning (IL) holds great promise for enabling agile locomotion in embodied agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/locomujoco-a-comprehensive-imitation-learning</guid>
    </item>
    <item>
      <title>Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems</title>
      <link>https://paperswithcode.com/paper/advances-and-challenges-in-foundation-agents</link>
      <description><![CDATA[The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advances-and-challenges-in-foundation-agents</guid>
    </item>
    <item>
      <title>REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/repa-e-unlocking-vae-for-end-to-end-tuning</link>
      <description><![CDATA[We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repa-e-unlocking-vae-for-end-to-end-tuning</guid>
    </item>
    <item>
      <title>Event-Enhanced Blurry Video Super-Resolution</title>
      <link>https://paperswithcode.com/paper/event-enhanced-blurry-video-super-resolution</link>
      <description><![CDATA[In this paper, we tackle the task of blurry video super-resolution (BVSR), aiming to generate high-resolution (HR) videos from low-resolution (LR) and blurry inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/event-enhanced-blurry-video-super-resolution</guid>
    </item>
    <item>
      <title>LTX-Video: Realtime Video Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</link>
      <description><![CDATA[To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</guid>
    </item>
    <item>
      <title>Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</title>
      <link>https://paperswithcode.com/paper/syzygy-of-thoughts-improving-llm-cot-with-the</link>
      <description><![CDATA[Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/syzygy-of-thoughts-improving-llm-cot-with-the</guid>
    </item>
    <item>
      <title>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL</title>
      <link>https://paperswithcode.com/paper/simplear-pushing-the-frontier-of</link>
      <description><![CDATA[This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simplear-pushing-the-frontier-of</guid>
    </item>
    <item>
      <title>UniK3D: Universal Camera Monocular 3D Estimation</title>
      <link>https://paperswithcode.com/paper/unik3d-universal-camera-monocular-3d</link>
      <description><![CDATA[Monocular 3D estimation is crucial for visual perception.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unik3d-universal-camera-monocular-3d</guid>
    </item>
  </channel>
</rss>
