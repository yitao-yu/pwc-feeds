<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 17 Aug 2024 21:07:10 +0000</lastBuildDate>
    <item>
      <title>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</title>
      <link>https://paperswithcode.com/paper/the-ai-scientist-towards-fully-automated-open</link>
      <description><![CDATA[This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-ai-scientist-towards-fully-automated-open</guid>
    </item>
    <item>
      <title>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title>
      <link>https://paperswithcode.com/paper/longwriter-unleashing-10000-word-generation</link>
      <description><![CDATA[By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10, 000 words while maintaining output quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longwriter-unleashing-10000-word-generation</guid>
    </item>
    <item>
      <title>ControlNeXt: Powerful and Efficient Control for Image and Video Generation</title>
      <link>https://paperswithcode.com/paper/controlnext-powerful-and-efficient-control</link>
      <description><![CDATA[In this paper, we propose ControlNeXt: a powerful and efficient method for controllable image and video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controlnext-powerful-and-efficient-control</guid>
    </item>
    <item>
      <title>Zero-Shot Surgical Tool Segmentation in Monocular Video Using Segment Anything Model 2</title>
      <link>https://paperswithcode.com/paper/2408-01648</link>
      <description><![CDATA[The Segment Anything Model 2 (SAM 2) is the latest generation foundation model for image and video segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-01648</guid>
    </item>
    <item>
      <title>OpenResearcher: Unleashing AI for Accelerated Scientific Research</title>
      <link>https://paperswithcode.com/paper/openresearcher-unleashing-ai-for-accelerated</link>
      <description><![CDATA[The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openresearcher-unleashing-ai-for-accelerated</guid>
    </item>
    <item>
      <title>FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework</title>
      <link>https://paperswithcode.com/paper/fruitnerf-a-unified-neural-radiance-field</link>
      <description><![CDATA[We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fruitnerf-a-unified-neural-radiance-field</guid>
    </item>
    <item>
      <title>Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</title>
      <link>https://paperswithcode.com/paper/samba-simple-hybrid-state-space-models-for</link>
      <description><![CDATA[When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/samba-simple-hybrid-state-space-models-for</guid>
    </item>
    <item>
      <title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title>
      <link>https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</link>
      <description><![CDATA[Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</guid>
    </item>
    <item>
      <title>Qwen2-Audio Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-audio-technical-report</link>
      <description><![CDATA[We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-audio-technical-report</guid>
    </item>
    <item>
      <title>AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology</title>
      <link>https://paperswithcode.com/paper/agilecoder-dynamic-collaborative-agents-for</link>
      <description><![CDATA[Software agents have emerged as promising tools for addressing complex software engineering tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agilecoder-dynamic-collaborative-agents-for</guid>
    </item>
    <item>
      <title>BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search</title>
      <link>https://paperswithcode.com/paper/bmx-entropy-weighted-similarity-and-semantic</link>
      <description><![CDATA[BM25, a widely-used lexical search algorithm, remains crucial in information retrieval despite the rise of pre-trained and large language models (PLMs/LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bmx-entropy-weighted-similarity-and-semantic</guid>
    </item>
    <item>
      <title>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</title>
      <link>https://paperswithcode.com/paper/llava-next-interleave-tackling-multi-image</link>
      <description><![CDATA[To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-next-interleave-tackling-multi-image</guid>
    </item>
    <item>
      <title>1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data</title>
      <link>https://paperswithcode.com/paper/1-5-pints-technical-report-pretraining-in</link>
      <description><![CDATA[This paper presents a compute-efficient approach to pre-training a Language Model-the "1. 5-Pints"-in only 9 days, while outperforming state-of-the-art models as an instruction-following assistant. Based on MT-Bench (a benchmark that emulates human judgments), 1. 5-Pints outperforms Apple's OpenELM and Microsoft's Phi. This is achieved by a carefully curated pre-training dataset of 57 billion tokens, using a mix of automated workflows and manual human review.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/1-5-pints-technical-report-pretraining-in</guid>
    </item>
    <item>
      <title>T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge</title>
      <link>https://paperswithcode.com/paper/t-mac-cpu-renaissance-via-table-lookup-for</link>
      <description><![CDATA[Weight quantization is crucial for reducing the memory footprint of LLMs on devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t-mac-cpu-renaissance-via-table-lookup-for</guid>
    </item>
    <item>
      <title>FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title>
      <link>https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</link>
      <description><![CDATA[This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</guid>
    </item>
    <item>
      <title>Accelerating High-Fidelity Waveform Generation via Adversarial Flow Matching Optimization</title>
      <link>https://paperswithcode.com/paper/accelerating-high-fidelity-waveform</link>
      <description><![CDATA[This paper introduces PeriodWave-Turbo, a high-fidelity and high-efficient waveform generation model via adversarial flow matching optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/accelerating-high-fidelity-waveform</guid>
    </item>
    <item>
      <title>MooER: LLM-based Speech Recognition and Translation Models from Moore Threads</title>
      <link>https://paperswithcode.com/paper/mooer-llm-based-speech-recognition-and</link>
      <description><![CDATA[We achieve performance comparable to other open source models trained with up to hundreds of thousands of hours of labeled speech data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mooer-llm-based-speech-recognition-and</guid>
    </item>
    <item>
      <title>UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling</title>
      <link>https://paperswithcode.com/paper/unibench-visual-reasoning-requires-rethinking</link>
      <description><![CDATA[To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unibench-visual-reasoning-requires-rethinking</guid>
    </item>
    <item>
      <title>VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents</title>
      <link>https://paperswithcode.com/paper/visualagentbench-towards-large-multimodal</link>
      <description><![CDATA[Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visualagentbench-towards-large-multimodal</guid>
    </item>
    <item>
      <title>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2408-02545</link>
      <description><![CDATA[We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-02545</guid>
    </item>
  </channel>
</rss>
