<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 21 May 2025 21:10:03 +0000</lastBuildDate>
    <item>
      <title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
      <link>https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</link>
      <description><![CDATA[Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</guid>
    </item>
    <item>
      <title>Aligning Anime Video Generation with Human Feedback</title>
      <link>https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</link>
      <description><![CDATA[Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</guid>
    </item>
    <item>
      <title>AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment</title>
      <link>https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</link>
      <description><![CDATA[In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</guid>
    </item>
    <item>
      <title>Parallel Scaling Law for Language Models</title>
      <link>https://paperswithcode.com/paper/parallel-scaling-law-for-language-models</link>
      <description><![CDATA[We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/parallel-scaling-law-for-language-models</guid>
    </item>
    <item>
      <title>Fully Open Source Moxin-7B Technical Report</title>
      <link>https://paperswithcode.com/paper/fully-open-source-moxin-7b-technical-report</link>
      <description><![CDATA[Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fully-open-source-moxin-7b-technical-report</guid>
    </item>
    <item>
      <title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
      <link>https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</link>
      <description><![CDATA[At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</guid>
    </item>
    <item>
      <title>Thinkless: LLM Learns When to Think</title>
      <link>https://paperswithcode.com/paper/thinkless-llm-learns-when-to-think</link>
      <description><![CDATA[Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/thinkless-llm-learns-when-to-think</guid>
    </item>
    <item>
      <title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/openthinkimg-learning-to-think-with-images</link>
      <description><![CDATA[We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openthinkimg-learning-to-think-with-images</guid>
    </item>
    <item>
      <title>MASS: Multi-Agent Simulation Scaling for Portfolio Construction</title>
      <link>https://paperswithcode.com/paper/mass-multi-agent-simulation-scaling-for</link>
      <description><![CDATA[LLM-based multi-agent has gained significant attention for their potential in simulation and enhancing performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mass-multi-agent-simulation-scaling-for</guid>
    </item>
    <item>
      <title>WorldPM: Scaling Human Preference Modeling</title>
      <link>https://paperswithcode.com/paper/worldpm-scaling-human-preference-modeling</link>
      <description><![CDATA[Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/worldpm-scaling-human-preference-modeling</guid>
    </item>
    <item>
      <title>SOAP: Style-Omniscient Animatable Portraits</title>
      <link>https://paperswithcode.com/paper/soap-style-omniscient-animatable-portraits</link>
      <description><![CDATA[Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soap-style-omniscient-animatable-portraits</guid>
    </item>
    <item>
      <title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
      <link>https://paperswithcode.com/paper/paper2code-automating-code-generation-from</link>
      <description><![CDATA[Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paper2code-automating-code-generation-from</guid>
    </item>
    <item>
      <title>Continuous Thought Machines</title>
      <link>https://paperswithcode.com/paper/continuous-thought-machines</link>
      <description><![CDATA[The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/continuous-thought-machines</guid>
    </item>
    <item>
      <title>HealthBench: Evaluating Large Language Models Towards Improved Human Health</title>
      <link>https://paperswithcode.com/paper/healthbench-evaluating-large-language-models</link>
      <description><![CDATA[We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/healthbench-evaluating-large-language-models</guid>
    </item>
    <item>
      <title>AlphaNet: Scaling Up Local-frame-based Atomistic Interatomic Potential</title>
      <link>https://paperswithcode.com/paper/alphanet-scaling-up-local-frame-based</link>
      <description><![CDATA[Molecular dynamics simulations demand an unprecedented combination of accuracy and scalability to tackle grand challenges in catalysis and materials design.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphanet-scaling-up-local-frame-based</guid>
    </item>
    <item>
      <title>Spherical Channels for Modeling Atomic Interactions</title>
      <link>https://paperswithcode.com/paper/spherical-channels-for-modeling-atomic</link>
      <description><![CDATA[We propose the Spherical Channel Network (SCN) to model atomic energies and forces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spherical-channels-for-modeling-atomic</guid>
    </item>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</link>
      <description><![CDATA[Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</guid>
    </item>
    <item>
      <title>SkyReels-V2: Infinite-length Film Generative Model</title>
      <link>https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</link>
      <description><![CDATA[Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</guid>
    </item>
    <item>
      <title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
      <link>https://paperswithcode.com/paper/mtvcrafter-4d-motion-tokenization-for-open</link>
      <description><![CDATA[To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i. e., 4D motion) for human image animation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mtvcrafter-4d-motion-tokenization-for-open</guid>
    </item>
    <item>
      <title>Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/finetune-rag-fine-tuning-language-models-to</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/finetune-rag-fine-tuning-language-models-to</guid>
    </item>
  </channel>
</rss>
