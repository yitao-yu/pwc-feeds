<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 08 Jan 2024 21:06:05 +0000</lastBuildDate>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>AnyText: Multilingual Visual Text Generation And Editing</title>
      <link>https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</link>
      <description><![CDATA[Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</guid>
    </item>
    <item>
      <title>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</title>
      <link>https://paperswithcode.com/paper/from-audio-to-photoreal-embodiment</link>
      <description><![CDATA[We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-audio-to-photoreal-embodiment</guid>
    </item>
    <item>
      <title>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</title>
      <link>https://paperswithcode.com/paper/tinygpt-v-efficient-multimodal-large-language</link>
      <description><![CDATA[In the era of advanced multimodel learning, multimodal large language models (MLLMs) such as GPT-4V have made remarkable strides towards bridging language and visual elements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinygpt-v-efficient-multimodal-large-language</guid>
    </item>
    <item>
      <title>TinyLlama: An Open-Source Small Language Model</title>
      <link>https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model</link>
      <description><![CDATA[We present TinyLlama, a compact 1. 1B language model pretrained on around 1 trillion tokens for approximately 3 epochs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model</guid>
    </item>
    <item>
      <title>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</title>
      <link>https://paperswithcode.com/paper/music-understanding-llama-advancing-text-to</link>
      <description><![CDATA[To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/music-understanding-llama-advancing-text-to</guid>
    </item>
    <item>
      <title>Fast Inference of Mixture-of-Experts Language Models with Offloading</title>
      <link>https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</link>
      <description><![CDATA[In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</guid>
    </item>
    <item>
      <title>GPT-4V(ision) is a Generalist Web Agent, if Grounded</title>
      <link>https://paperswithcode.com/paper/gpt-4v-ision-is-a-generalist-web-agent-if</link>
      <description><![CDATA[The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-4v-ision-is-a-generalist-web-agent-if</guid>
    </item>
    <item>
      <title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
      <link>https://paperswithcode.com/paper/llama-pro-progressive-llama-with-block</link>
      <description><![CDATA[Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e. g., from LLaMA to CodeLLaMA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-pro-progressive-llama-with-block</guid>
    </item>
    <item>
      <title>MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices</title>
      <link>https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong</link>
      <description><![CDATA[We present MobileVLM, a competent multimodal vision language model (MMVLM) targeted to run on mobile devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong</guid>
    </item>
    <item>
      <title>EasyVolcap: Accelerating Neural Volumetric Video Research</title>
      <link>https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</link>
      <description><![CDATA[Volumetric video is a technology that digitally records dynamic events such as artistic performances, sporting events, and remote conversations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</guid>
    </item>
    <item>
      <title>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</title>
      <link>https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for</link>
      <description><![CDATA[This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for</guid>
    </item>
    <item>
      <title>WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia</title>
      <link>https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</link>
      <description><![CDATA[WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</guid>
    </item>
    <item>
      <title>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/textit-v-guided-visual-search-as-a-core</link>
      <description><![CDATA[However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textit-v-guided-visual-search-as-a-core</guid>
    </item>
    <item>
      <title>Revisiting AUC-oriented Adversarial Training with Loss-Agnostic Perturbations</title>
      <link>https://paperswithcode.com/paper/revisiting-auc-oriented-adversarial-training</link>
      <description><![CDATA[On top of this, we can show that: 1) Under mild conditions, AdAUC can be optimized equivalently with score-based or instance-wise-loss-based perturbations, which is compatible with most of the popular adversarial example generation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-auc-oriented-adversarial-training</guid>
    </item>
    <item>
      <title>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</title>
      <link>https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</link>
      <description><![CDATA[Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</guid>
    </item>
    <item>
      <title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</title>
      <link>https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</link>
      <description><![CDATA[To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</guid>
    </item>
    <item>
      <title>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</title>
      <link>https://paperswithcode.com/paper/atom-low-bit-quantization-for-efficient-and</link>
      <description><![CDATA[To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/atom-low-bit-quantization-for-efficient-and</guid>
    </item>
    <item>
      <title>Large Language Models for Generative Information Extraction: A Survey</title>
      <link>https://paperswithcode.com/paper/large-language-models-for-generative-1</link>
      <description><![CDATA[Information extraction (IE) aims to extract structural knowledge (such as entities, relations, and events) from plain natural language texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-models-for-generative-1</guid>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</link>
      <description><![CDATA[Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</guid>
    </item>
  </channel>
</rss>
