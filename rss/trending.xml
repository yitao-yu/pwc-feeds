<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 19 May 2023 09:11:24 +0000</lastBuildDate>
    <item>
      <title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title>
      <link>https://paperswithcode.com/paper/decentralization-and-acceleration-enables</link>
      <description><![CDATA[In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decentralization-and-acceleration-enables</guid>
    </item>
    <item>
      <title>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</link>
      <description><![CDATA[To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</guid>
    </item>
    <item>
      <title>Tool Learning with Foundation Models</title>
      <link>https://paperswithcode.com/paper/tool-learning-with-foundation-models</link>
      <description><![CDATA[Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tool-learning-with-foundation-models</guid>
    </item>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title>
      <link>https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</link>
      <description><![CDATA[We recruit annotators to search for relevant information using our interface and then answer questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</guid>
    </item>
    <item>
      <title>StructGPT: A General Framework for Large Language Model to Reason over Structured Data</title>
      <link>https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</link>
      <description><![CDATA[Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</guid>
    </item>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</title>
      <link>https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</link>
      <description><![CDATA[We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</guid>
    </item>
    <item>
      <title>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</link>
      <description><![CDATA[In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</guid>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <link>https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</link>
      <description><![CDATA[We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</guid>
    </item>
    <item>
      <title>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM</title>
      <link>https://paperswithcode.com/paper/co-slam-joint-coordinate-and-sparse</link>
      <description><![CDATA[We present Co-SLAM, a neural RGB-D SLAM system based on a hybrid representation, that performs robust camera tracking and high-fidelity surface reconstruction in real time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/co-slam-joint-coordinate-and-sparse</guid>
    </item>
    <item>
      <title>PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
      <link>https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</link>
      <description><![CDATA[Real-world applications have high demands for semantic segmentation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</guid>
    </item>
    <item>
      <title>Pre-Training to Learn in Context</title>
      <link>https://paperswithcode.com/paper/pre-training-to-learn-in-context</link>
      <description><![CDATA[In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pre-training-to-learn-in-context</guid>
    </item>
    <item>
      <title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</guid>
    </item>
    <item>
      <title>TidyBot: Personalized Robot Assistance with Large Language Models</title>
      <link>https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</link>
      <description><![CDATA[For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</guid>
    </item>
    <item>
      <title>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</title>
      <link>https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</link>
      <description><![CDATA[To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</guid>
    </item>
    <item>
      <title>Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</link>
      <description><![CDATA[A medical image adapted SAM, which we have dubbed Medical SAM Adapter (MSA), shows superior performance on 19 medical image segmentation tasks with various image modalities including CT, MRI, ultrasound image, fundus image, and dermoscopic images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</guid>
    </item>
    <item>
      <title>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</title>
      <link>https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</link>
      <description><![CDATA[In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</guid>
    </item>
    <item>
      <title>Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title>
      <link>https://paperswithcode.com/paper/efficient-and-effective-text-encoding-for</link>
      <description><![CDATA[Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-and-effective-text-encoding-for</guid>
    </item>
  </channel>
</rss>
