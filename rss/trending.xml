<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 05 Nov 2024 09:16:07 +0000</lastBuildDate>
    <item>
      <title>DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation</title>
      <link>https://paperswithcode.com/paper/dreamclear-high-capacity-real-world-image</link>
      <description><![CDATA[Our second contribution, DreamClear, is a DiT-based image restoration model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamclear-high-capacity-real-world-image</guid>
    </item>
    <item>
      <title>Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation</title>
      <link>https://paperswithcode.com/paper/emilia-an-extensive-multilingual-and-diverse</link>
      <description><![CDATA[To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/emilia-an-extensive-multilingual-and-diverse</guid>
    </item>
    <item>
      <title>MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer</title>
      <link>https://paperswithcode.com/paper/maskgct-zero-shot-text-to-speech-with-masked</link>
      <description><![CDATA[The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/maskgct-zero-shot-text-to-speech-with-masked</guid>
    </item>
    <item>
      <title>Docling Technical Report</title>
      <link>https://paperswithcode.com/paper/docling-technical-report</link>
      <description><![CDATA[This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/docling-technical-report</guid>
    </item>
    <item>
      <title>OmniGen: Unified Image Generation</title>
      <link>https://paperswithcode.com/paper/omnigen-unified-image-generation</link>
      <description><![CDATA[In this work, we introduce OmniGen, a new diffusion model for unified image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnigen-unified-image-generation</guid>
    </item>
    <item>
      <title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2409-13731</link>
      <description><![CDATA[The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2409-13731</guid>
    </item>
    <item>
      <title>Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/senna-bridging-large-vision-language-models</link>
      <description><![CDATA[In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/senna-bridging-large-vision-language-models</guid>
    </item>
    <item>
      <title>D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement</title>
      <link>https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</link>
      <description><![CDATA[When pretrained on Objects365, D-FINE-L / X attains 57. 1% / 59. 3% AP, surpassing all existing real-time detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-fine-redefine-regression-task-in-detrs-as</guid>
    </item>
    <item>
      <title>Data Formulator 2: Iteratively Creating Rich Visualizations with AI</title>
      <link>https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</link>
      <description><![CDATA[To create rich visualizations, data analysts often need to iterate back and forth among data processing and chart specification to achieve their goals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</guid>
    </item>
    <item>
      <title>TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</title>
      <link>https://paperswithcode.com/paper/tokenformer-rethinking-transformer-scaling</link>
      <description><![CDATA[By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tokenformer-rethinking-transformer-scaling</guid>
    </item>
    <item>
      <title>$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources</title>
      <link>https://paperswithcode.com/paper/100k-or-100-days-trade-offs-when-pre-training</link>
      <description><![CDATA[We introduce a benchmark to measure the time to pre-train models on given GPUs and also identify ideal settings for maximizing training speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/100k-or-100-days-trade-offs-when-pre-training</guid>
    </item>
    <item>
      <title>MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector</title>
      <link>https://paperswithcode.com/paper/mia-tuner-adapting-large-language-models-as</link>
      <description><![CDATA[Existing studies have partially addressed this need through an exploration of the pre-training data detection problem, which is an instance of a membership inference attack (MIA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mia-tuner-adapting-large-language-models-as</guid>
    </item>
    <item>
      <title>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</title>
      <link>https://paperswithcode.com/paper/ichigo-mixed-modal-early-fusion-realtime</link>
      <description><![CDATA[Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ichigo-mixed-modal-early-fusion-realtime</guid>
    </item>
    <item>
      <title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title>
      <link>https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</link>
      <description><![CDATA[It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</guid>
    </item>
    <item>
      <title>Moonshine: Speech Recognition for Live Transcription and Voice Commands</title>
      <link>https://paperswithcode.com/paper/moonshine-speech-recognition-for-live</link>
      <description><![CDATA[This paper introduces Moonshine, a family of speech recognition models optimized for live transcription and voice command processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moonshine-speech-recognition-for-live</guid>
    </item>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</link>
      <description><![CDATA[While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</title>
      <link>https://paperswithcode.com/paper/longvu-spatiotemporal-adaptive-compression</link>
      <description><![CDATA[Given a light-weight LLM, our LongVU also scales effectively into a smaller size with state-of-the-art video understanding performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longvu-spatiotemporal-adaptive-compression</guid>
    </item>
    <item>
      <title>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title>
      <link>https://paperswithcode.com/paper/shadowkv-kv-cache-in-shadows-for-high</link>
      <description><![CDATA[By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3. 1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3. 04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shadowkv-kv-cache-in-shadows-for-high</guid>
    </item>
    <item>
      <title>Grounding Image Matching in 3D with MASt3R</title>
      <link>https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</link>
      <description><![CDATA[Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</guid>
    </item>
  </channel>
</rss>
