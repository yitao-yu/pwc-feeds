<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 14 Aug 2024 21:07:46 +0000</lastBuildDate>
    <item>
      <title>MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</title>
      <link>https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</link>
      <description><![CDATA[Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mindsearch-mimicking-human-minds-elicits-deep</guid>
    </item>
    <item>
      <title>Qwen2-Audio Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-audio-technical-report</link>
      <description><![CDATA[We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-audio-technical-report</guid>
    </item>
    <item>
      <title>AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology</title>
      <link>https://paperswithcode.com/paper/agilecoder-dynamic-collaborative-agents-for</link>
      <description><![CDATA[Software agents have emerged as promising tools for addressing complex software engineering tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agilecoder-dynamic-collaborative-agents-for</guid>
    </item>
    <item>
      <title>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2408-02545</link>
      <description><![CDATA[We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-02545</guid>
    </item>
    <item>
      <title>LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models</title>
      <link>https://paperswithcode.com/paper/llava-next-interleave-tackling-multi-image</link>
      <description><![CDATA[To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-next-interleave-tackling-multi-image</guid>
    </item>
    <item>
      <title>MixTex: Unambiguous Recognition Should Not Rely Solely on Real Data</title>
      <link>https://paperswithcode.com/paper/unambiguous-recognition-should-not-rely</link>
      <description><![CDATA[This paper introduces MixTex, an end-to-end LaTeX OCR model designed for low-bias multilingual recognition, along with its novel data collection method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unambiguous-recognition-should-not-rely</guid>
    </item>
    <item>
      <title>MiniCPM-V: A GPT-4V Level MLLM on Your Phone</title>
      <link>https://paperswithcode.com/paper/2408-01800</link>
      <description><![CDATA[The recent surge of Multimodal Large Language Models (MLLMs) has fundamentally reshaped the landscape of AI research and industry, shedding light on a promising path toward the next AI milestone.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-01800</guid>
    </item>
    <item>
      <title>FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title>
      <link>https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</link>
      <description><![CDATA[This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</guid>
    </item>
    <item>
      <title>T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge</title>
      <link>https://paperswithcode.com/paper/t-mac-cpu-renaissance-via-table-lookup-for</link>
      <description><![CDATA[Weight quantization is crucial for reducing the memory footprint of LLMs on devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t-mac-cpu-renaissance-via-table-lookup-for</guid>
    </item>
    <item>
      <title>CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents</title>
      <link>https://paperswithcode.com/paper/crab-cross-environment-agent-benchmark-for</link>
      <description><![CDATA[Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 100 tasks in computer desktop and mobile phone environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/crab-cross-environment-agent-benchmark-for</guid>
    </item>
    <item>
      <title>CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer</title>
      <link>https://paperswithcode.com/paper/cogvideox-text-to-video-diffusion-models-with</link>
      <description><![CDATA[To improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogvideox-text-to-video-diffusion-models-with</guid>
    </item>
    <item>
      <title>RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness</title>
      <link>https://paperswithcode.com/paper/rlaif-v-aligning-mllms-through-open-source-ai</link>
      <description><![CDATA[While traditional methods rely on labor-intensive and time-consuming manual labeling, recent approaches employing models as automatic labelers have shown promising results without human intervention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rlaif-v-aligning-mllms-through-open-source-ai</guid>
    </item>
    <item>
      <title>Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development</title>
      <link>https://paperswithcode.com/paper/data-juicer-sandbox-a-comprehensive-suite-for</link>
      <description><![CDATA[The emergence of large-scale multi-modal generative models has drastically advanced artificial intelligence, introducing unprecedented levels of performance and functionality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-juicer-sandbox-a-comprehensive-suite-for</guid>
    </item>
    <item>
      <title>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</title>
      <link>https://paperswithcode.com/paper/2408-02657</link>
      <description><![CDATA[We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-02657</guid>
    </item>
    <item>
      <title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</title>
      <link>https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</link>
      <description><![CDATA[Compared to both open-source and proprietary models, InternVL 1. 5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</guid>
    </item>
    <item>
      <title>Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/img-diff-contrastive-data-synthesis-for</link>
      <description><![CDATA[Besides, we investigate alternative methods for generating image difference data through "object removal" and conduct a thorough evaluation to confirm the dataset's diversity, quality, and robustness, presenting several insights on the synthesis of such a contrastive dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/img-diff-contrastive-data-synthesis-for</guid>
    </item>
    <item>
      <title>Comprehensive Assessment of Jailbreak Attacks Against LLMs</title>
      <link>https://paperswithcode.com/paper/comprehensive-assessment-of-jailbreak-attacks</link>
      <description><![CDATA[Some jailbreak prompt datasets, available from the Internet, can also achieve high attack success rates on many LLMs, such as ChatGLM3, GPT-3. 5, and PaLM2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/comprehensive-assessment-of-jailbreak-attacks</guid>
    </item>
    <item>
      <title>EasySpider: A No-Code Visual System for Crawling the Web</title>
      <link>https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</link>
      <description><![CDATA[As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</guid>
    </item>
    <item>
      <title>Medical SAM 2: Segment medical images as video via Segment Anything Model 2</title>
      <link>https://paperswithcode.com/paper/2408-00874</link>
      <description><![CDATA[By adopting the philosophy of taking medical images as videos, MedSAM-2 not only applies to 3D medical images but also unlocks new One-prompt Segmentation capability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2408-00874</guid>
    </item>
    <item>
      <title>Pearl: A Production-ready Reinforcement Learning Agent</title>
      <link>https://paperswithcode.com/paper/pearl-a-production-ready-reinforcement</link>
      <description><![CDATA[Reinforcement Learning (RL) offers a versatile framework for achieving long-term goals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pearl-a-production-ready-reinforcement</guid>
    </item>
  </channel>
</rss>
