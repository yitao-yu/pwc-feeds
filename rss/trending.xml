<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 19 May 2023 21:06:02 +0000</lastBuildDate>
    <item>
      <title>Sparks of Artificial General Intelligence: Early experiments with GPT-4</title>
      <link>https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence</link>
      <description><![CDATA[We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence</guid>
    </item>
    <item>
      <title>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention</title>
      <link>https://paperswithcode.com/paper/fastcomposer-tuning-free-multi-subject-image</link>
      <description><![CDATA[FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastcomposer-tuning-free-multi-subject-image</guid>
    </item>
    <item>
      <title>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</link>
      <description><![CDATA[To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</guid>
    </item>
    <item>
      <title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title>
      <link>https://paperswithcode.com/paper/decentralization-and-acceleration-enables</link>
      <description><![CDATA[In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decentralization-and-acceleration-enables</guid>
    </item>
    <item>
      <title>Reflexion: an autonomous agent with dynamic memory and self-reflection</title>
      <link>https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</link>
      <description><![CDATA[To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</guid>
    </item>
    <item>
      <title>StructGPT: A General Framework for Large Language Model to Reason over Structured Data</title>
      <link>https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</link>
      <description><![CDATA[Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</guid>
    </item>
    <item>
      <title>Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes</title>
      <link>https://paperswithcode.com/paper/rethinking-the-open-loop-evaluation-of-end-to</link>
      <description><![CDATA[Our observation also indicates that we need to rethink the current open-loop evaluation scheme of end-to-end autonomous driving in nuScenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-the-open-loop-evaluation-of-end-to</guid>
    </item>
    <item>
      <title>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback</title>
      <link>https://paperswithcode.com/paper/improving-language-model-negotiation-with</link>
      <description><![CDATA[We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-language-model-negotiation-with</guid>
    </item>
    <item>
      <title>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</title>
      <link>https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</link>
      <description><![CDATA[We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</guid>
    </item>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title>
      <link>https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</link>
      <description><![CDATA[We recruit annotators to search for relevant information using our interface and then answer questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</guid>
    </item>
    <item>
      <title>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</title>
      <link>https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</link>
      <description><![CDATA[To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</guid>
    </item>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</link>
      <description><![CDATA[In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</guid>
    </item>
    <item>
      <title>Pre-Training to Learn in Context</title>
      <link>https://paperswithcode.com/paper/pre-training-to-learn-in-context</link>
      <description><![CDATA[In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pre-training-to-learn-in-context</guid>
    </item>
    <item>
      <title>Revisiting the Minimalist Approach to Offline Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/revisiting-the-minimalist-approach-to-offline</link>
      <description><![CDATA[In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-the-minimalist-approach-to-offline</guid>
    </item>
    <item>
      <title>Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM</title>
      <link>https://paperswithcode.com/paper/co-slam-joint-coordinate-and-sparse</link>
      <description><![CDATA[We present Co-SLAM, a neural RGB-D SLAM system based on a hybrid representation, that performs robust camera tracking and high-fidelity surface reconstruction in real time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/co-slam-joint-coordinate-and-sparse</guid>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <link>https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</link>
      <description><![CDATA[We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</guid>
    </item>
    <item>
      <title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</guid>
    </item>
  </channel>
</rss>
