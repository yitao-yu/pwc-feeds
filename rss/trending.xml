<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 18 Apr 2024 09:12:54 +0000</lastBuildDate>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</title>
      <link>https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</link>
      <description><![CDATA[We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i. e., high-resolution visual tokens, high-quality data, and VLM-guided generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</guid>
    </item>
    <item>
      <title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
      <link>https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</link>
      <description><![CDATA[To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</guid>
    </item>
    <item>
      <title>Magic Clothing: Controllable Garment-Driven Image Synthesis</title>
      <link>https://paperswithcode.com/paper/magic-clothing-controllable-garment-driven</link>
      <description><![CDATA[We propose Magic Clothing, a latent diffusion model (LDM)-based network architecture for an unexplored garment-driven image synthesis task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magic-clothing-controllable-garment-driven</guid>
    </item>
    <item>
      <title>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</title>
      <link>https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</link>
      <description><![CDATA[We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</guid>
    </item>
    <item>
      <title>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</title>
      <link>https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</link>
      <description><![CDATA[Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</guid>
    </item>
    <item>
      <title>Probing the 3D Awareness of Visual Foundation Models</title>
      <link>https://paperswithcode.com/paper/probing-the-3d-awareness-of-visual-foundation</link>
      <description><![CDATA[Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probing-the-3d-awareness-of-visual-foundation</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>State Space Model for New-Generation Network Alternative to Transformers: A Survey</title>
      <link>https://paperswithcode.com/paper/state-space-model-for-new-generation-network</link>
      <description><![CDATA[In this paper, we give the first comprehensive review of these works and also provide experimental comparisons and analysis to better demonstrate the features and advantages of SSM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/state-space-model-for-new-generation-network</guid>
    </item>
    <item>
      <title>MyGO: Discrete Modality Information as Fine-Grained Tokens for Multi-modal Knowledge Graph Completion</title>
      <link>https://paperswithcode.com/paper/mygo-discrete-modality-information-as-fine</link>
      <description><![CDATA[To overcome their inherent incompleteness, multi-modal knowledge graph completion (MMKGC) aims to discover unobserved knowledge from given MMKGs, leveraging both structural information from the triples and multi-modal information of the entities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mygo-discrete-modality-information-as-fine</guid>
    </item>
    <item>
      <title>Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</title>
      <link>https://paperswithcode.com/paper/megalodon-efficient-llm-pretraining-and</link>
      <description><![CDATA[The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/megalodon-efficient-llm-pretraining-and</guid>
    </item>
    <item>
      <title>LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding</title>
      <link>https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</link>
      <description><![CDATA[The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</guid>
    </item>
    <item>
      <title>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM</title>
      <link>https://paperswithcode.com/paper/branch-train-mix-mixing-expert-llms-into-a</link>
      <description><![CDATA[We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/branch-train-mix-mixing-expert-llms-into-a</guid>
    </item>
    <item>
      <title>Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models</title>
      <link>https://paperswithcode.com/paper/prepacking-a-simple-method-for-fast</link>
      <description><![CDATA[In this work, we highlight the following pitfall of prefilling: for batches containing high-varying prompt lengths, significant computation is wasted by the standard practice of padding sequences to the maximum length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prepacking-a-simple-method-for-fast</guid>
    </item>
    <item>
      <title>Arc2Face: A Foundation Model of Human Faces</title>
      <link>https://paperswithcode.com/paper/arc2face-a-foundation-model-of-human-faces</link>
      <description><![CDATA[This paper presents Arc2Face, an identity-conditioned face foundation model, which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of face similarity than existing models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/arc2face-a-foundation-model-of-human-faces</guid>
    </item>
    <item>
      <title>Rho-1: Not All Tokens Are What You Need</title>
      <link>https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</link>
      <description><![CDATA[After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40. 6% and 51. 8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</guid>
    </item>
    <item>
      <title>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</title>
      <link>https://paperswithcode.com/paper/champ-controllable-and-consistent-human-image</link>
      <description><![CDATA[In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/champ-controllable-and-consistent-human-image</guid>
    </item>
    <item>
      <title>LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition</title>
      <link>https://paperswithcode.com/paper/lafs-landmark-based-facial-self-supervised</link>
      <description><![CDATA[This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lafs-landmark-based-facial-self-supervised</guid>
    </item>
    <item>
      <title>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</title>
      <link>https://paperswithcode.com/paper/leave-no-context-behind-efficient-infinite</link>
      <description><![CDATA[This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leave-no-context-behind-efficient-infinite</guid>
    </item>
    <item>
      <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
      <link>https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</link>
      <description><![CDATA[To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</guid>
    </item>
  </channel>
</rss>
