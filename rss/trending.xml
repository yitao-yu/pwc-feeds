<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 18 Dec 2023 09:13:13 +0000</lastBuildDate>
    <item>
      <title>EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM</title>
      <link>https://paperswithcode.com/paper/edgesam-prompt-in-the-loop-distillation-for</link>
      <description><![CDATA[It is also the first SAM variant that can run at over 30 FPS on an iPhone 14.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/edgesam-prompt-in-the-loop-distillation-for</guid>
    </item>
    <item>
      <title>FreeInit: Bridging Initialization Gap in Video Diffusion Models</title>
      <link>https://paperswithcode.com/paper/freeinit-bridging-initialization-gap-in-video</link>
      <description><![CDATA[Though diffusion-based video generation has witnessed rapid progress, the inference results of existing models still exhibit unsatisfactory temporal consistency and unnatural dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/freeinit-bridging-initialization-gap-in-video</guid>
    </item>
    <item>
      <title>Pearl: A Production-ready Reinforcement Learning Agent</title>
      <link>https://paperswithcode.com/paper/pearl-a-production-ready-reinforcement</link>
      <description><![CDATA[Reinforcement Learning (RL) offers a versatile framework for achieving long-term goals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pearl-a-production-ready-reinforcement</guid>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <link>https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</link>
      <description><![CDATA[Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</guid>
    </item>
    <item>
      <title>OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/occnerf-self-supervised-multi-camera</link>
      <description><![CDATA[Moreover, for semantic occupancy prediction, we design several strategies to polish the prompts and filter the outputs of a pretrained open-vocabulary 2D segmentation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/occnerf-self-supervised-multi-camera</guid>
    </item>
    <item>
      <title>PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation</title>
      <link>https://paperswithcode.com/paper/patchfusion-an-end-to-end-tile-based</link>
      <description><![CDATA[Single image depth estimation is a foundational task in computer vision and generative modeling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/patchfusion-an-end-to-end-tile-based</guid>
    </item>
    <item>
      <title>DemoFusion: Democratising High-Resolution Image Generation With No $$$</title>
      <link>https://paperswithcode.com/paper/demofusion-democratising-high-resolution</link>
      <description><![CDATA[High-resolution image generation with Generative Artificial Intelligence (GenAI) has immense potential but, due to the enormous capital investment required for training, it is increasingly centralised to a few large corporations, and hidden behind paywalls.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demofusion-democratising-high-resolution</guid>
    </item>
    <item>
      <title>Mistral 7B</title>
      <link>https://paperswithcode.com/paper/mistral-7b</link>
      <description><![CDATA[We introduce Mistral 7B v0. 1, a 7-billion-parameter language model engineered for superior performance and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mistral-7b</guid>
    </item>
    <item>
      <title>AnimateZero: Video Diffusion Models are Zero-Shot Image Animators</title>
      <link>https://paperswithcode.com/paper/animatezero-video-diffusion-models-are-zero</link>
      <description><![CDATA[For appearance control, we borrow intermediate latents and their features from the text-to-image (T2I) generation for ensuring the generated first frame is equal to the given generated image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatezero-video-diffusion-models-are-zero</guid>
    </item>
    <item>
      <title>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
      <link>https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</link>
      <description><![CDATA[In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i. e., $7. 5$).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</guid>
    </item>
    <item>
      <title>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</title>
      <link>https://paperswithcode.com/paper/repurposing-diffusion-based-image-generators</link>
      <description><![CDATA[Monocular depth estimation is a fundamental computer vision task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repurposing-diffusion-based-image-generators</guid>
    </item>
    <item>
      <title>A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions</title>
      <link>https://paperswithcode.com/paper/a-picture-is-worth-more-than-77-text-tokens</link>
      <description><![CDATA[Curation methods for massive vision-language datasets trade off between dataset size and quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-picture-is-worth-more-than-77-text-tokens</guid>
    </item>
    <item>
      <title>Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</title>
      <link>https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</link>
      <description><![CDATA[To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</guid>
    </item>
    <item>
      <title>0.1% Data Makes Segment Anything Slim</title>
      <link>https://paperswithcode.com/paper/0-1-data-makes-segment-anything-slim</link>
      <description><![CDATA[To address this issue, this paper introduces SlimSAM, a novel SAM compression method that achieves superior performance with remarkably low training costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/0-1-data-makes-segment-anything-slim</guid>
    </item>
    <item>
      <title>TaskWeaver: A Code-First Agent Framework</title>
      <link>https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</link>
      <description><![CDATA[TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</guid>
    </item>
    <item>
      <title>KwaiAgents: Generalized Information-seeking Agent System with Large Language Models</title>
      <link>https://paperswithcode.com/paper/kwaiagents-generalized-information-seeking</link>
      <description><![CDATA[Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kwaiagents-generalized-information-seeking</guid>
    </item>
    <item>
      <title>An LLM Compiler for Parallel Function Calling</title>
      <link>https://paperswithcode.com/paper/an-llm-compiler-for-parallel-function-calling</link>
      <description><![CDATA[LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-llm-compiler-for-parallel-function-calling</guid>
    </item>
    <item>
      <title>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</title>
      <link>https://paperswithcode.com/paper/alpha-clip-a-clip-model-focusing-on-wherever</link>
      <description><![CDATA[Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alpha-clip-a-clip-model-focusing-on-wherever</guid>
    </item>
    <item>
      <title>Disentangling Writer and Character Styles for Handwriting Generation</title>
      <link>https://paperswithcode.com/paper/disentangling-writer-and-character-styles-for</link>
      <description><![CDATA[In light of this, we propose to disentangle the style representations at both writer and character levels from individual handwritings to synthesize realistic stylized online handwritten characters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/disentangling-writer-and-character-styles-for</guid>
    </item>
    <item>
      <title>NEFTune: Noisy Embeddings Improve Instruction Finetuning</title>
      <link>https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</link>
      <description><![CDATA[We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</guid>
    </item>
  </channel>
</rss>
