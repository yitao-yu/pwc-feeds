<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 16 May 2024 21:08:11 +0000</lastBuildDate>
    <item>
      <title>MambaOut: Do We Really Need Mamba for Vision?</title>
      <link>https://paperswithcode.com/paper/mambaout-do-we-really-need-mamba-for-vision</link>
      <description><![CDATA[For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mambaout-do-we-really-need-mamba-for-vision</guid>
    </item>
    <item>
      <title>A decoder-only foundation model for time-series forecasting</title>
      <link>https://paperswithcode.com/paper/a-decoder-only-foundation-model-for-time</link>
      <description><![CDATA[Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-decoder-only-foundation-model-for-time</guid>
    </item>
    <item>
      <title>AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding</title>
      <link>https://paperswithcode.com/paper/anitalker-animate-vivid-and-diverse-talking</link>
      <description><![CDATA[The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anitalker-animate-vivid-and-diverse-talking</guid>
    </item>
    <item>
      <title>AgentScope: A Flexible yet Robust Multi-Agent Platform</title>
      <link>https://paperswithcode.com/paper/agentscope-a-flexible-yet-robust-multi-agent</link>
      <description><![CDATA[With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentscope-a-flexible-yet-robust-multi-agent</guid>
    </item>
    <item>
      <title>Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</link>
      <description><![CDATA[Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</guid>
    </item>
    <item>
      <title>Autonomous LLM-driven research from data to human-verifiable research papers</title>
      <link>https://paperswithcode.com/paper/autonomous-llm-driven-research-from-data-to</link>
      <description><![CDATA[As AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autonomous-llm-driven-research-from-data-to</guid>
    </item>
    <item>
      <title>Sakuga-42M Dataset: Scaling Up Cartoon Research</title>
      <link>https://paperswithcode.com/paper/sakuga-42m-dataset-scaling-up-cartoon</link>
      <description><![CDATA[Can we harness the success of the scaling paradigm to benefit cartoon research?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sakuga-42m-dataset-scaling-up-cartoon</guid>
    </item>
    <item>
      <title>DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</title>
      <link>https://paperswithcode.com/paper/deepseek-v2-a-strong-economical-and-efficient</link>
      <description><![CDATA[MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-v2-a-strong-economical-and-efficient</guid>
    </item>
    <item>
      <title>StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</title>
      <link>https://paperswithcode.com/paper/storydiffusion-consistent-self-attention-for</link>
      <description><![CDATA[This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/storydiffusion-consistent-self-attention-for</guid>
    </item>
    <item>
      <title>OS-Copilot: Towards Generalist Computer Agents with Self-Improvement</title>
      <link>https://paperswithcode.com/paper/os-copilot-towards-generalist-computer-agents</link>
      <description><![CDATA[Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/os-copilot-towards-generalist-computer-agents</guid>
    </item>
    <item>
      <title>Granite Code Models: A Family of Open Foundation Models for Code Intelligence</title>
      <link>https://paperswithcode.com/paper/granite-code-models-a-family-of-open</link>
      <description><![CDATA[Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/granite-code-models-a-family-of-open</guid>
    </item>
    <item>
      <title>KAN: Kolmogorov-Arnold Networks</title>
      <link>https://paperswithcode.com/paper/kan-kolmogorov-arnold-networks</link>
      <description><![CDATA[Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kan-kolmogorov-arnold-networks</guid>
    </item>
    <item>
      <title>RLHF Workflow: From Reward Modeling to Online RLHF</title>
      <link>https://paperswithcode.com/paper/rlhf-workflow-from-reward-modeling-to-online</link>
      <description><![CDATA[We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rlhf-workflow-from-reward-modeling-to-online</guid>
    </item>
    <item>
      <title>A Multi-Level Superoptimizer for Tensor Programs</title>
      <link>https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</link>
      <description><![CDATA[We introduce Mirage, the first multi-level superoptimizer for tensor programs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</guid>
    </item>
    <item>
      <title>Improving Diffusion Models for Virtual Try-on</title>
      <link>https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</link>
      <description><![CDATA[Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</guid>
    </item>
    <item>
      <title>MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels</title>
      <link>https://paperswithcode.com/paper/ms-marco-web-search-a-large-scale-information</link>
      <description><![CDATA[Recent breakthroughs in large models have highlighted the critical significance of data scale, labels and modals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ms-marco-web-search-a-large-scale-information</guid>
    </item>
    <item>
      <title>Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models</title>
      <link>https://paperswithcode.com/paper/fishing-for-magikarp-automatically-detecting</link>
      <description><![CDATA[The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fishing-for-magikarp-automatically-detecting</guid>
    </item>
    <item>
      <title>Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/time-evidence-fusion-network-multi-source</link>
      <description><![CDATA[TEFN is not a model that achieves the ultimate in single aspect, but a model that balances performance, accuracy, stability, and interpretability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/time-evidence-fusion-network-multi-source</guid>
    </item>
    <item>
      <title>Linearizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/linearizing-large-language-models</link>
      <description><![CDATA[Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/linearizing-large-language-models</guid>
    </item>
    <item>
      <title>Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond</title>
      <link>https://paperswithcode.com/paper/is-sora-a-world-simulator-a-comprehensive</link>
      <description><![CDATA[General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/is-sora-a-world-simulator-a-comprehensive</guid>
    </item>
  </channel>
</rss>
