<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 20 Oct 2024 21:08:37 +0000</lastBuildDate>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching</title>
      <link>https://paperswithcode.com/paper/f5-tts-a-fairytaler-that-fakes-fluent-and</link>
      <description><![CDATA[This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/f5-tts-a-fairytaler-that-fakes-fluent-and</guid>
    </item>
    <item>
      <title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title>
      <link>https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</link>
      <description><![CDATA[It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-omni2-towards-open-source-gpt-4o-model</guid>
    </item>
    <item>
      <title>DepthSplat: Connecting Gaussian Splatting and Depth</title>
      <link>https://paperswithcode.com/paper/depthsplat-connecting-gaussian-splatting-and</link>
      <description><![CDATA[Gaussian splatting and single/multi-view depth estimation are typically studied in isolation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depthsplat-connecting-gaussian-splatting-and</guid>
    </item>
    <item>
      <title>Movie Gen: A Cast of Media Foundation Models</title>
      <link>https://paperswithcode.com/paper/movie-gen-a-cast-of-media-foundation-models</link>
      <description><![CDATA[Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/movie-gen-a-cast-of-media-foundation-models</guid>
    </item>
    <item>
      <title>ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood</title>
      <link>https://paperswithcode.com/paper/asft-aligned-supervised-fine-tuning-through</link>
      <description><![CDATA[Additionally, we compare ASFT to DPO and its latest variants, such as the single-step approach ORPO, using the latest instruction-tuned model Llama3, which has been fine-tuned on UltraFeedback and HH-RLHF.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/asft-aligned-supervised-fine-tuning-through</guid>
    </item>
    <item>
      <title>DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</title>
      <link>https://paperswithcode.com/paper/duoattention-efficient-long-context-llm</link>
      <description><![CDATA[Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/duoattention-efficient-long-context-llm</guid>
    </item>
    <item>
      <title>Pyramidal Flow Matching for Efficient Video Generative Modeling</title>
      <link>https://paperswithcode.com/paper/pyramidal-flow-matching-for-efficient-video</link>
      <description><![CDATA[Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pyramidal-flow-matching-for-efficient-video</guid>
    </item>
    <item>
      <title>Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</title>
      <link>https://paperswithcode.com/paper/representation-alignment-for-generation</link>
      <description><![CDATA[Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/representation-alignment-for-generation</guid>
    </item>
    <item>
      <title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title>
      <link>https://paperswithcode.com/paper/hart-efficient-visual-generation-with-hybrid</link>
      <description><![CDATA[To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hart-efficient-visual-generation-with-hybrid</guid>
    </item>
    <item>
      <title>From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline</title>
      <link>https://paperswithcode.com/paper/from-crowdsourced-data-to-high-quality</link>
      <description><![CDATA[The rapid evolution of Large Language Models (LLMs) has outpaced the development of model evaluation, highlighting the need for continuous curation of new, challenging benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-crowdsourced-data-to-high-quality</guid>
    </item>
    <item>
      <title>Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</title>
      <link>https://paperswithcode.com/paper/depth-pro-sharp-monocular-metric-depth-in</link>
      <description><![CDATA[We present a foundation model for zero-shot metric monocular depth estimation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-pro-sharp-monocular-metric-depth-in</guid>
    </item>
    <item>
      <title>Aria: An Open Multimodal Native Mixture-of-Experts Model</title>
      <link>https://paperswithcode.com/paper/aria-an-open-multimodal-native-mixture-of</link>
      <description><![CDATA[Information comes in diverse modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aria-an-open-multimodal-native-mixture-of</guid>
    </item>
    <item>
      <title>Baichuan-Omni Technical Report</title>
      <link>https://paperswithcode.com/paper/baichuan-omni-technical-report</link>
      <description><![CDATA[The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/baichuan-omni-technical-report</guid>
    </item>
    <item>
      <title>Agent S: An Open Agentic Framework that Uses Computers Like a Human</title>
      <link>https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</link>
      <description><![CDATA[We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</guid>
    </item>
    <item>
      <title>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
      <link>https://paperswithcode.com/paper/segformer-simple-and-efficient-design-for</link>
      <description><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segformer-simple-and-efficient-design-for</guid>
    </item>
    <item>
      <title>Diffusion for World Modeling: Visual Details Matter in Atari</title>
      <link>https://paperswithcode.com/paper/diffusion-for-world-modeling-visual-details</link>
      <description><![CDATA[Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-for-world-modeling-visual-details</guid>
    </item>
    <item>
      <title>GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks</title>
      <link>https://paperswithcode.com/paper/gtsinger-a-global-multi-technique-singing</link>
      <description><![CDATA[The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gtsinger-a-global-multi-technique-singing</guid>
    </item>
    <item>
      <title>LoLCATs: On Low-Rank Linearizing of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lolcats-on-low-rank-linearizing-of-large</link>
      <description><![CDATA[When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3. 1 70B and 405B LLMs by 77. 8% and 78. 1% on 5-shot MMLU.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lolcats-on-low-rank-linearizing-of-large</guid>
    </item>
    <item>
      <title>CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos</title>
      <link>https://paperswithcode.com/paper/cotracker3-simpler-and-better-point-tracking</link>
      <description><![CDATA[Most state-of-the-art point trackers are trained on synthetic data due to the difficulty of annotating real videos for this task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cotracker3-simpler-and-better-point-tracking</guid>
    </item>
  </channel>
</rss>
