<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 29 Mar 2024 09:11:44 +0000</lastBuildDate>
    <item>
      <title>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</title>
      <link>https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</link>
      <description><![CDATA[In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aniportrait-audio-driven-synthesis-of</guid>
    </item>
    <item>
      <title>AIOS: LLM Agent Operating System</title>
      <link>https://paperswithcode.com/paper/llm-agent-operating-system</link>
      <description><![CDATA[Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system "with soul" -- an important step towards AGI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm-agent-operating-system</guid>
    </item>
    <item>
      <title>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</title>
      <link>https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</link>
      <description><![CDATA[Sora is the first large-scale generalist video generation model that garnered significant attention across society.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</guid>
    </item>
    <item>
      <title>VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</title>
      <link>https://paperswithcode.com/paper/voicecraft-zero-shot-speech-editing-and-text</link>
      <description><![CDATA[We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voicecraft-zero-shot-speech-editing-and-text</guid>
    </item>
    <item>
      <title>SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</title>
      <link>https://paperswithcode.com/paper/sdxs-real-time-one-step-latent-diffusion</link>
      <description><![CDATA[Recent advancements in diffusion models have positioned them at the forefront of image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sdxs-real-time-one-step-latent-diffusion</guid>
    </item>
    <item>
      <title>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</title>
      <link>https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</link>
      <description><![CDATA[We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i. e., high-resolution visual tokens, high-quality data, and VLM-guided generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</guid>
    </item>
    <item>
      <title>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</title>
      <link>https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</link>
      <description><![CDATA[Recognizing the complementary strengths and weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes both prompts within a single model through contrastive learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</guid>
    </item>
    <item>
      <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
      <link>https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</link>
      <description><![CDATA[To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</guid>
    </item>
    <item>
      <title>General Object Foundation Model for Images and Videos at Scale</title>
      <link>https://paperswithcode.com/paper/general-object-foundation-model-for-images</link>
      <description><![CDATA[We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-object-foundation-model-for-images</guid>
    </item>
    <item>
      <title>Evolutionary Optimization of Model Merging Recipes</title>
      <link>https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</link>
      <description><![CDATA[Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</guid>
    </item>
    <item>
      <title>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</title>
      <link>https://paperswithcode.com/paper/make-your-anchor-a-diffusion-based-2d-avatar</link>
      <description><![CDATA[We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/make-your-anchor-a-diffusion-based-2d-avatar</guid>
    </item>
    <item>
      <title>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</title>
      <link>https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</link>
      <description><![CDATA[Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</guid>
    </item>
    <item>
      <title>Analyzing and Improving the Training Dynamics of Diffusion Models</title>
      <link>https://paperswithcode.com/paper/analyzing-and-improving-the-training-dynamics</link>
      <description><![CDATA[Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/analyzing-and-improving-the-training-dynamics</guid>
    </item>
    <item>
      <title>Long-CLIP: Unlocking the Long-Text Capability of CLIP</title>
      <link>https://paperswithcode.com/paper/long-clip-unlocking-the-long-text-capability</link>
      <description><![CDATA[Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/long-clip-unlocking-the-long-text-capability</guid>
    </item>
    <item>
      <title>One-Step Image Translation with Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</link>
      <description><![CDATA[In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</guid>
    </item>
    <item>
      <title>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title>
      <link>https://paperswithcode.com/paper/llmlingua-2-data-distillation-for-efficient</link>
      <description><![CDATA[The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llmlingua-2-data-distillation-for-efficient</guid>
    </item>
    <item>
      <title>A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond</title>
      <link>https://paperswithcode.com/paper/a-survey-of-neural-code-intelligence</link>
      <description><![CDATA[Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-neural-code-intelligence</guid>
    </item>
    <item>
      <title>LLM4Decompile: Decompiling Binary Code with Large Language Models</title>
      <link>https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</link>
      <description><![CDATA[Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</guid>
    </item>
    <item>
      <title>Long-form factuality in large language models</title>
      <link>https://paperswithcode.com/paper/long-form-factuality-in-large-language-models</link>
      <description><![CDATA[Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/long-form-factuality-in-large-language-models</guid>
    </item>
    <item>
      <title>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</title>
      <link>https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</link>
      <description><![CDATA[We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</guid>
    </item>
  </channel>
</rss>
