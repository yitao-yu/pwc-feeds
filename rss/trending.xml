<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 17 Dec 2024 21:07:57 +0000</lastBuildDate>
    <item>
      <title>Large Concept Models: Language Modeling in a Sentence Representation Space</title>
      <link>https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</link>
      <description><![CDATA[In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</guid>
    </item>
    <item>
      <title>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</title>
      <link>https://paperswithcode.com/paper/gaze-lle-gaze-target-estimation-via-large</link>
      <description><![CDATA[We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaze-lle-gaze-target-estimation-via-large</guid>
    </item>
    <item>
      <title>HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</title>
      <link>https://paperswithcode.com/paper/hellomeme-integrating-spatial-knitting</link>
      <description><![CDATA[We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hellomeme-integrating-spatial-knitting</guid>
    </item>
    <item>
      <title>Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation</title>
      <link>https://paperswithcode.com/paper/neural-localizer-fields-for-continuous-3d</link>
      <description><![CDATA[With the explosive growth of available training data, single-image 3D human modeling is ahead of a transition to a data-centric paradigm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-localizer-fields-for-continuous-3d</guid>
    </item>
    <item>
      <title>Learning Flow Fields in Attention for Controllable Person Image Generation</title>
      <link>https://paperswithcode.com/paper/learning-flow-fields-in-attention-for</link>
      <description><![CDATA[Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-flow-fields-in-attention-for</guid>
    </item>
    <item>
      <title>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</title>
      <link>https://paperswithcode.com/paper/syncammaster-synchronizing-multi-camera-video</link>
      <description><![CDATA[Recent advancements in video diffusion models have shown exceptional abilities in simulating real-world dynamics and maintaining 3D consistency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/syncammaster-synchronizing-multi-camera-video</guid>
    </item>
    <item>
      <title>Video Seal: Open and Efficient Video Watermarking</title>
      <link>https://paperswithcode.com/paper/video-seal-open-and-efficient-video</link>
      <description><![CDATA[To reduce these gaps, this paper introduces Video Seal, a comprehensive framework for neural video watermarking and a competitive open-sourced model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-seal-open-and-efficient-video</guid>
    </item>
    <item>
      <title>Best-of-N Jailbreaking</title>
      <link>https://paperswithcode.com/paper/best-of-n-jailbreaking</link>
      <description><![CDATA[We find that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3. 5 Sonnet when sampling 10, 000 augmented prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/best-of-n-jailbreaking</guid>
    </item>
    <item>
      <title>Docling Technical Report</title>
      <link>https://paperswithcode.com/paper/docling-technical-report</link>
      <description><![CDATA[This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/docling-technical-report</guid>
    </item>
    <item>
      <title>StableAnimator: High-Quality Identity-Preserving Human Image Animation</title>
      <link>https://paperswithcode.com/paper/stableanimator-high-quality-identity</link>
      <description><![CDATA[During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stableanimator-high-quality-identity</guid>
    </item>
    <item>
      <title>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</title>
      <link>https://paperswithcode.com/paper/llms-as-judges-a-comprehensive-survey-on-llm</link>
      <description><![CDATA[Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llms-as-judges-a-comprehensive-survey-on-llm</guid>
    </item>
    <item>
      <title>FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing</title>
      <link>https://paperswithcode.com/paper/fireflow-fast-inversion-of-rectified-flow-for</link>
      <description><![CDATA[Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fireflow-fast-inversion-of-rectified-flow-for</guid>
    </item>
    <item>
      <title>HunyuanVideo: A Systematic Framework For Large Video Generative Models</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-a-systematic-framework-for-large</link>
      <description><![CDATA[In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-a-systematic-framework-for-large</guid>
    </item>
    <item>
      <title>EasyVolcap: Accelerating Neural Volumetric Video Research</title>
      <link>https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</link>
      <description><![CDATA[Volumetric video is a technology that digitally records dynamic events such as artistic performances, sporting events, and remote conversations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyvolcap-accelerating-neural-volumetric</guid>
    </item>
    <item>
      <title>Representing Long Volumetric Video with Temporal Gaussian Hierarchy</title>
      <link>https://paperswithcode.com/paper/representing-long-volumetric-video-with</link>
      <description><![CDATA[In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/representing-long-volumetric-video-with</guid>
    </item>
    <item>
      <title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title>
      <link>https://paperswithcode.com/paper/omnidocbench-benchmarking-diverse-pdf</link>
      <description><![CDATA[Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnidocbench-benchmarking-diverse-pdf</guid>
    </item>
    <item>
      <title>Arbitrary-steps Image Super-resolution via Diffusion Inversion</title>
      <link>https://paperswithcode.com/paper/arbitrary-steps-image-super-resolution-via</link>
      <description><![CDATA[This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/arbitrary-steps-image-super-resolution-via</guid>
    </item>
    <item>
      <title>Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/towards-controllable-speech-synthesis-in-the</link>
      <description><![CDATA[In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-controllable-speech-synthesis-in-the</guid>
    </item>
    <item>
      <title>MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions</title>
      <link>https://paperswithcode.com/paper/mossformer-pushing-the-performance-limit-of</link>
      <description><![CDATA[To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mossformer-pushing-the-performance-limit-of</guid>
    </item>
    <item>
      <title>Maya: An Instruction Finetuned Multilingual Multimodal Model</title>
      <link>https://paperswithcode.com/paper/maya-an-instruction-finetuned-multilingual</link>
      <description><![CDATA[The rapid development of large Vision-Language Models (VLMs) has led to impressive results on academic benchmarks, primarily in widely spoken languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/maya-an-instruction-finetuned-multilingual</guid>
    </item>
  </channel>
</rss>
