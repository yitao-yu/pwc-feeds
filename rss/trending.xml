<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 27 Dec 2023 09:11:48 +0000</lastBuildDate>
    <item>
      <title>StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation</title>
      <link>https://paperswithcode.com/paper/streamdiffusion-a-pipeline-level-solution-for</link>
      <description><![CDATA[We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamdiffusion-a-pipeline-level-solution-for</guid>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <link>https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</link>
      <description><![CDATA[We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</guid>
    </item>
    <item>
      <title>PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/pia-your-personalized-image-animator-via-plug</link>
      <description><![CDATA[Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pia-your-personalized-image-animator-via-plug</guid>
    </item>
    <item>
      <title>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</title>
      <link>https://paperswithcode.com/paper/powerinfer-fast-large-language-model-serving</link>
      <description><![CDATA[This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/powerinfer-fast-large-language-model-serving</guid>
    </item>
    <item>
      <title>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</title>
      <link>https://paperswithcode.com/paper/internvl-scaling-up-vision-foundation-models</link>
      <description><![CDATA[However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internvl-scaling-up-vision-foundation-models</guid>
    </item>
    <item>
      <title>Splatter Image: Ultra-Fast Single-View 3D Reconstruction</title>
      <link>https://paperswithcode.com/paper/splatter-image-ultra-fast-single-view-3d</link>
      <description><![CDATA[We introduce the Splatter Image, an ultra-fast approach for monocular 3D object reconstruction which operates at 38 FPS.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/splatter-image-ultra-fast-single-view-3d</guid>
    </item>
    <item>
      <title>Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models</title>
      <link>https://paperswithcode.com/paper/paint3d-paint-anything-3d-with-lighting-less</link>
      <description><![CDATA[This paper presents Paint3D, a novel coarse-to-fine generative framework that is capable of producing high-resolution, lighting-less, and diverse 2K UV texture maps for untextured 3D meshes conditioned on text or image inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paint3d-paint-anything-3d-with-lighting-less</guid>
    </item>
    <item>
      <title>PromptBench: A Unified Library for Evaluation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/promptbench-a-unified-library-for-evaluation</link>
      <description><![CDATA[The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/promptbench-a-unified-library-for-evaluation</guid>
    </item>
    <item>
      <title>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</title>
      <link>https://paperswithcode.com/paper/repurposing-diffusion-based-image-generators</link>
      <description><![CDATA[Monocular depth estimation is a fundamental computer vision task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repurposing-diffusion-based-image-generators</guid>
    </item>
    <item>
      <title>Using Sequences of Life-events to Predict Human Lives</title>
      <link>https://paperswithcode.com/paper/using-sequences-of-life-events-to-predict</link>
      <description><![CDATA[We can also represent human lives in a way that shares this structural similarity to language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/using-sequences-of-life-events-to-predict</guid>
    </item>
    <item>
      <title>Osprey: Pixel Understanding with Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/osprey-pixel-understanding-with-visual</link>
      <description><![CDATA[In this paper, we propose Osprey, a mask-text instruction tuning approach, to extend MLLMs by incorporating fine-grained mask regions into language instruction, aiming at achieving pixel-wise visual understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/osprey-pixel-understanding-with-visual</guid>
    </item>
    <item>
      <title>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</title>
      <link>https://paperswithcode.com/paper/longllmlingua-accelerating-and-enhancing-llms</link>
      <description><![CDATA[Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longllmlingua-accelerating-and-enhancing-llms</guid>
    </item>
    <item>
      <title>Simplifying and Empowering Transformers for Large-Graph Representations</title>
      <link>https://paperswithcode.com/paper/simplifying-and-empowering-transformers-for-1</link>
      <description><![CDATA[Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simplifying-and-empowering-transformers-for-1</guid>
    </item>
    <item>
      <title>UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition</title>
      <link>https://paperswithcode.com/paper/unireplknet-a-universal-perception-large</link>
      <description><![CDATA[1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unireplknet-a-universal-perception-large</guid>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</link>
      <description><![CDATA[RAG effectively combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases, making it one of the most important methods for implementing large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</guid>
    </item>
    <item>
      <title>Generative Multimodal Models are In-Context Learners</title>
      <link>https://paperswithcode.com/paper/generative-multimodal-models-are-in-context</link>
      <description><![CDATA[The human ability to easily solve multimodal tasks in context (i. e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-multimodal-models-are-in-context</guid>
    </item>
    <item>
      <title>TinySAM: Pushing the Envelope for Efficient Segment Anything Model</title>
      <link>https://paperswithcode.com/paper/tinysam-pushing-the-envelope-for-efficient</link>
      <description><![CDATA[We first propose a full-stage knowledge distillation method with online hard prompt sampling strategy to distill a lightweight student model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinysam-pushing-the-envelope-for-efficient</guid>
    </item>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia</title>
      <link>https://paperswithcode.com/paper/generative-agent-based-modeling-with-actions</link>
      <description><![CDATA[Agent-based modeling has been around for decades, and applied widely across the social and natural sciences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-agent-based-modeling-with-actions</guid>
    </item>
    <item>
      <title>Model scale versus domain knowledge in statistical forecasting of chaotic systems</title>
      <link>https://paperswithcode.com/paper/large-statistical-learning-models-effectively</link>
      <description><![CDATA[Here, we perform the largest to-date comparative study of forecasting methods on the classical problem of forecasting chaos: we benchmark 24 state-of-the-art forecasting methods on a crowdsourced database of 135 low-dimensional systems with 17 forecast metrics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-statistical-learning-models-effectively</guid>
    </item>
  </channel>
</rss>
