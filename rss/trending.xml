<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 24 Jul 2022 09:14:34 +0000</lastBuildDate>
    <item>
      <title>XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model</title>
      <link>https://paperswithcode.com/paper/xmem-long-term-video-object-segmentation-with</link>
      <description><![CDATA[We present XMem, a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/xmem-long-term-video-object-segmentation-with</guid>
    </item>
    <item>
      <title>In Defense of Online Models for Video Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/in-defense-of-online-models-for-video</link>
      <description><![CDATA[In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-defense-of-online-models-for-video</guid>
    </item>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>Towards Grand Unification of Object Tracking</title>
      <link>https://paperswithcode.com/paper/towards-grand-unification-of-object-tracking</link>
      <description><![CDATA[We present a unified method, termed Unicorn, that can simultaneously solve four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the same model parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-grand-unification-of-object-tracking</guid>
    </item>
    <item>
      <title>OCR-free Document Understanding Transformer</title>
      <link>https://paperswithcode.com/paper/donut-document-understanding-transformer</link>
      <description><![CDATA[Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/donut-document-understanding-transformer</guid>
    </item>
    <item>
      <title>3D Clothed Human Reconstruction in the Wild</title>
      <link>https://paperswithcode.com/paper/3d-clothed-human-reconstruction-in-the-wild</link>
      <description><![CDATA[Although much progress has been made in 3D clothed human reconstruction, most of the existing methods fail to produce robust results from in-the-wild images, which contain diverse human poses and appearances.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-clothed-human-reconstruction-in-the-wild</guid>
    </item>
    <item>
      <title>Improved Vector Quantized Diffusion Models</title>
      <link>https://paperswithcode.com/paper/improved-vector-quantized-diffusion-models</link>
      <description><![CDATA[When trained on ImageNet, we dramatically improve the FID score from 11. 89 to 4. 83, demonstrating the superiority of our proposed techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improved-vector-quantized-diffusion-models</guid>
    </item>
    <item>
      <title>Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild</title>
      <link>https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</link>
      <description><![CDATA[Omni3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 97 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</guid>
    </item>
    <item>
      <title>Generative Multiplane Images: Making a 2D GAN 3D-Aware</title>
      <link>https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</link>
      <description><![CDATA[What is really needed to make an existing 2D GAN 3D-aware?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</guid>
    </item>
    <item>
      <title>Collaborative Neural Rendering using Anime Character Sheets</title>
      <link>https://paperswithcode.com/paper/collaborative-neural-rendering-using-anime</link>
      <description><![CDATA[Drawing images of characters at desired poses is an essential but laborious task in anime production.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/collaborative-neural-rendering-using-anime</guid>
    </item>
    <item>
      <title>AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/adanerf-adaptive-sampling-for-real-time</link>
      <description><![CDATA[However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adanerf-adaptive-sampling-for-real-time</guid>
    </item>
    <item>
      <title>Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics</title>
      <link>https://paperswithcode.com/paper/towards-scale-aware-robust-and-generalizable</link>
      <description><![CDATA[Unsupervised monocular depth and ego-motion estimation has drawn extensive research attention in recent years.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-scale-aware-robust-and-generalizable</guid>
    </item>
    <item>
      <title>Demystifying MMD GANs</title>
      <link>https://paperswithcode.com/paper/demystifying-mmd-gans</link>
      <description><![CDATA[We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demystifying-mmd-gans</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Tracking Objects as Pixel-wise Distributions</title>
      <link>https://paperswithcode.com/paper/tracking-objects-as-pixel-wise-distributions</link>
      <description><![CDATA[During inference, a pixel-wise association procedure is proposed to recover object connections through frames based on the pixel-wise prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tracking-objects-as-pixel-wise-distributions</guid>
    </item>
    <item>
      <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
      <link>https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</link>
      <description><![CDATA[This paper deals with the problem of audio source separation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</guid>
    </item>
    <item>
      <title>Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis</title>
      <link>https://paperswithcode.com/paper/injecting-3d-perception-of-controllable-nerf</link>
      <description><![CDATA[To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/injecting-3d-perception-of-controllable-nerf</guid>
    </item>
    <item>
      <title>AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection</title>
      <link>https://paperswithcode.com/paper/autoalignv2-deformable-feature-aggregation</link>
      <description><![CDATA[Recently, AutoAlign presents a learnable paradigm in combining these two modalities for 3D object detection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoalignv2-deformable-feature-aggregation</guid>
    </item>
    <item>
      <title>Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution</title>
      <link>https://paperswithcode.com/paper/open-high-resolution-satellite-imagery-the</link>
      <description><![CDATA[We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/open-high-resolution-satellite-imagery-the</guid>
    </item>
    <item>
      <title>Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression</title>
      <link>https://paperswithcode.com/paper/unsupervised-night-image-enhancement-when</link>
      <description><![CDATA[To address this problem, we need to suppress the light effects in bright regions while, at the same time, boosting the intensity of dark regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-night-image-enhancement-when</guid>
    </item>
  </channel>
</rss>
