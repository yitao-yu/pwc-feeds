<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 11 Aug 2022 09:15:29 +0000</lastBuildDate>
    <item>
      <title>3D Vision with Transformers: A Survey</title>
      <link>https://paperswithcode.com/paper/3d-vision-with-transformers-a-survey</link>
      <description><![CDATA[The success of the transformer architecture in natural language processing has recently triggered attention in the computer vision field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-vision-with-transformers-a-survey</guid>
    </item>
    <item>
      <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
      <link>https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</link>
      <description><![CDATA[This paper deals with the problem of audio source separation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</guid>
    </item>
    <item>
      <title>Hybrid Spectrogram and Waveform Source Separation</title>
      <link>https://paperswithcode.com/paper/hybrid-spectrogram-and-waveform-source</link>
      <description><![CDATA[Source separation models either work on the spectrogram or waveform domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hybrid-spectrogram-and-waveform-source</guid>
    </item>
    <item>
      <title>Pretraining is All You Need for Image-to-Image Translation</title>
      <link>https://paperswithcode.com/paper/pretraining-is-all-you-need-for-image-to</link>
      <description><![CDATA[We propose to use pretraining to boost general image-to-image translation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pretraining-is-all-you-need-for-image-to</guid>
    </item>
    <item>
      <title>MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures</title>
      <link>https://paperswithcode.com/paper/mobilenerf-exploiting-the-polygon</link>
      <description><![CDATA[Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilenerf-exploiting-the-polygon</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>Reconstructing 3D Human Pose by Watching Humans in the Mirror</title>
      <link>https://paperswithcode.com/paper/reconstructing-3d-human-pose-by-watching</link>
      <description><![CDATA[In this paper, we introduce the new task of reconstructing 3D human pose from a single image in which we can see the person and the person's image through a mirror.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reconstructing-3d-human-pose-by-watching</guid>
    </item>
    <item>
      <title>Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language</title>
      <link>https://paperswithcode.com/paper/multi-scale-2d-temporal-adjacent-networks-for</link>
      <description><![CDATA[It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-2d-temporal-adjacent-networks-for</guid>
    </item>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Expanding Language-Image Pretrained Models for General Video Recognition</title>
      <link>https://paperswithcode.com/paper/expanding-language-image-pretrained-models</link>
      <description><![CDATA[Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/expanding-language-image-pretrained-models</guid>
    </item>
    <item>
      <title>Package for Fast ABC-Boost</title>
      <link>https://paperswithcode.com/paper/package-for-fast-abc-boost</link>
      <description><![CDATA[Although the gain formula in Li (2010) was derived for logistic regression loss, it is a generic formula for loss functions with second-derivatives.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/package-for-fast-abc-boost</guid>
    </item>
    <item>
      <title>GAUDI: A Neural Architect for Immersive 3D Scene Generation</title>
      <link>https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</link>
      <description><![CDATA[We introduce GAUDI, a generative model capable of capturing the distribution of complex and realistic 3D scenes that can be rendered immersively from a moving camera.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaudi-a-neural-architect-for-immersive-3d</guid>
    </item>
    <item>
      <title>In Defense of Online Models for Video Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/in-defense-of-online-models-for-video</link>
      <description><![CDATA[In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-defense-of-online-models-for-video</guid>
    </item>
    <item>
      <title>ParlAI</title>
      <link>https://github.com/facebookresearch/ParlAI</link>
      <description><![CDATA[A framework for training and evaluating AI models on a variety of openly available dialogue datasets.]]></description>
      <guid isPermaLink="true">https://github.com/facebookresearch/ParlAI</guid>
    </item>
    <item>
      <title>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</title>
      <link>https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</link>
      <description><![CDATA[Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</guid>
    </item>
    <item>
      <title>A Conversational Paradigm for Program Synthesis</title>
      <link>https://paperswithcode.com/paper/a-conversational-paradigm-for-program</link>
      <description><![CDATA[We train a family of large language models, called CodeGen, on natural language and programming language data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-conversational-paradigm-for-program</guid>
    </item>
    <item>
      <title>Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models</title>
      <link>https://paperswithcode.com/paper/text-guided-synthesis-of-artistic-images-with</link>
      <description><![CDATA[In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-guided-synthesis-of-artistic-images-with</guid>
    </item>
    <item>
      <title>DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</title>
      <link>https://paperswithcode.com/paper/dpm-solver-a-fast-ode-solver-for-diffusion</link>
      <description><![CDATA[In this work, we propose an exact formulation of the solution of diffusion ODEs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dpm-solver-a-fast-ode-solver-for-diffusion</guid>
    </item>
    <item>
      <title>Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios</title>
      <link>https://paperswithcode.com/paper/next-vit-next-generation-vision-transformer</link>
      <description><![CDATA[Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/next-vit-next-generation-vision-transformer</guid>
    </item>
  </channel>
</rss>
