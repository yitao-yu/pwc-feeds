<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 27 Aug 2023 09:09:59 +0000</lastBuildDate>
    <item>
      <title>Code Llama: Open Foundation Models for Code</title>
      <link>https://paperswithcode.com/paper/code-llama-open-foundation-models-for-code</link>
      <description><![CDATA[We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/code-llama-open-foundation-models-for-code</guid>
    </item>
    <item>
      <title>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</title>
      <link>https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</link>
      <description><![CDATA[What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</guid>
    </item>
    <item>
      <title>CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</title>
      <link>https://paperswithcode.com/paper/codef-content-deformation-fields-for</link>
      <description><![CDATA[We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i. e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e. g., the object shape) from the video. With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found at https://qiuyu96. github. io/CoDeF/.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codef-content-deformation-fields-for</guid>
    </item>
    <item>
      <title>Prompt2Model: Generating Deployable Models from Natural Language Instructions</title>
      <link>https://paperswithcode.com/paper/prompt2model-generating-deployable-models</link>
      <description><![CDATA[In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompt2model-generating-deployable-models</guid>
    </item>
    <item>
      <title>Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities</title>
      <link>https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</link>
      <description><![CDATA[We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</guid>
    </item>
    <item>
      <title>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</title>
      <link>https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</link>
      <description><![CDATA[In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</guid>
    </item>
    <item>
      <title>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</title>
      <link>https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</link>
      <description><![CDATA[We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</guid>
    </item>
    <item>
      <title>Dense Text-to-Image Generation with Attention Modulation</title>
      <link>https://paperswithcode.com/paper/dense-text-to-image-generation-with-attention</link>
      <description><![CDATA[To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dense-text-to-image-generation-with-attention</guid>
    </item>
    <item>
      <title>ChatHaruhi: Reviving Anime Character in Reality via Large Language Model</title>
      <link>https://paperswithcode.com/paper/chatharuhi-reviving-anime-character-in</link>
      <description><![CDATA[Role-playing chatbots built on large language models have drawn interest, but better techniques are needed to enable mimicking specific fictional characters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatharuhi-reviving-anime-character-in</guid>
    </item>
    <item>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
      <link>https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</link>
      <description><![CDATA[Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</guid>
    </item>
    <item>
      <title>Neuralangelo: High-Fidelity Neural Surface Reconstruction</title>
      <link>https://paperswithcode.com/paper/neuralangelo-high-fidelity-neural-surface-1</link>
      <description><![CDATA[Neural surface reconstruction has been shown to be powerful for recovering dense 3D surfaces via image-based neural rendering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neuralangelo-high-fidelity-neural-surface-1</guid>
    </item>
    <item>
      <title>Giraffe: Adventures in Expanding Context Lengths in LLMs</title>
      <link>https://paperswithcode.com/paper/giraffe-adventures-in-expanding-context</link>
      <description><![CDATA[To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/giraffe-adventures-in-expanding-context</guid>
    </item>
    <item>
      <title>A Survey on Large Language Model based Autonomous Agents</title>
      <link>https://paperswithcode.com/paper/a-survey-on-large-language-model-based</link>
      <description><![CDATA[Based on the previous studies, we also present several challenges and future directions in this field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-large-language-model-based</guid>
    </item>
    <item>
      <title>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</title>
      <link>https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</link>
      <description><![CDATA[Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</guid>
    </item>
    <item>
      <title>TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition</title>
      <link>https://paperswithcode.com/paper/tf-icon-diffusion-based-training-free-cross</link>
      <description><![CDATA[In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tf-icon-diffusion-based-training-free-cross</guid>
    </item>
    <item>
      <title>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</title>
      <link>https://paperswithcode.com/paper/scenimefy-learning-to-craft-anime-scene-via</link>
      <description><![CDATA[The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scenimefy-learning-to-craft-anime-scene-via</guid>
    </item>
    <item>
      <title>DiffusionTrack: Diffusion Model For Multi-Object Tracking</title>
      <link>https://paperswithcode.com/paper/diffusiontrack-diffusion-model-for-multi</link>
      <description><![CDATA[In inference, the model refines a set of paired randomly generated boxes to the detection and tracking results in a flexible one-step or multi-step denoising diffusion process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusiontrack-diffusion-model-for-multi</guid>
    </item>
    <item>
      <title>SONAR: Sentence-Level Multimodal and Language-Agnostic Representations</title>
      <link>https://paperswithcode.com/paper/sentence-level-multimodal-and-language</link>
      <description><![CDATA[Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sentence-level-multimodal-and-language</guid>
    </item>
    <item>
      <title>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</title>
      <link>https://paperswithcode.com/paper/neus-learning-neural-implicit-surfaces-by</link>
      <description><![CDATA[In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neus-learning-neural-implicit-surfaces-by</guid>
    </item>
    <item>
      <title>StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views</title>
      <link>https://paperswithcode.com/paper/streetsurf-extending-multi-view-implicit</link>
      <description><![CDATA[We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streetsurf-extending-multi-view-implicit</guid>
    </item>
  </channel>
</rss>
