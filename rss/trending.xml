<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 02 Feb 2024 21:06:35 +0000</lastBuildDate>
    <item>
      <title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title>
      <link>https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</link>
      <description><![CDATA[The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</guid>
    </item>
    <item>
      <title>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</link>
      <description><![CDATA[In this work, we propose a novel training strategy MoE-tuning for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</guid>
    </item>
    <item>
      <title>Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</title>
      <link>https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</link>
      <description><![CDATA[To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</guid>
    </item>
    <item>
      <title>High-Quality Image Restoration Following Human Instructions</title>
      <link>https://paperswithcode.com/paper/high-quality-image-restoration-following</link>
      <description><![CDATA[All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-quality-image-restoration-following</guid>
    </item>
    <item>
      <title>InstantID: Zero-shot Identity-Preserving Generation in Seconds</title>
      <link>https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</link>
      <description><![CDATA[There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</title>
      <link>https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</link>
      <description><![CDATA[To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</guid>
    </item>
    <item>
      <title>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</title>
      <link>https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</link>
      <description><![CDATA[Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</guid>
    </item>
    <item>
      <title>Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization</title>
      <link>https://paperswithcode.com/paper/flexibly-scaling-large-language-models</link>
      <description><![CDATA[Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flexibly-scaling-large-language-models</guid>
    </item>
    <item>
      <title>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</title>
      <link>https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</link>
      <description><![CDATA[Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</guid>
    </item>
    <item>
      <title>LongAlign: A Recipe for Long Context Alignment of Large Language Models</title>
      <link>https://paperswithcode.com/paper/longalign-a-recipe-for-long-context-alignment</link>
      <description><![CDATA[Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longalign-a-recipe-for-long-context-alignment</guid>
    </item>
    <item>
      <title>InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model</title>
      <link>https://paperswithcode.com/paper/internlm-xcomposer2-mastering-free-form-text</link>
      <description><![CDATA[We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internlm-xcomposer2-mastering-free-form-text</guid>
    </item>
    <item>
      <title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</title>
      <link>https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</link>
      <description><![CDATA[In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</guid>
    </item>
    <item>
      <title>In-Context Learning for Extreme Multi-Label Classification</title>
      <link>https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</link>
      <description><![CDATA[Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</guid>
    </item>
    <item>
      <title>DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</link>
      <description><![CDATA[The rapid development of large language models has revolutionized code intelligence in software development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-when-the-large-language-model</guid>
    </item>
    <item>
      <title>MouSi: Poly-Visual-Expert Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/mousi-poly-visual-expert-vision-language</link>
      <description><![CDATA[This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mousi-poly-visual-expert-vision-language</guid>
    </item>
    <item>
      <title>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</title>
      <link>https://paperswithcode.com/paper/dspy-compiling-declarative-language-model</link>
      <description><![CDATA[The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dspy-compiling-declarative-language-model</guid>
    </item>
    <item>
      <title>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</title>
      <link>https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation</link>
      <description><![CDATA[The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation</guid>
    </item>
    <item>
      <title>DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines</title>
      <link>https://paperswithcode.com/paper/dspy-assertions-computational-constraints-for</link>
      <description><![CDATA[Our reference implementation of LM Assertions is integrated into DSPy at https://github. com/stanfordnlp/dspy]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dspy-assertions-computational-constraints-for</guid>
    </item>
    <item>
      <title>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</title>
      <link>https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</link>
      <description><![CDATA[In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-vl-a-frontier-large-vision-language</guid>
    </item>
    <item>
      <title>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook</title>
      <link>https://paperswithcode.com/paper/large-models-for-time-series-and-spatio</link>
      <description><![CDATA[In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-models-for-time-series-and-spatio</guid>
    </item>
  </channel>
</rss>
