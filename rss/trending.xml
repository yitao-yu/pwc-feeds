<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 15 Oct 2023 21:05:23 +0000</lastBuildDate>
    <item>
      <title>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</title>
      <link>https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</link>
      <description><![CDATA[AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</guid>
    </item>
    <item>
      <title>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</title>
      <link>https://paperswithcode.com/paper/scalecrafter-tuning-free-higher-resolution</link>
      <description><![CDATA[Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalecrafter-tuning-free-higher-resolution</guid>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <link>https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</link>
      <description><![CDATA[We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</guid>
    </item>
    <item>
      <title>Separate Anything You Describe</title>
      <link>https://paperswithcode.com/paper/separate-anything-you-describe</link>
      <description><![CDATA[In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/separate-anything-you-describe</guid>
    </item>
    <item>
      <title>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</link>
      <description><![CDATA[In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</guid>
    </item>
    <item>
      <title>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving</title>
      <link>https://paperswithcode.com/paper/tora-a-tool-integrated-reasoning-agent-for</link>
      <description><![CDATA[Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tora-a-tool-integrated-reasoning-agent-for</guid>
    </item>
    <item>
      <title>Improved Baselines with Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/improved-baselines-with-visual-instruction</link>
      <description><![CDATA[Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improved-baselines-with-visual-instruction</guid>
    </item>
    <item>
      <title>Text Embeddings Reveal (Almost) As Much As Text</title>
      <link>https://paperswithcode.com/paper/text-embeddings-reveal-almost-as-much-as-text</link>
      <description><![CDATA[How much private information do text embeddings reveal about the original text?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-embeddings-reveal-almost-as-much-as-text</guid>
    </item>
    <item>
      <title>UniPose: Detecting Any Keypoints</title>
      <link>https://paperswithcode.com/paper/unipose-detecting-any-keypoints</link>
      <description><![CDATA[This work proposes a unified framework called UniPose to detect keypoints of any articulated (e. g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unipose-detecting-any-keypoints</guid>
    </item>
    <item>
      <title>Efficient Streaming Language Models with Attention Sinks</title>
      <link>https://paperswithcode.com/paper/efficient-streaming-language-models-with</link>
      <description><![CDATA[In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-streaming-language-models-with</guid>
    </item>
    <item>
      <title>Mistral 7B</title>
      <link>https://paperswithcode.com/paper/mistral-7b</link>
      <description><![CDATA[We introduce Mistral 7B v0. 1, a 7-billion-parameter language model engineered for superior performance and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mistral-7b</guid>
    </item>
    <item>
      <title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</title>
      <link>https://paperswithcode.com/paper/minigpt-5-interleaved-vision-and-language</link>
      <description><![CDATA[Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minigpt-5-interleaved-vision-and-language</guid>
    </item>
    <item>
      <title>EasyPhoto: Your Smart AI Photo Generator</title>
      <link>https://paperswithcode.com/paper/easyphoto-your-smart-ai-photo-generator</link>
      <description><![CDATA[By training a digital doppelganger of a specific user ID using 5 to 20 relevant images, the finetuned model (according to the trained LoRA model) allows for the generation of AI photos using arbitrary templates.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyphoto-your-smart-ai-photo-generator</guid>
    </item>
    <item>
      <title>Controlling Vision-Language Models for Universal Image Restoration</title>
      <link>https://paperswithcode.com/paper/controlling-vision-language-models-for</link>
      <description><![CDATA[In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a universal framework for image restoration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controlling-vision-language-models-for</guid>
    </item>
    <item>
      <title>Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition</title>
      <link>https://paperswithcode.com/paper/whispering-llama-a-cross-modal-generative</link>
      <description><![CDATA[We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/whispering-llama-a-cross-modal-generative</guid>
    </item>
    <item>
      <title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title>
      <link>https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</link>
      <description><![CDATA[In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</guid>
    </item>
    <item>
      <title>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</title>
      <link>https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long</link>
      <description><![CDATA[LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longlora-efficient-fine-tuning-of-long</guid>
    </item>
    <item>
      <title>Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models</title>
      <link>https://paperswithcode.com/paper/mini-dalle3-interactive-text-to-image-by</link>
      <description><![CDATA[The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-dalle3-interactive-text-to-image-by</guid>
    </item>
    <item>
      <title>InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists</title>
      <link>https://paperswithcode.com/paper/instructcv-instruction-tuned-text-to-image</link>
      <description><![CDATA[We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructcv-instruction-tuned-text-to-image</guid>
    </item>
    <item>
      <title>Uni3D: Exploring Unified 3D Representation at Scale</title>
      <link>https://paperswithcode.com/paper/uni3d-exploring-unified-3d-representation-at</link>
      <description><![CDATA[Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uni3d-exploring-unified-3d-representation-at</guid>
    </item>
  </channel>
</rss>
