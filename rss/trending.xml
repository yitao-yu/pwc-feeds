<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 24 Mar 2024 09:13:12 +0000</lastBuildDate>
    <item>
      <title>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</title>
      <link>https://paperswithcode.com/paper/grokking-generalization-beyond-overfitting-on</link>
      <description><![CDATA[In this paper we propose to study generalization of neural networks on small algorithmically generated datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grokking-generalization-beyond-overfitting-on</guid>
    </item>
    <item>
      <title>Evolutionary Optimization of Model Merging Recipes</title>
      <link>https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</link>
      <description><![CDATA[Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</guid>
    </item>
    <item>
      <title>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</title>
      <link>https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</link>
      <description><![CDATA[Sora is the first large-scale generalist video generation model that garnered significant attention across society.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</guid>
    </item>
    <item>
      <title>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</title>
      <link>https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</link>
      <description><![CDATA[We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</guid>
    </item>
    <item>
      <title>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</title>
      <link>https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</link>
      <description><![CDATA[We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0. 1s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</guid>
    </item>
    <item>
      <title>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</title>
      <link>https://paperswithcode.com/paper/fresco-spatial-temporal-correspondence-for</link>
      <description><![CDATA[In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fresco-spatial-temporal-correspondence-for</guid>
    </item>
    <item>
      <title>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</title>
      <link>https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</link>
      <description><![CDATA[Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</guid>
    </item>
    <item>
      <title>LLM4Decompile: Decompiling Binary Code with Large Language Models</title>
      <link>https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</link>
      <description><![CDATA[Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</guid>
    </item>
    <item>
      <title>Chronos: Learning the Language of Time Series</title>
      <link>https://paperswithcode.com/paper/chronos-learning-the-language-of-time-series</link>
      <description><![CDATA[We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chronos-learning-the-language-of-time-series</guid>
    </item>
    <item>
      <title>OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models</title>
      <link>https://paperswithcode.com/paper/omg-occlusion-friendly-personalized-multi</link>
      <description><![CDATA[We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omg-occlusion-friendly-personalized-multi</guid>
    </item>
    <item>
      <title>One-Step Image Translation with Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</link>
      <description><![CDATA[In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</guid>
    </item>
    <item>
      <title>Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models</title>
      <link>https://paperswithcode.com/paper/agent-flan-designing-data-and-methods-of</link>
      <description><![CDATA[Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-flan-designing-data-and-methods-of</guid>
    </item>
    <item>
      <title>APISR: Anime Production Inspired Real-World Anime Super-Resolution</title>
      <link>https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</link>
      <description><![CDATA[In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</guid>
    </item>
    <item>
      <title>StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control</title>
      <link>https://paperswithcode.com/paper/streammultidiffusion-real-time-interactive</link>
      <description><![CDATA[The enormous success of diffusion models in text-to-image synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streammultidiffusion-real-time-interactive</guid>
    </item>
    <item>
      <title>RewardBench: Evaluating Reward Models for Language Modeling</title>
      <link>https://paperswithcode.com/paper/rewardbench-evaluating-reward-models-for</link>
      <description><![CDATA[In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rewardbench-evaluating-reward-models-for</guid>
    </item>
    <item>
      <title>Aggregated Contextual Transformations for High-Resolution Image Inpainting</title>
      <link>https://paperswithcode.com/paper/aggregated-contextual-transformations-for</link>
      <description><![CDATA[For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aggregated-contextual-transformations-for</guid>
    </item>
    <item>
      <title>General Object Foundation Model for Images and Videos at Scale</title>
      <link>https://paperswithcode.com/paper/general-object-foundation-model-for-images</link>
      <description><![CDATA[We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-object-foundation-model-for-images</guid>
    </item>
    <item>
      <title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
      <link>https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</link>
      <description><![CDATA[To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</guid>
    </item>
    <item>
      <title>UniTS: Building a Unified Time Series Model</title>
      <link>https://paperswithcode.com/paper/units-building-a-unified-time-series-model</link>
      <description><![CDATA[However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/units-building-a-unified-time-series-model</guid>
    </item>
    <item>
      <title>When Do We Not Need Larger Vision Models?</title>
      <link>https://paperswithcode.com/paper/when-do-we-not-need-larger-vision-models</link>
      <description><![CDATA[Our results show that a multi-scale smaller model has comparable learning capacity to a larger model, and pre-training smaller models with S$^2$ can match or even exceed the advantage of larger models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/when-do-we-not-need-larger-vision-models</guid>
    </item>
  </channel>
</rss>
