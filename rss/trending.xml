<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 01 Oct 2022 09:20:32 +0000</lastBuildDate>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Human Motion Diffusion Model</title>
      <link>https://paperswithcode.com/paper/human-motion-diffusion-model</link>
      <description><![CDATA[In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-motion-diffusion-model</guid>
    </item>
    <item>
      <title>Efficient Few-Shot Learning Without Prompts</title>
      <link>https://paperswithcode.com/paper/efficient-few-shot-learning-without-prompts</link>
      <description><![CDATA[This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-few-shot-learning-without-prompts</guid>
    </item>
    <item>
      <title>Learning to Learn with Generative Models of Neural Network Checkpoints</title>
      <link>https://paperswithcode.com/paper/learning-to-learn-with-generative-models-of</link>
      <description><![CDATA[We explore a data-driven approach for learning to optimize neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-learn-with-generative-models-of</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale</guid>
    </item>
    <item>
      <title>KILT: a Benchmark for Knowledge Intensive Language Tasks</title>
      <link>https://paperswithcode.com/paper/kilt-a-benchmark-for-knowledge-intensive</link>
      <description><![CDATA[We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kilt-a-benchmark-for-knowledge-intensive</guid>
    </item>
    <item>
      <title>LAVIS: A Library for Language-Vision Intelligence</title>
      <link>https://paperswithcode.com/paper/lavis-a-library-for-language-vision</link>
      <description><![CDATA[We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lavis-a-library-for-language-vision</guid>
    </item>
    <item>
      <title>TVLT: Textless Vision-Language Transformer</title>
      <link>https://paperswithcode.com/paper/tvlt-textless-vision-language-transformer</link>
      <description><![CDATA[In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tvlt-textless-vision-language-transformer</guid>
    </item>
    <item>
      <title>towhee</title>
      <link>https://github.com/towhee-io/towhee</link>
      <description><![CDATA[Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.]]></description>
      <guid isPermaLink="true">https://github.com/towhee-io/towhee</guid>
    </item>
    <item>
      <title>Zero-Shot Text-Guided Object Generation with Dream Fields</title>
      <link>https://paperswithcode.com/paper/zero-shot-text-guided-object-generation-with</link>
      <description><![CDATA[Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-text-guided-object-generation-with</guid>
    </item>
    <item>
      <title>Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks</title>
      <link>https://paperswithcode.com/paper/obj2seq-formatting-objects-as-sequences-with</link>
      <description><![CDATA[Obj2Seq is able to flexibly determine input categories to satisfy customized requirements, and be easily extended to different visual tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/obj2seq-formatting-objects-as-sequences-with</guid>
    </item>
    <item>
      <title>High-Resolution Image Synthesis with Latent Diffusion Models</title>
      <link>https://paperswithcode.com/paper/high-resolution-image-synthesis-with-latent</link>
      <description><![CDATA[By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-resolution-image-synthesis-with-latent</guid>
    </item>
    <item>
      <title>Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</title>
      <link>https://paperswithcode.com/paper/analog-bits-generating-discrete-data-using</link>
      <description><![CDATA[The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/analog-bits-generating-discrete-data-using</guid>
    </item>
    <item>
      <title>Min-Max Similarity: A Contrastive Semi-Supervised Deep Learning Network for Surgical Tools Segmentation</title>
      <link>https://paperswithcode.com/paper/min-max-similarity-a-contrastive-learning</link>
      <description><![CDATA[To address this issue, we proposed a semi-supervised segmentation network based on contrastive learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/min-max-similarity-a-contrastive-learning</guid>
    </item>
    <item>
      <title>VToonify: Controllable High-Resolution Portrait Video Style Transfer</title>
      <link>https://paperswithcode.com/paper/vtoonify-controllable-high-resolution</link>
      <description><![CDATA[Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vtoonify-controllable-high-resolution</guid>
    </item>
    <item>
      <title>NP-Match: When Neural Processes meet Semi-Supervised Learning</title>
      <link>https://paperswithcode.com/paper/np-match-when-neural-processes-meet-semi</link>
      <description><![CDATA[Semi-supervised learning (SSL) has been widely explored in recent years, and it is an effective way of leveraging unlabeled data to reduce the reliance on labeled data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/np-match-when-neural-processes-meet-semi</guid>
    </item>
    <item>
      <title>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</title>
      <link>https://paperswithcode.com/paper/dino-detr-with-improved-denoising-anchor-1</link>
      <description><![CDATA[Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dino-detr-with-improved-denoising-anchor-1</guid>
    </item>
    <item>
      <title>Poisson Flow Generative Models</title>
      <link>https://paperswithcode.com/paper/poisson-flow-generative-models</link>
      <description><![CDATA[We interpret the data points as electrical charges on the $z=0$ hyperplane in a space augmented with an additional dimension $z$, generating a high-dimensional electric field (the gradient of the solution to Poisson equation).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/poisson-flow-generative-models</guid>
    </item>
    <item>
      <title>SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/segnext-rethinking-convolutional-attention</link>
      <description><![CDATA[Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90. 6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segnext-rethinking-convolutional-attention</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
  </channel>
</rss>
