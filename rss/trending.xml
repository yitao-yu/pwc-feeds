<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 23 May 2025 09:19:35 +0000</lastBuildDate>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment</title>
      <link>https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</link>
      <description><![CDATA[In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</guid>
    </item>
    <item>
      <title>Aligning Anime Video Generation with Human Feedback</title>
      <link>https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</link>
      <description><![CDATA[Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</guid>
    </item>
    <item>
      <title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
      <link>https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</link>
      <description><![CDATA[Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</guid>
    </item>
    <item>
      <title>Parallel Scaling Law for Language Models</title>
      <link>https://paperswithcode.com/paper/parallel-scaling-law-for-language-models</link>
      <description><![CDATA[We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/parallel-scaling-law-for-language-models</guid>
    </item>
    <item>
      <title>Fully Open Source Moxin-7B Technical Report</title>
      <link>https://paperswithcode.com/paper/fully-open-source-moxin-7b-technical-report</link>
      <description><![CDATA[Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fully-open-source-moxin-7b-technical-report</guid>
    </item>
    <item>
      <title>Thinkless: LLM Learns When to Think</title>
      <link>https://paperswithcode.com/paper/thinkless-llm-learns-when-to-think</link>
      <description><![CDATA[Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/thinkless-llm-learns-when-to-think</guid>
    </item>
    <item>
      <title>Visual Planning: Let's Think Only with Images</title>
      <link>https://paperswithcode.com/paper/2505-11409</link>
      <description><![CDATA[Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2505-11409</guid>
    </item>
    <item>
      <title>Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation &amp; Smoke-Tests for Continuous LLM Evaluation</title>
      <link>https://paperswithcode.com/paper/tiny-qa-benchmark-ultra-lightweight-synthetic</link>
      <description><![CDATA[Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tiny-qa-benchmark-ultra-lightweight-synthetic</guid>
    </item>
    <item>
      <title>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</title>
      <link>https://paperswithcode.com/paper/from-automation-to-autonomy-a-survey-on-large</link>
      <description><![CDATA[Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-automation-to-autonomy-a-survey-on-large</guid>
    </item>
    <item>
      <title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
      <link>https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</link>
      <description><![CDATA[At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</guid>
    </item>
    <item>
      <title>Group-in-Group Policy Optimization for LLM Agent Training</title>
      <link>https://paperswithcode.com/paper/2505-10978</link>
      <description><![CDATA[In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2505-10978</guid>
    </item>
    <item>
      <title>VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/visionreasoner-unified-visual-perception-and</link>
      <description><![CDATA[Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visionreasoner-unified-visual-perception-and</guid>
    </item>
    <item>
      <title>SOAP: Style-Omniscient Animatable Portraits</title>
      <link>https://paperswithcode.com/paper/soap-style-omniscient-animatable-portraits</link>
      <description><![CDATA[Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soap-style-omniscient-animatable-portraits</guid>
    </item>
    <item>
      <title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
      <link>https://paperswithcode.com/paper/mtvcrafter-4d-motion-tokenization-for-open</link>
      <description><![CDATA[To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i. e., 4D motion) for human image animation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mtvcrafter-4d-motion-tokenization-for-open</guid>
    </item>
    <item>
      <title>AdaptThink: Reasoning Models Can Learn When to Think</title>
      <link>https://paperswithcode.com/paper/adaptthink-reasoning-models-can-learn-when-to</link>
      <description><![CDATA[Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptthink-reasoning-models-can-learn-when-to</guid>
    </item>
    <item>
      <title>Spherical Channels for Modeling Atomic Interactions</title>
      <link>https://paperswithcode.com/paper/spherical-channels-for-modeling-atomic</link>
      <description><![CDATA[We propose the Spherical Channel Network (SCN) to model atomic energies and forces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spherical-channels-for-modeling-atomic</guid>
    </item>
    <item>
      <title>Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/grouping-first-attending-smartly-training</link>
      <description><![CDATA[We validate GRAT on pretrained Flux and HunyuanVideo for image and video generation, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grouping-first-attending-smartly-training</guid>
    </item>
    <item>
      <title>KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</title>
      <link>https://paperswithcode.com/paper/korgym-a-dynamic-game-platform-for-llm</link>
      <description><![CDATA[Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/korgym-a-dynamic-game-platform-for-llm</guid>
    </item>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</link>
      <description><![CDATA[Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</guid>
    </item>
  </channel>
</rss>
