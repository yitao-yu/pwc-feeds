<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 06 Nov 2023 09:12:53 +0000</lastBuildDate>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling</title>
      <link>https://paperswithcode.com/paper/distil-whisper-robust-knowledge-distillation</link>
      <description><![CDATA[As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/distil-whisper-robust-knowledge-distillation</guid>
    </item>
    <item>
      <title>Low-latency Real-time Voice Conversion on CPU</title>
      <link>https://paperswithcode.com/paper/low-latency-real-time-voice-conversion-on-cpu</link>
      <description><![CDATA[To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/low-latency-real-time-voice-conversion-on-cpu</guid>
    </item>
    <item>
      <title>VideoCrafter1: Open Diffusion Models for High-Quality Video Generation</title>
      <link>https://paperswithcode.com/paper/videocrafter1-open-diffusion-models-for-high</link>
      <description><![CDATA[The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videocrafter1-open-diffusion-models-for-high</guid>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</link>
      <description><![CDATA[We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</guid>
    </item>
    <item>
      <title>Skywork: A More Open Bilingual Foundation Model</title>
      <link>https://paperswithcode.com/paper/skywork-a-more-open-bilingual-foundation</link>
      <description><![CDATA[In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3. 2 trillion tokens drawn from both English and Chinese texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/skywork-a-more-open-bilingual-foundation</guid>
    </item>
    <item>
      <title>Evaluating Large Language Models: A Comprehensive Survey</title>
      <link>https://paperswithcode.com/paper/evaluating-large-language-models-a</link>
      <description><![CDATA[We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-large-language-models-a</guid>
    </item>
    <item>
      <title>A Survey of Large Language Models for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/a-survey-of-large-language-models-for-1</link>
      <description><![CDATA[Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-large-language-models-for-1</guid>
    </item>
    <item>
      <title>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks</title>
      <link>https://paperswithcode.com/paper/battle-of-the-backbones-a-large-scale</link>
      <description><![CDATA[Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/battle-of-the-backbones-a-large-scale</guid>
    </item>
    <item>
      <title>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</title>
      <link>https://paperswithcode.com/paper/openchat-advancing-open-source-language</link>
      <description><![CDATA[Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openchat-advancing-open-source-language</guid>
    </item>
    <item>
      <title>GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond</title>
      <link>https://paperswithcode.com/paper/gpt-fathom-benchmarking-large-language-models</link>
      <description><![CDATA[With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-fathom-benchmarking-large-language-models</guid>
    </item>
    <item>
      <title>OpenAgents: An Open Platform for Language Agents in the Wild</title>
      <link>https://paperswithcode.com/paper/openagents-an-open-platform-for-language</link>
      <description><![CDATA[Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openagents-an-open-platform-for-language</guid>
    </item>
    <item>
      <title>Efficient LLM Inference on CPUs</title>
      <link>https://paperswithcode.com/paper/efficient-llm-inference-on-cpus</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-llm-inference-on-cpus</guid>
    </item>
    <item>
      <title>ManimML: Communicating Machine Learning Architectures with Animation</title>
      <link>https://paperswithcode.com/paper/manimml-communicating-machine-learning</link>
      <description><![CDATA[A user can take a preexisting neural network architecture and easily write a specification for an animation in ManimML, which will then automatically compose animations for different components of the system into a final animation of the entire neural network.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/manimml-communicating-machine-learning</guid>
    </item>
    <item>
      <title>VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</title>
      <link>https://paperswithcode.com/paper/videoretalking-audio-based-lip</link>
      <description><![CDATA[Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videoretalking-audio-based-lip</guid>
    </item>
    <item>
      <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1</link>
      <description><![CDATA[While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1</guid>
    </item>
    <item>
      <title>Diffusion Models for Reinforcement Learning: A Survey</title>
      <link>https://paperswithcode.com/paper/diffusion-models-for-reinforcement-learning-a</link>
      <description><![CDATA[Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-for-reinforcement-learning-a</guid>
    </item>
    <item>
      <title>Learning From Mistakes Makes LLM Better Reasoner</title>
      <link>https://paperswithcode.com/paper/learning-from-mistakes-makes-llm-better</link>
      <description><![CDATA[Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-from-mistakes-makes-llm-better</guid>
    </item>
    <item>
      <title>FP8-LM: Training FP8 Large Language Models</title>
      <link>https://paperswithcode.com/paper/fp8-lm-training-fp8-large-language-models</link>
      <description><![CDATA[In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fp8-lm-training-fp8-large-language-models</guid>
    </item>
    <item>
      <title>Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</title>
      <link>https://paperswithcode.com/paper/latent-consistency-models-synthesizing-high</link>
      <description><![CDATA[Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/latent-consistency-models-synthesizing-high</guid>
    </item>
  </channel>
</rss>
