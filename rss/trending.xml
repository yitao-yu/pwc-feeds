<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 16 Mar 2023 21:06:39 +0000</lastBuildDate>
    <item>
      <title>GPT-4 Technical Report</title>
      <link>https://paperswithcode.com/paper/gpt-4-technical-report-1</link>
      <description><![CDATA[We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-4-technical-report-1</guid>
    </item>
    <item>
      <title>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</title>
      <link>https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</link>
      <description><![CDATA[To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</guid>
    </item>
    <item>
      <title>One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</title>
      <link>https://paperswithcode.com/paper/one-transformer-fits-all-distributions-in</link>
      <description><![CDATA[Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -- perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-transformer-fits-all-distributions-in</guid>
    </item>
    <item>
      <title>FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization</title>
      <link>https://paperswithcode.com/paper/freenerf-improving-few-shot-neural-rendering</link>
      <description><![CDATA[One is to regularize the frequency range of NeRF's inputs, while the other is to penalize the near-camera density fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/freenerf-improving-few-shot-neural-rendering</guid>
    </item>
    <item>
      <title>Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws</title>
      <link>https://paperswithcode.com/paper/deep-symbolic-regression-for-physics-guided</link>
      <description><![CDATA[Here we present $\Phi$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-symbolic-regression-for-physics-guided</guid>
    </item>
    <item>
      <title>Universal Instance Perception as Object Discovery and Retrieval</title>
      <link>https://paperswithcode.com/paper/universal-instance-perception-as-object</link>
      <description><![CDATA[All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/universal-instance-perception-as-object</guid>
    </item>
    <item>
      <title>Self-Instruct: Aligning Language Model with Self Generated Instructions</title>
      <link>https://paperswithcode.com/paper/self-instruct-aligning-language-model-with</link>
      <description><![CDATA[Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT_001, which is trained with private user data and human annotations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-instruct-aligning-language-model-with</guid>
    </item>
    <item>
      <title>Erasing Concepts from Diffusion Models</title>
      <link>https://paperswithcode.com/paper/erasing-concepts-from-diffusion-models</link>
      <description><![CDATA[We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/erasing-concepts-from-diffusion-models</guid>
    </item>
    <item>
      <title>Eliciting Latent Predictions from Transformers with the Tuned Lens</title>
      <link>https://paperswithcode.com/paper/eliciting-latent-predictions-from</link>
      <description><![CDATA[We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eliciting-latent-predictions-from</guid>
    </item>
    <item>
      <title>Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis</title>
      <link>https://paperswithcode.com/paper/parameter-is-not-all-you-need-starting-from</link>
      <description><![CDATA[We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/parameter-is-not-all-you-need-starting-from</guid>
    </item>
    <item>
      <title>StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces</title>
      <link>https://paperswithcode.com/paper/styleganex-stylegan-based-manipulation-beyond</link>
      <description><![CDATA[Recent advances in face manipulation using StyleGAN have produced impressive results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/styleganex-stylegan-based-manipulation-beyond</guid>
    </item>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</link>
      <description><![CDATA[We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</guid>
    </item>
    <item>
      <title>A Simple Framework for Open-Vocabulary Segmentation and Detection</title>
      <link>https://paperswithcode.com/paper/a-simple-framework-for-open-vocabulary</link>
      <description><![CDATA[We present \ourmodel{}, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-simple-framework-for-open-vocabulary</guid>
    </item>
    <item>
      <title>Prismer: A Vision-Language Model with An Ensemble of Experts</title>
      <link>https://paperswithcode.com/paper/prismer-a-vision-language-model-with-an</link>
      <description><![CDATA[Recent vision-language models have shown impressive multi-modal generation capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prismer-a-vision-language-model-with-an</guid>
    </item>
    <item>
      <title>ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions</title>
      <link>https://paperswithcode.com/paper/chatgpt-asks-blip-2-answers-automatic</link>
      <description><![CDATA[By keeping acquiring new visual information from BLIP-2's answers, ChatCaptioner is able to generate more enriched image descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatgpt-asks-blip-2-answers-automatic</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Efficient Teacher: Semi-Supervised Object Detection for YOLOv5</title>
      <link>https://paperswithcode.com/paper/efficient-teacher-semi-supervised-object</link>
      <description><![CDATA[The Pseudo Label Assigner prevents the occurrence of bias caused by a large number of low-quality pseudo labels that may interfere with the Dense Detector during the student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes domain and distribution adaptation to allow Dense Detector to learn globally distributed consistent features, making the training independent of the proportion of labeled data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-teacher-semi-supervised-object</guid>
    </item>
    <item>
      <title>Zero-Shot Information Extraction via Chatting with ChatGPT</title>
      <link>https://paperswithcode.com/paper/zero-shot-information-extraction-via-chatting</link>
      <description><![CDATA[Zero-shot information extraction (IE) aims to build IE systems from the unannotated text.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-information-extraction-via-chatting</guid>
    </item>
    <item>
      <title>Editing Implicit Assumptions in Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/editing-implicit-assumptions-in-text-to-image</link>
      <description><![CDATA[Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a "source" under-specified prompt for which the model makes an implicit assumption (e. g., "a pack of roses"), and a "destination" prompt that describes the same setting, but with a specified desired attribute (e. g., "a pack of blue roses").]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/editing-implicit-assumptions-in-text-to-image</guid>
    </item>
    <item>
      <title>BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</title>
      <link>https://paperswithcode.com/paper/bevformer-v2-adapting-modern-image-backbones</link>
      <description><![CDATA[The proposed method is verified with a wide spectrum of traditional and modern image backbones and achieves new SoTA results on the large-scale nuScenes dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bevformer-v2-adapting-modern-image-backbones</guid>
    </item>
  </channel>
</rss>
