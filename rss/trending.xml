<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 19 Nov 2023 09:11:02 +0000</lastBuildDate>
    <item>
      <title>GraphCast: Learning skillful medium-range global weather forecasting</title>
      <link>https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</link>
      <description><![CDATA[Global medium-range weather forecasting is critical to decision-making across many social and economic domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</guid>
    </item>
    <item>
      <title>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</title>
      <link>https://paperswithcode.com/paper/lcm-lora-a-universal-stable-diffusion</link>
      <description><![CDATA[Latent Consistency Models (LCMs) have achieved impressive performance in accelerating text-to-image generative tasks, producing high-quality images with minimal inference steps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lcm-lora-a-universal-stable-diffusion</guid>
    </item>
    <item>
      <title>Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models</title>
      <link>https://paperswithcode.com/paper/qwen-audio-advancing-universal-audio</link>
      <description><![CDATA[Recently, instruction-following audio-language models have received broad attention for audio interaction with humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-audio-advancing-universal-audio</guid>
    </item>
    <item>
      <title>Zephyr: Direct Distillation of LM Alignment</title>
      <link>https://paperswithcode.com/paper/zephyr-direct-distillation-of-lm-alignment</link>
      <description><![CDATA[Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zephyr-direct-distillation-of-lm-alignment</guid>
    </item>
    <item>
      <title>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</title>
      <link>https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora</link>
      <description><![CDATA[To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora</guid>
    </item>
    <item>
      <title>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</title>
      <link>https://paperswithcode.com/paper/monkey-image-resolution-and-text-label-are</link>
      <description><![CDATA[Large Multimodal Models have demonstrated impressive capabilities in understanding general vision-language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monkey-image-resolution-and-text-label-are</guid>
    </item>
    <item>
      <title>JaxMARL: Multi-Agent RL Environments in JAX</title>
      <link>https://paperswithcode.com/paper/jaxmarl-multi-agent-rl-environments-in-jax</link>
      <description><![CDATA[This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/jaxmarl-multi-agent-rl-environments-in-jax</guid>
    </item>
    <item>
      <title>Mustango: Toward Controllable Text-to-Music Generation</title>
      <link>https://paperswithcode.com/paper/mustango-toward-controllable-text-to-music</link>
      <description><![CDATA[With recent advancements in text-to-audio and text-to-music based on latent diffusion models, the quality of generated content has been reaching new heights.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mustango-toward-controllable-text-to-music</guid>
    </item>
    <item>
      <title>A Survey on Language Models for Code</title>
      <link>https://paperswithcode.com/paper/a-survey-on-language-models-for-code</link>
      <description><![CDATA[In this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, and 500 related works.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-language-models-for-code</guid>
    </item>
    <item>
      <title>Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs</title>
      <link>https://paperswithcode.com/paper/lumos-learning-agents-with-unified-data</link>
      <description><![CDATA[Leveraging this unified data and modular design, Lumos not only achieves comparable or superior performance to current, state-of-the-art agents, but also exhibits several key advantages: (1) Lumos surpasses GPT-4/3. 5-based agents in complex question answering and web tasks, while equalling the performance of significantly larger LLM agents on math tasks; (2) Lumos outperforms open-source agents created through conventional training methods and those using chain-of-thoughts training; and (3) Lumos is capable of effectively generalizing to unseen interactive tasks, outperforming larger LLM-based agents and even exceeding performance of specialized agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lumos-learning-agents-with-unified-data</guid>
    </item>
    <item>
      <title>Learning to (Learn at Test Time)</title>
      <link>https://paperswithcode.com/paper/learning-to-learn-at-test-time</link>
      <description><![CDATA[Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-learn-at-test-time</guid>
    </item>
    <item>
      <title>The Rise and Potential of Large Language Model Based Agents: A Survey</title>
      <link>https://paperswithcode.com/paper/the-rise-and-potential-of-large-language</link>
      <description><![CDATA[Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-rise-and-potential-of-large-language</guid>
    </item>
    <item>
      <title>Learning to Filter Context for Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/learning-to-filter-context-for-retrieval</link>
      <description><![CDATA[To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-filter-context-for-retrieval</guid>
    </item>
    <item>
      <title>NEFTune: Noisy Embeddings Improve Instruction Finetuning</title>
      <link>https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</link>
      <description><![CDATA[We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</guid>
    </item>
    <item>
      <title>Plum: Prompt Learning using Metaheuristic</title>
      <link>https://paperswithcode.com/paper/plum-prompt-learning-using-metaheuristic</link>
      <description><![CDATA[Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/plum-prompt-learning-using-metaheuristic</guid>
    </item>
    <item>
      <title>QLoRA: Efficient Finetuning of Quantized LLMs</title>
      <link>https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</link>
      <description><![CDATA[Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99. 3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</guid>
    </item>
    <item>
      <title>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/to-see-is-to-believe-prompting-gpt-4v-for</link>
      <description><![CDATA[Existing visual instruction tuning methods typically prompt large language models with textual descriptions to generate instruction-following data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/to-see-is-to-believe-prompting-gpt-4v-for</guid>
    </item>
    <item>
      <title>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</title>
      <link>https://paperswithcode.com/paper/rethinking-benchmark-and-contamination-for</link>
      <description><![CDATA[Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-benchmark-and-contamination-for</guid>
    </item>
    <item>
      <title>VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</title>
      <link>https://paperswithcode.com/paper/videoretalking-audio-based-lip</link>
      <description><![CDATA[Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videoretalking-audio-based-lip</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
  </channel>
</rss>
