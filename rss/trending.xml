<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 06 Mar 2024 21:07:15 +0000</lastBuildDate>
    <item>
      <title>DUSt3R: Geometric 3D Vision Made Easy</title>
      <link>https://paperswithcode.com/paper/dust3r-geometric-3d-vision-made-easy</link>
      <description><![CDATA[Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dust3r-geometric-3d-vision-made-easy</guid>
    </item>
    <item>
      <title>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</title>
      <link>https://paperswithcode.com/paper/mobileclip-fast-image-text-models-through</link>
      <description><![CDATA[We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2. 9% average performance improvement on 38 evaluation benchmarks compared to the previous best.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobileclip-fast-image-text-models-through</guid>
    </item>
    <item>
      <title>Transparent Image Layer Diffusion using Latent Transparency</title>
      <link>https://paperswithcode.com/paper/transparent-image-layer-diffusion-using</link>
      <description><![CDATA[We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transparent-image-layer-diffusion-using</guid>
    </item>
    <item>
      <title>Datasets for Large Language Models: A Comprehensive Survey</title>
      <link>https://paperswithcode.com/paper/datasets-for-large-language-models-a</link>
      <description><![CDATA[Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datasets-for-large-language-models-a</guid>
    </item>
    <item>
      <title>Simple linear attention language models balance the recall-throughput tradeoff</title>
      <link>https://paperswithcode.com/paper/simple-linear-attention-language-models</link>
      <description><![CDATA[In this work, we explore whether we can improve language model efficiency (e. g. by reducing memory consumption) without compromising on recall.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-linear-attention-language-models</guid>
    </item>
    <item>
      <title>VisionLLaMA: A Unified LLaMA Interface for Vision Tasks</title>
      <link>https://paperswithcode.com/paper/visionllama-a-unified-llama-interface-for</link>
      <description><![CDATA[Large language models are built on top of a transformer-based architecture to process textual inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visionllama-a-unified-llama-interface-for</guid>
    </item>
    <item>
      <title>OLMo: Accelerating the Science of Language Models</title>
      <link>https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</link>
      <description><![CDATA[Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</guid>
    </item>
    <item>
      <title>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</title>
      <link>https://paperswithcode.com/paper/learning-to-generate-instruction-tuning</link>
      <description><![CDATA[Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-generate-instruction-tuning</guid>
    </item>
    <item>
      <title>UFO: A UI-Focused Agent for Windows OS Interaction</title>
      <link>https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</link>
      <description><![CDATA[We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</guid>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for AI-Generated Content: A Survey</title>
      <link>https://paperswithcode.com/paper/retrieval-augmented-generation-for-ai</link>
      <description><![CDATA[Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-augmented-generation-for-ai</guid>
    </item>
    <item>
      <title>TryOnDiffusion: A Tale of Two UNets</title>
      <link>https://paperswithcode.com/paper/tryondiffusion-a-tale-of-two-unets-1</link>
      <description><![CDATA[Given two images depicting a person and a garment worn by another person, our goal is to generate a visualization of how the garment might look on the input person.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tryondiffusion-a-tale-of-two-unets-1</guid>
    </item>
    <item>
      <title>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</title>
      <link>https://paperswithcode.com/paper/discrete-diffusion-language-modeling-by</link>
      <description><![CDATA[Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/discrete-diffusion-language-modeling-by</guid>
    </item>
    <item>
      <title>OMG-Seg: Is One Model Good Enough For All Segmentation?</title>
      <link>https://paperswithcode.com/paper/omg-seg-is-one-model-good-enough-for-all</link>
      <description><![CDATA[In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omg-seg-is-one-model-good-enough-for-all</guid>
    </item>
    <item>
      <title>Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases</title>
      <link>https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing</link>
      <description><![CDATA[Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing</guid>
    </item>
    <item>
      <title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title>
      <link>https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</link>
      <description><![CDATA[The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</guid>
    </item>
    <item>
      <title>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</title>
      <link>https://paperswithcode.com/paper/sora-a-review-on-background-technology</link>
      <description><![CDATA[Sora is a text-to-video generative AI model, released by OpenAI in February 2024.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sora-a-review-on-background-technology</guid>
    </item>
    <item>
      <title>Rethinking Inductive Biases for Surface Normal Estimation</title>
      <link>https://paperswithcode.com/paper/rethinking-inductive-biases-for-surface</link>
      <description><![CDATA[Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-inductive-biases-for-surface</guid>
    </item>
    <item>
      <title>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</title>
      <link>https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using</link>
      <description><![CDATA[It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using</guid>
    </item>
    <item>
      <title>Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures</title>
      <link>https://paperswithcode.com/paper/vision-rwkv-efficient-and-scalable-visual</link>
      <description><![CDATA[Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-rwkv-efficient-and-scalable-visual</guid>
    </item>
    <item>
      <title>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</title>
      <link>https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight</link>
      <description><![CDATA["Bigger the better" has been the predominant trend in recent Large Language Models (LLMs) development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight</guid>
    </item>
  </channel>
</rss>
