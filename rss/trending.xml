<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 11 Aug 2023 21:05:20 +0000</lastBuildDate>
    <item>
      <title>Generative Agents: Interactive Simulacra of Human Behavior</title>
      <link>https://paperswithcode.com/paper/generative-agents-interactive-simulacra-of</link>
      <description><![CDATA[Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-agents-interactive-simulacra-of</guid>
    </item>
    <item>
      <title>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</title>
      <link>https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent</link>
      <description><![CDATA[Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent</guid>
    </item>
    <item>
      <title>AgentBench: Evaluating LLMs as Agents</title>
      <link>https://paperswithcode.com/paper/agentbench-evaluating-llms-as-agents</link>
      <description><![CDATA[Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentbench-evaluating-llms-as-agents</guid>
    </item>
    <item>
      <title>Separate Anything You Describe</title>
      <link>https://paperswithcode.com/paper/separate-anything-you-describe</link>
      <description><![CDATA[In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/separate-anything-you-describe</guid>
    </item>
    <item>
      <title>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</title>
      <link>https://paperswithcode.com/paper/toolllm-facilitating-large-language-models-to</link>
      <description><![CDATA[We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/toolllm-facilitating-large-language-models-to</guid>
    </item>
    <item>
      <title>LISA: Reasoning Segmentation via Large Language Model</title>
      <link>https://paperswithcode.com/paper/lisa-reasoning-segmentation-via-large</link>
      <description><![CDATA[In this work, we propose a new segmentation task -- reasoning segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lisa-reasoning-segmentation-via-large</guid>
    </item>
    <item>
      <title>Effective Whole-body Pose Estimation with Two-stages Distillation</title>
      <link>https://paperswithcode.com/paper/effective-whole-body-pose-estimation-with-two</link>
      <description><![CDATA[Different from the previous self-knowledge distillation, this stage finetunes the student's head with only 20% training time as a plug-and-play training strategy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/effective-whole-body-pose-estimation-with-two</guid>
    </item>
    <item>
      <title>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</title>
      <link>https://paperswithcode.com/paper/pug-photorealistic-and-semantically</link>
      <description><![CDATA[Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pug-photorealistic-and-semantically</guid>
    </item>
    <item>
      <title>SynJax: Structured Probability Distributions for JAX</title>
      <link>https://paperswithcode.com/paper/synjax-structured-probability-distributions</link>
      <description><![CDATA[The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/synjax-structured-probability-distributions</guid>
    </item>
    <item>
      <title>Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors</title>
      <link>https://paperswithcode.com/paper/magic123-one-image-to-high-quality-3d-object</link>
      <description><![CDATA[We present Magic123, a two-stage coarse-to-fine approach for high-quality, textured 3D meshes generation from a single unposed image in the wild using both2D and 3D priors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magic123-one-image-to-high-quality-3d-object</guid>
    </item>
    <item>
      <title>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</title>
      <link>https://paperswithcode.com/paper/lighting-every-darkness-in-two-pairs-a</link>
      <description><![CDATA[Calibration-based methods have dominated RAW image denoising under extremely low-light environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lighting-every-darkness-in-two-pairs-a</guid>
    </item>
    <item>
      <title>Simple synthetic data reduces sycophancy in large language models</title>
      <link>https://paperswithcode.com/paper/simple-synthetic-data-reduces-sycophancy-in</link>
      <description><![CDATA[Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-synthetic-data-reduces-sycophancy-in</guid>
    </item>
    <item>
      <title>Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions</title>
      <link>https://paperswithcode.com/paper/empowering-vision-language-models-to-follow</link>
      <description><![CDATA[To address this issue, we propose a generic and lightweight controllable knowledge re-injection module, which utilizes the sophisticated reasoning ability of LLMs to control the VPG to conditionally extract instruction-specific visual information and re-inject it into the LLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/empowering-vision-language-models-to-follow</guid>
    </item>
    <item>
      <title>QLoRA: Efficient Finetuning of Quantized LLMs</title>
      <link>https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</link>
      <description><![CDATA[Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99. 3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</guid>
    </item>
    <item>
      <title>Dual Aggregation Transformer for Image Super-Resolution</title>
      <link>https://paperswithcode.com/paper/dual-aggregation-transformer-for-image-super</link>
      <description><![CDATA[Based on the above idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT), for image SR. Our DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dual-aggregation-transformer-for-image-super</guid>
    </item>
    <item>
      <title>Fine-Tuning Language Models from Human Preferences</title>
      <link>https://paperswithcode.com/paper/fine-tuning-language-models-from-human</link>
      <description><![CDATA[Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-tuning-language-models-from-human</guid>
    </item>
    <item>
      <title>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP</title>
      <link>https://paperswithcode.com/paper/convolutions-die-hard-open-vocabulary</link>
      <description><![CDATA[The proposed FC-CLIP, benefits from the following observations: the frozen CLIP backbone maintains the ability of open-vocabulary classification and can also serve as a strong mask generator, and the convolutional CLIP generalizes well to a larger input resolution than the one used during contrastive image-text pretraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/convolutions-die-hard-open-vocabulary</guid>
    </item>
    <item>
      <title>UniVTG: Towards Unified Video-Language Temporal Grounding</title>
      <link>https://paperswithcode.com/paper/univtg-towards-unified-video-language</link>
      <description><![CDATA[Most methods in this direction develop taskspecific models that are trained with type-specific labels, such as moment retrieval (time interval) and highlight detection (worthiness curve), which limits their abilities to generalize to various VTG tasks and labels.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/univtg-towards-unified-video-language</guid>
    </item>
    <item>
      <title>Universal and Transferable Adversarial Attacks on Aligned Language Models</title>
      <link>https://paperswithcode.com/paper/universal-and-transferable-adversarial</link>
      <description><![CDATA[Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/universal-and-transferable-adversarial</guid>
    </item>
    <item>
      <title>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</title>
      <link>https://paperswithcode.com/paper/automatically-correcting-large-language</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automatically-correcting-large-language</guid>
    </item>
  </channel>
</rss>
