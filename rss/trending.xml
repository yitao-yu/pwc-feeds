<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 22 Oct 2023 09:10:30 +0000</lastBuildDate>
    <item>
      <title>OpenAgents: An Open Platform for Language Agents in the Wild</title>
      <link>https://paperswithcode.com/paper/openagents-an-open-platform-for-language</link>
      <description><![CDATA[Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openagents-an-open-platform-for-language</guid>
    </item>
    <item>
      <title>PixArt-$Î±$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</title>
      <link>https://paperswithcode.com/paper/pixart-a-fast-training-of-diffusion</link>
      <description><![CDATA[We hope PIXART-$\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixart-a-fast-training-of-diffusion</guid>
    </item>
    <item>
      <title>Putting the Object Back into Video Object Segmentation</title>
      <link>https://paperswithcode.com/paper/putting-the-object-back-into-video-object</link>
      <description><![CDATA[The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/putting-the-object-back-into-video-object</guid>
    </item>
    <item>
      <title>Llemma: An Open Language Model For Mathematics</title>
      <link>https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</link>
      <description><![CDATA[We present Llemma, a large language model for mathematics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</guid>
    </item>
    <item>
      <title>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</title>
      <link>https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</link>
      <description><![CDATA[AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</guid>
    </item>
    <item>
      <title>GRID: A Platform for General Robot Intelligence Development</title>
      <link>https://paperswithcode.com/paper/grid-a-platform-for-general-robot</link>
      <description><![CDATA[In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grid-a-platform-for-general-robot</guid>
    </item>
    <item>
      <title>Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</title>
      <link>https://paperswithcode.com/paper/latent-consistency-models-synthesizing-high</link>
      <description><![CDATA[Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/latent-consistency-models-synthesizing-high</guid>
    </item>
    <item>
      <title>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/from-clip-to-dino-visual-encoders-shout-in</link>
      <description><![CDATA[By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-clip-to-dino-visual-encoders-shout-in</guid>
    </item>
    <item>
      <title>BitNet: Scaling 1-bit Transformers for Large Language Models</title>
      <link>https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</link>
      <description><![CDATA[The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</guid>
    </item>
    <item>
      <title>VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</title>
      <link>https://paperswithcode.com/paper/videoretalking-audio-based-lip</link>
      <description><![CDATA[Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videoretalking-audio-based-lip</guid>
    </item>
    <item>
      <title>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</link>
      <description><![CDATA[In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</guid>
    </item>
    <item>
      <title>Character-LLM: A Trainable Agent for Role-Playing</title>
      <link>https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</link>
      <description><![CDATA[Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</guid>
    </item>
    <item>
      <title>Separate Anything You Describe</title>
      <link>https://paperswithcode.com/paper/separate-anything-you-describe</link>
      <description><![CDATA[In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/separate-anything-you-describe</guid>
    </item>
    <item>
      <title>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/itransformer-inverted-transformers-are</link>
      <description><![CDATA[These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/itransformer-inverted-transformers-are</guid>
    </item>
    <item>
      <title>AutoMix: Automatically Mixing Language Models</title>
      <link>https://paperswithcode.com/paper/automix-automatically-mixing-language-models</link>
      <description><![CDATA[Large language models (LLMs) are now available in various sizes and configurations from cloud API providers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automix-automatically-mixing-language-models</guid>
    </item>
    <item>
      <title>MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion</title>
      <link>https://paperswithcode.com/paper/meflut-unsupervised-1d-lookup-tables-for</link>
      <description><![CDATA[In this paper, we introduce a new approach for high-quality multi-exposure image fusion (MEF).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meflut-unsupervised-1d-lookup-tables-for</guid>
    </item>
    <item>
      <title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title>
      <link>https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</link>
      <description><![CDATA[In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</guid>
    </item>
    <item>
      <title>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</link>
      <description><![CDATA[Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</guid>
    </item>
    <item>
      <title>LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation</title>
      <link>https://paperswithcode.com/paper/lamp-learn-a-motion-pattern-for-few-shot</link>
      <description><![CDATA[Specifically, we design a first-frame-conditioned pipeline that uses an off-the-shelf text-to-image model for content generation so that our tuned video diffusion model mainly focuses on motion learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lamp-learn-a-motion-pattern-for-few-shot</guid>
    </item>
    <item>
      <title>A Survey on Video Diffusion Models</title>
      <link>https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</link>
      <description><![CDATA[However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</guid>
    </item>
  </channel>
</rss>
