<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 15 Apr 2024 09:11:46 +0000</lastBuildDate>
    <item>
      <title>AutoCodeRover: Autonomous Program Improvement</title>
      <link>https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</link>
      <description><![CDATA[Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</title>
      <link>https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</link>
      <description><![CDATA[Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</guid>
    </item>
    <item>
      <title>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</title>
      <link>https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</link>
      <description><![CDATA[We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</guid>
    </item>
    <item>
      <title>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</title>
      <link>https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</link>
      <description><![CDATA[The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</guid>
    </item>
    <item>
      <title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
      <link>https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</link>
      <description><![CDATA[To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</guid>
    </item>
    <item>
      <title>Rho-1: Not All Tokens Are What You Need</title>
      <link>https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</link>
      <description><![CDATA[After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40. 6% and 51. 8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</guid>
    </item>
    <item>
      <title>InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</link>
      <description><![CDATA[Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
      <link>https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</link>
      <description><![CDATA[We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</guid>
    </item>
    <item>
      <title>From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples</title>
      <link>https://paperswithcode.com/paper/from-words-to-numbers-your-large-language</link>
      <description><![CDATA[We analyze how well pre-trained large language models (e. g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-words-to-numbers-your-large-language</guid>
    </item>
    <item>
      <title>Vision-Language Pre-Training for Boosting Scene Text Detectors</title>
      <link>https://paperswithcode.com/paper/vision-language-pre-training-for-boosting</link>
      <description><![CDATA[In this paper, we specifically adapt vision-language joint learning for scene text detection, a task that intrinsically involves cross-modal interaction between the two modalities: vision and language, since text is the written form of language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-language-pre-training-for-boosting</guid>
    </item>
    <item>
      <title>Levenshtein OCR</title>
      <link>https://paperswithcode.com/paper/levenshtein-ocr</link>
      <description><![CDATA[A novel scene text recognizer based on Vision-Language Transformer (VLT) is presented.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/levenshtein-ocr</guid>
    </item>
    <item>
      <title>LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding</title>
      <link>https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</link>
      <description><![CDATA[The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</guid>
    </item>
    <item>
      <title>OmniFusion Technical Report</title>
      <link>https://paperswithcode.com/paper/omnifusion-technical-report</link>
      <description><![CDATA[We propose an \textit{OmniFusion} model based on a pretrained LLM and adapters for visual modality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnifusion-technical-report</guid>
    </item>
    <item>
      <title>ReFT: Representation Finetuning for Language Models</title>
      <link>https://paperswithcode.com/paper/reft-representation-finetuning-for-language</link>
      <description><![CDATA[LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reft-representation-finetuning-for-language</guid>
    </item>
    <item>
      <title>Video-Based Human Pose Regression via Decoupled Space-Time Aggregation</title>
      <link>https://paperswithcode.com/paper/video-based-human-pose-regression-via</link>
      <description><![CDATA[In light of this, we propose a novel Decoupled Space-Time Aggregation network (DSTA) to separately capture the spatial contexts between adjacent joints and the temporal cues of each individual joint, thereby avoiding the conflation of spatiotemporal dimensions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-based-human-pose-regression-via</guid>
    </item>
    <item>
      <title>Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</title>
      <link>https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</link>
      <description><![CDATA[To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-s-think-outside-the-box-exploring-leap-of</guid>
    </item>
    <item>
      <title>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</title>
      <link>https://paperswithcode.com/paper/gomvs-geometrically-consistent-cost</link>
      <description><![CDATA[More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gomvs-geometrically-consistent-cost</guid>
    </item>
    <item>
      <title>Hash3D: Training-free Acceleration for 3D Generation</title>
      <link>https://paperswithcode.com/paper/hash3d-training-free-acceleration-for-3d</link>
      <description><![CDATA[The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hash3d-training-free-acceleration-for-3d</guid>
    </item>
  </channel>
</rss>
