<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 17 Apr 2024 09:13:14 +0000</lastBuildDate>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators</title>
      <link>https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</link>
      <description><![CDATA[Recent advances in Text-to-Video generation (T2V) have achieved remarkable success in synthesizing high-quality general videos from textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magictime-time-lapse-video-generation-models</guid>
    </item>
    <item>
      <title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
      <link>https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</link>
      <description><![CDATA[To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</guid>
    </item>
    <item>
      <title>AutoCodeRover: Autonomous Program Improvement</title>
      <link>https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</link>
      <description><![CDATA[Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autocoderover-autonomous-program-improvement</guid>
    </item>
    <item>
      <title>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</title>
      <link>https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</link>
      <description><![CDATA[We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</title>
      <link>https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</link>
      <description><![CDATA[We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i. e., high-resolution visual tokens, high-quality data, and VLM-guided generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-gemini-mining-the-potential-of-multi</guid>
    </item>
    <item>
      <title>Rho-1: Not All Tokens Are What You Need</title>
      <link>https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</link>
      <description><![CDATA[After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40. 6% and 51. 8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rho-1-not-all-tokens-are-what-you-need</guid>
    </item>
    <item>
      <title>LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding</title>
      <link>https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</link>
      <description><![CDATA[The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layoutllm-layout-instruction-tuning-with</guid>
    </item>
    <item>
      <title>Probing the 3D Awareness of Visual Foundation Models</title>
      <link>https://paperswithcode.com/paper/probing-the-3d-awareness-of-visual-foundation</link>
      <description><![CDATA[Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probing-the-3d-awareness-of-visual-foundation</guid>
    </item>
    <item>
      <title>From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples</title>
      <link>https://paperswithcode.com/paper/from-words-to-numbers-your-large-language</link>
      <description><![CDATA[We analyze how well pre-trained large language models (e. g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-words-to-numbers-your-large-language</guid>
    </item>
    <item>
      <title>TinyLlama: An Open-Source Small Language Model</title>
      <link>https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model</link>
      <description><![CDATA[We present TinyLlama, a compact 1. 1B language model pretrained on around 1 trillion tokens for approximately 3 epochs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model</guid>
    </item>
    <item>
      <title>Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</title>
      <link>https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</link>
      <description><![CDATA[The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/patch-n-pack-navit-a-vision-transformer-for</guid>
    </item>
    <item>
      <title>SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System</title>
      <link>https://paperswithcode.com/paper/schurvins-schur-complement-based-lightweight</link>
      <description><![CDATA[To this end, we propose a novel filter-based VINS framework named SchurVINS, which could guarantee both high accuracy by building a complete residual model and low computational complexity with Schur complement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/schurvins-schur-complement-based-lightweight</guid>
    </item>
    <item>
      <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
      <link>https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</link>
      <description><![CDATA[We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm2vec-large-language-models-are-secretly</guid>
    </item>
    <item>
      <title>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</title>
      <link>https://paperswithcode.com/paper/champ-controllable-and-consistent-human-image</link>
      <description><![CDATA[In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/champ-controllable-and-consistent-human-image</guid>
    </item>
    <item>
      <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
      <link>https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</link>
      <description><![CDATA[To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</guid>
    </item>
    <item>
      <title>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</title>
      <link>https://paperswithcode.com/paper/gomvs-geometrically-consistent-cost</link>
      <description><![CDATA[More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gomvs-geometrically-consistent-cost</guid>
    </item>
    <item>
      <title>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</title>
      <link>https://paperswithcode.com/paper/griffin-mixing-gated-linear-recurrences-with</link>
      <description><![CDATA[Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/griffin-mixing-gated-linear-recurrences-with</guid>
    </item>
    <item>
      <title>InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</link>
      <description><![CDATA[Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantstyle-free-lunch-towards-style</guid>
    </item>
  </channel>
</rss>
