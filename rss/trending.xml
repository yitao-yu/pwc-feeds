<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 23 Jul 2025 09:26:59 +0000</lastBuildDate>
    <item>
      <title>WebSailor: Navigating Super-human Reasoning for Web Agent</title>
      <link>https://paperswithcode.com/paper/websailor-navigating-super-human-reasoning</link>
      <description><![CDATA[Transcending human cognitive limitations represents a critical frontier in LLM training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/websailor-navigating-super-human-reasoning</guid>
    </item>
    <item>
      <title>Streaming 4D Visual Geometry Transformer</title>
      <link>https://paperswithcode.com/paper/streaming-4d-visual-geometry-transformer</link>
      <description><![CDATA[To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streaming-4d-visual-geometry-transformer</guid>
    </item>
    <item>
      <title>IQ-Learn: Inverse soft-Q Learning for Imitation</title>
      <link>https://paperswithcode.com/paper/iq-learn-inverse-soft-q-learning-for</link>
      <description><![CDATA[In many sequential decision-making problems (e. g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/iq-learn-inverse-soft-q-learning-for</guid>
    </item>
    <item>
      <title>ComfyUI-R1: Exploring Reasoning Models for Workflow Generation</title>
      <link>https://paperswithcode.com/paper/comfyui-r1-exploring-reasoning-models-for</link>
      <description><![CDATA[Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/comfyui-r1-exploring-reasoning-models-for</guid>
    </item>
    <item>
      <title>UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens</title>
      <link>https://paperswithcode.com/paper/unictokens-boosting-personalized</link>
      <description><![CDATA[To address the limitation, we present UniCTokens, a novel framework that effectively integrates personalized information into a unified vision language model (VLM) for understanding and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unictokens-boosting-personalized</guid>
    </item>
    <item>
      <title>SpatialTrackerV2: 3D Point Tracking Made Easy</title>
      <link>https://paperswithcode.com/paper/spatialtrackerv2-3d-point-tracking-made-easy-1</link>
      <description><![CDATA[We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spatialtrackerv2-3d-point-tracking-made-easy-1</guid>
    </item>
    <item>
      <title>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</title>
      <link>https://paperswithcode.com/paper/mc-llava-multi-concept-personalized-vision-1</link>
      <description><![CDATA[To reduce the costs related to joint training, we propose a personalized textual prompt that uses visual token information to initialize concept tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mc-llava-multi-concept-personalized-vision-1</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>PhysX: Physical-Grounded 3D Asset Generation</title>
      <link>https://paperswithcode.com/paper/physx-physical-grounded-3d-asset-generation</link>
      <description><![CDATA[3D modeling is moving from virtual to physical.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/physx-physical-grounded-3d-asset-generation</guid>
    </item>
    <item>
      <title>$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting</title>
      <link>https://paperswithcode.com/paper/i-2-world-intra-inter-tokenization-for</link>
      <description><![CDATA[Our method decouples scene tokenization into intra-scene and inter-scene tokenizers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/i-2-world-intra-inter-tokenization-for</guid>
    </item>
    <item>
      <title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title>
      <link>https://paperswithcode.com/paper/sepllm-accelerate-large-language-models-by</link>
      <description><![CDATA[This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sepllm-accelerate-large-language-models-by</guid>
    </item>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs</title>
      <link>https://paperswithcode.com/paper/the-devil-behind-the-mask-an-emergent-safety</link>
      <description><![CDATA[Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i. e., bidirectional modeling and parallel decoding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-devil-behind-the-mask-an-emergent-safety</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
    <item>
      <title>Zep: A Temporal Knowledge Graph Architecture for Agent Memory</title>
      <link>https://paperswithcode.com/paper/zep-a-temporal-knowledge-graph-architecture</link>
      <description><![CDATA[We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zep-a-temporal-knowledge-graph-architecture</guid>
    </item>
    <item>
      <title>DiC: Rethinking Conv3x3 Designs in Diffusion Models</title>
      <link>https://paperswithcode.com/paper/dic-rethinking-conv3x3-designs-in-diffusion</link>
      <description><![CDATA[Diffusion models have shown exceptional performance in visual generation tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dic-rethinking-conv3x3-designs-in-diffusion</guid>
    </item>
    <item>
      <title>LTX-Video: Realtime Video Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</link>
      <description><![CDATA[To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</guid>
    </item>
    <item>
      <title>REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites</title>
      <link>https://paperswithcode.com/paper/real-benchmarking-autonomous-agents-on</link>
      <description><![CDATA[We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-benchmarking-autonomous-agents-on</guid>
    </item>
    <item>
      <title>Do Large Language Models Need a Content Delivery Network?</title>
      <link>https://paperswithcode.com/paper/do-large-language-models-need-a-content</link>
      <description><![CDATA[As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-large-language-models-need-a-content</guid>
    </item>
    <item>
      <title>Energy-Based Transformers are Scalable Learners and Thinkers</title>
      <link>https://paperswithcode.com/paper/energy-based-transformers-are-scalable</link>
      <description><![CDATA[Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/energy-based-transformers-are-scalable</guid>
    </item>
  </channel>
</rss>
