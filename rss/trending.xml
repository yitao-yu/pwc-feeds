<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 18 Jan 2024 09:12:49 +0000</lastBuildDate>
    <item>
      <title>PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</title>
      <link>https://paperswithcode.com/paper/photomaker-customizing-realistic-human-photos</link>
      <description><![CDATA[Recent advances in text-to-image generation have made remarkable progress in synthesizing realistic human photos conditioned on given text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photomaker-customizing-realistic-human-photos</guid>
    </item>
    <item>
      <title>InstantID: Zero-shot Identity-Preserving Generation in Seconds</title>
      <link>https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</link>
      <description><![CDATA[There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception</title>
      <link>https://paperswithcode.com/paper/aesbench-an-expert-benchmark-for-multimodal</link>
      <description><![CDATA[An obvious obstacle lies in the absence of a specific benchmark to evaluate the effectiveness of MLLMs on aesthetic perception.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aesbench-an-expert-benchmark-for-multimodal</guid>
    </item>
    <item>
      <title>Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</title>
      <link>https://paperswithcode.com/paper/efficient-deformable-convnets-rethinking</link>
      <description><![CDATA[The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-deformable-convnets-rethinking</guid>
    </item>
    <item>
      <title>DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders</title>
      <link>https://paperswithcode.com/paper/ddcolor-towards-photo-realistic-and-semantic</link>
      <description><![CDATA[Image colorization is a challenging problem due to multi-modal uncertainty and high ill-posedness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ddcolor-towards-photo-realistic-and-semantic</guid>
    </item>
    <item>
      <title>MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices</title>
      <link>https://paperswithcode.com/paper/mi-gan-a-simple-baseline-for-image-inpainting</link>
      <description><![CDATA[In this paper we present a simple image inpainting baseline, Mobile Inpainting GAN (MI-GAN), which is approximately one order of magnitude computationally cheaper and smaller than existing state-of-the-art inpainting models, and can be efficiently deployed on mobile devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mi-gan-a-simple-baseline-for-image-inpainting</guid>
    </item>
    <item>
      <title>INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/inters-unlocking-the-power-of-large-language</link>
      <description><![CDATA[Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/inters-unlocking-the-power-of-large-language</guid>
    </item>
    <item>
      <title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title>
      <link>https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert</link>
      <description><![CDATA[Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert</guid>
    </item>
    <item>
      <title>Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities</title>
      <link>https://paperswithcode.com/paper/forging-vision-foundation-models-for</link>
      <description><![CDATA[The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/forging-vision-foundation-models-for</guid>
    </item>
    <item>
      <title>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</title>
      <link>https://paperswithcode.com/paper/longllmlingua-accelerating-and-enhancing-llms</link>
      <description><![CDATA[Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longllmlingua-accelerating-and-enhancing-llms</guid>
    </item>
    <item>
      <title>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</title>
      <link>https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</link>
      <description><![CDATA[Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</guid>
    </item>
    <item>
      <title>TrustLLM: Trustworthiness in Large Language Models</title>
      <link>https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language</link>
      <description><![CDATA[This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language</guid>
    </item>
    <item>
      <title>LEGO:Language Enhanced Multi-modal Grounding Model</title>
      <link>https://paperswithcode.com/paper/lego-language-enhanced-multi-modal-grounding</link>
      <description><![CDATA[Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lego-language-enhanced-multi-modal-grounding</guid>
    </item>
    <item>
      <title>Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models</title>
      <link>https://paperswithcode.com/paper/inferflow-an-efficient-and-highly</link>
      <description><![CDATA[We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/inferflow-an-efficient-and-highly</guid>
    </item>
    <item>
      <title>A Survey of Resource-efficient LLM and Multimodal Foundation Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-resource-efficient-llm-and</link>
      <description><![CDATA[Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-resource-efficient-llm-and</guid>
    </item>
    <item>
      <title>Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture</title>
      <link>https://paperswithcode.com/paper/monarch-mixer-a-simple-sub-quadratic-gemm-1</link>
      <description><![CDATA[We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monarch-mixer-a-simple-sub-quadratic-gemm-1</guid>
    </item>
    <item>
      <title>TaskWeaver: A Code-First Agent Framework</title>
      <link>https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</link>
      <description><![CDATA[TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</guid>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <link>https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</link>
      <description><![CDATA[Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</guid>
    </item>
    <item>
      <title>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</title>
      <link>https://paperswithcode.com/paper/motionctrl-a-unified-and-flexible-motion</link>
      <description><![CDATA[Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/motionctrl-a-unified-and-flexible-motion</guid>
    </item>
    <item>
      <title>NEFTune: Noisy Embeddings Improve Instruction Finetuning</title>
      <link>https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</link>
      <description><![CDATA[We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neftune-noisy-embeddings-improve-instruction</guid>
    </item>
  </channel>
</rss>
