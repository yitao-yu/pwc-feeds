<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 22 Jun 2022 21:07:06 +0000</lastBuildDate>
    <item>
      <title>Multiplying Matrices Without Multiplying</title>
      <link>https://paperswithcode.com/paper/multiplying-matrices-without-multiplying</link>
      <description><![CDATA[Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multiplying-matrices-without-multiplying</guid>
    </item>
    <item>
      <title>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</title>
      <link>https://paperswithcode.com/paper/minedojo-building-open-ended-embodied-agents</link>
      <description><![CDATA[Autonomous agents have made great strides in specialist domains like Atari games and Go.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minedojo-building-open-ended-embodied-agents</guid>
    </item>
    <item>
      <title>Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case</title>
      <link>https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</link>
      <description><![CDATA[In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</guid>
    </item>
    <item>
      <title>Zero-Shot Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/zero-shot-text-to-image-generation</link>
      <description><![CDATA[Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-text-to-image-generation</guid>
    </item>
    <item>
      <title>Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing</title>
      <link>https://paperswithcode.com/paper/spatially-adaptive-multilayer-selection-for-1</link>
      <description><![CDATA[We propose a new method to invert and edit such complex images in the latent space of GANs, such as StyleGAN2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spatially-adaptive-multilayer-selection-for-1</guid>
    </item>
    <item>
      <title>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</title>
      <link>https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</link>
      <description><![CDATA[The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</guid>
    </item>
    <item>
      <title>Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging</title>
      <link>https://paperswithcode.com/paper/degradation-aware-unfolding-half-shuffle</link>
      <description><![CDATA[In coded aperture snapshot spectral compressive imaging (CASSI) systems, hyperspectral image (HSI) reconstruction methods are employed to recover the spatial-spectral signal from a compressed measurement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/degradation-aware-unfolding-half-shuffle</guid>
    </item>
    <item>
      <title>RegionCLIP: Region-based Language-Image Pretraining</title>
      <link>https://paperswithcode.com/paper/regionclip-region-based-language-image</link>
      <description><![CDATA[However, we show that directly applying such models to recognize image regions for object detection leads to poor performance due to a domain shift: CLIP was trained to match an image as a whole to a text description, without capturing the fine-grained alignment between image regions and text spans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/regionclip-region-based-language-image</guid>
    </item>
    <item>
      <title>CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers</title>
      <link>https://paperswithcode.com/paper/cogview2-faster-and-better-text-to-image</link>
      <description><![CDATA[The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogview2-faster-and-better-text-to-image</guid>
    </item>
    <item>
      <title>The Shapley Value in Machine Learning</title>
      <link>https://paperswithcode.com/paper/the-shapley-value-in-machine-learning</link>
      <description><![CDATA[Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-shapley-value-in-machine-learning</guid>
    </item>
    <item>
      <title>HaGRID -- HAnd Gesture Recognition Image Dataset</title>
      <link>https://paperswithcode.com/paper/hagrid-hand-gesture-recognition-image-dataset</link>
      <description><![CDATA[In this paper, we introduce an enormous dataset HaGRID (HAnd Gesture Recognition Image Dataset) for hand gesture recognition (HGR) systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hagrid-hand-gesture-recognition-image-dataset</guid>
    </item>
    <item>
      <title>ARF: Artistic Radiance Fields</title>
      <link>https://paperswithcode.com/paper/arf-artistic-radiance-fields</link>
      <description><![CDATA[We present a method for transferring the artistic features of an arbitrary style image to a 3D scene.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/arf-artistic-radiance-fields</guid>
    </item>
    <item>
      <title>OmniXAI: A Library for Explainable AI</title>
      <link>https://paperswithcode.com/paper/omnixai-a-library-for-explainable-ai</link>
      <description><![CDATA[We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnixai-a-library-for-explainable-ai</guid>
    </item>
    <item>
      <title>Improving GAN Equilibrium by Raising Spatial Awareness</title>
      <link>https://paperswithcode.com/paper/improving-gan-equilibrium-by-raising-spatial</link>
      <description><![CDATA[We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-gan-equilibrium-by-raising-spatial</guid>
    </item>
    <item>
      <title>Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning</title>
      <link>https://paperswithcode.com/paper/bridge-tower-building-bridges-between</link>
      <description><![CDATA[Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a cross-modal encoder, or feed the last-layer uni-modal features directly into the top cross-modal encoder, ignoring the semantic information at the different levels in the deep uni-modal encoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bridge-tower-building-bridges-between</guid>
    </item>
    <item>
      <title>How Well Do Sparse Imagenet Models Transfer?</title>
      <link>https://paperswithcode.com/paper/how-well-do-sparse-imagenet-models-transfer</link>
      <description><![CDATA[Transfer learning is a classic paradigm by which models pretrained on large "upstream" datasets are adapted to yield good results on "downstream" specialized datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-well-do-sparse-imagenet-models-transfer</guid>
    </item>
    <item>
      <title>General-purpose, long-context autoregressive modeling with Perceiver AR</title>
      <link>https://paperswithcode.com/paper/general-purpose-long-context-autoregressive</link>
      <description><![CDATA[Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-purpose-long-context-autoregressive</guid>
    </item>
    <item>
      <title>Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning</title>
      <link>https://paperswithcode.com/paper/scaling-vision-transformers-to-gigapixel-1</link>
      <description><![CDATA[Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e. g. - 256x256, 384384).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-vision-transformers-to-gigapixel-1</guid>
    </item>
    <item>
      <title>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
      <link>https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</link>
      <description><![CDATA[We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</guid>
    </item>
    <item>
      <title>Self-supervised Heterogeneous Graph Neural Network with Co-contrastive Learning</title>
      <link>https://paperswithcode.com/paper/self-supervised-heterogeneous-graph-neural</link>
      <description><![CDATA[Then the cross-view contrastive learning, as well as a view mask mechanism, is proposed, which is able to extract the positive and negative embeddings from two views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-supervised-heterogeneous-graph-neural</guid>
    </item>
  </channel>
</rss>
