<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 20 Jun 2023 21:06:15 +0000</lastBuildDate>
    <item>
      <title>FinGPT: Open-Source Financial Large Language Models</title>
      <link>https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</link>
      <description><![CDATA[While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fingpt-open-source-financial-large-language</guid>
    </item>
    <item>
      <title>Full Parameter Fine-tuning for Large Language Models with Limited Resources</title>
      <link>https://paperswithcode.com/paper/full-parameter-fine-tuning-for-large-language</link>
      <description><![CDATA[Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/full-parameter-fine-tuning-for-large-language</guid>
    </item>
    <item>
      <title>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</title>
      <link>https://paperswithcode.com/paper/self-supervised-learning-from-images-with-a</link>
      <description><![CDATA[This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-supervised-learning-from-images-with-a</guid>
    </item>
    <item>
      <title>Augmenting Language Models with Long-Term Memory</title>
      <link>https://paperswithcode.com/paper/augmenting-language-models-with-long-term</link>
      <description><![CDATA[Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/augmenting-language-models-with-long-term</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>WizardCoder: Empowering Code Large Language Models with Evol-Instruct</title>
      <link>https://paperswithcode.com/paper/wizardcoder-empowering-code-large-language</link>
      <description><![CDATA[Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardcoder-empowering-code-large-language</guid>
    </item>
    <item>
      <title>WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
      <link>https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</link>
      <description><![CDATA[We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webglm-towards-an-efficient-web-enhanced</guid>
    </item>
    <item>
      <title>ChemCrow: Augmenting large-language models with chemistry tools</title>
      <link>https://paperswithcode.com/paper/chemcrow-augmenting-large-language-models</link>
      <description><![CDATA[Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chemcrow-augmenting-large-language-models</guid>
    </item>
    <item>
      <title>MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing</title>
      <link>https://paperswithcode.com/paper/magicbrush-a-manually-annotated-dataset-for</link>
      <description><![CDATA[To address this issue, we introduce MagicBrush (https://osu-nlp-group. github. io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magicbrush-a-manually-annotated-dataset-for</guid>
    </item>
    <item>
      <title>SqueezeLLM: Dense-and-Sparse Quantization</title>
      <link>https://paperswithcode.com/paper/squeezellm-dense-and-sparse-quantization</link>
      <description><![CDATA[When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2. 1x as compared to the state-of-the-art methods with the same memory requirement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/squeezellm-dense-and-sparse-quantization</guid>
    </item>
    <item>
      <title>Segment Any Point Cloud Sequences by Distilling Vision Foundation Models</title>
      <link>https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by</link>
      <description><![CDATA[Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-any-point-cloud-sequences-by</guid>
    </item>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration</title>
      <link>https://paperswithcode.com/paper/macaw-llm-multi-modal-language-modeling-with</link>
      <description><![CDATA[Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/macaw-llm-multi-modal-language-modeling-with</guid>
    </item>
    <item>
      <title>DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</title>
      <link>https://paperswithcode.com/paper/dreamsim-learning-new-dimensions-of-human</link>
      <description><![CDATA[Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamsim-learning-new-dimensions-of-human</guid>
    </item>
    <item>
      <title>Reflexion: Language Agents with Verbal Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</link>
      <description><![CDATA[Large language models (LLMs) have been increasingly used to interact with external environments (e. g., games, compilers, APIs) as goal-driven agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</guid>
    </item>
    <item>
      <title>TAP-Vid: A Benchmark for Tracking Any Point in a Video</title>
      <link>https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</link>
      <description><![CDATA[Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</guid>
    </item>
    <item>
      <title>LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model</title>
      <link>https://paperswithcode.com/paper/lasuie-unifying-information-extraction-with</link>
      <description><![CDATA[Universally modeling all typical information extraction tasks (UIE) with one generative language model (GLM) has revealed great potential by the latest study, where various IE predictions are unified into a linearized hierarchical expression under a GLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lasuie-unifying-information-extraction-with</guid>
    </item>
    <item>
      <title>EasySpider: A No-Code Visual System for Crawling the Web</title>
      <link>https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</link>
      <description><![CDATA[As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</guid>
    </item>
    <item>
      <title>TART: A plug-and-play Transformer module for task-agnostic reasoning</title>
      <link>https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</link>
      <description><![CDATA[As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tart-a-plug-and-play-transformer-module-for</guid>
    </item>
    <item>
      <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
      <link>https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</link>
      <description><![CDATA[This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</guid>
    </item>
  </channel>
</rss>
