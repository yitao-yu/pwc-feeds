<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 05 May 2023 21:06:10 +0000</lastBuildDate>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models</title>
      <link>https://paperswithcode.com/paper/panda-llm-training-data-and-evaluation-for</link>
      <description><![CDATA[This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/panda-llm-training-data-and-evaluation-for</guid>
    </item>
    <item>
      <title>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</title>
      <link>https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</link>
      <description><![CDATA[In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i. e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
      <link>https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</link>
      <description><![CDATA[Real-world applications have high demands for semantic segmentation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</guid>
    </item>
    <item>
      <title>Unlimiformer: Long-Range Transformers with Unlimited Length Input</title>
      <link>https://paperswithcode.com/paper/unlimiformer-long-range-transformers-with</link>
      <description><![CDATA[This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unlimiformer-long-range-transformers-with</guid>
    </item>
    <item>
      <title>CodeGen2: Lessons for Training LLMs on Programming and Natural Languages</title>
      <link>https://paperswithcode.com/paper/codegen2-lessons-for-training-llms-on</link>
      <description><![CDATA[In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codegen2-lessons-for-training-llms-on</guid>
    </item>
    <item>
      <title>In-Context Learning Unlocked for Diffusion Models</title>
      <link>https://paperswithcode.com/paper/in-context-learning-unlocked-for-diffusion</link>
      <description><![CDATA[To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-unlocked-for-diffusion</guid>
    </item>
    <item>
      <title>A Survey of Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-survey-of-large-language-models</link>
      <description><![CDATA[To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-of-large-language-models</guid>
    </item>
    <item>
      <title>WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
      <link>https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</link>
      <description><![CDATA[By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</guid>
    </item>
    <item>
      <title>Track Anything: Segment Anything Meets Videos</title>
      <link>https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</link>
      <description><![CDATA[Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</guid>
    </item>
    <item>
      <title>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model</title>
      <link>https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</link>
      <description><![CDATA[This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-v2-parameter-efficient-visual</guid>
    </item>
    <item>
      <title>mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality</title>
      <link>https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</link>
      <description><![CDATA[Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github. com/X-PLUG/mPLUG-Owl.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mplug-owl-modularization-empowers-large</guid>
    </item>
    <item>
      <title>Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</title>
      <link>https://paperswithcode.com/paper/is-your-code-generated-by-chatgpt-really</link>
      <description><![CDATA[However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/is-your-code-generated-by-chatgpt-really</guid>
    </item>
    <item>
      <title>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</title>
      <link>https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</link>
      <description><![CDATA[In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</guid>
    </item>
    <item>
      <title>Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/pick-a-pic-an-open-dataset-of-user</link>
      <description><![CDATA[Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pick-a-pic-an-open-dataset-of-user</guid>
    </item>
    <item>
      <title>Segment Everything Everywhere All at Once</title>
      <link>https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</link>
      <description><![CDATA[Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</guid>
    </item>
    <item>
      <title>Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model</title>
      <link>https://paperswithcode.com/paper/text-to-audio-generation-using-instruction</link>
      <description><![CDATA[The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-to-audio-generation-using-instruction</guid>
    </item>
    <item>
      <title>Multimodal Procedural Planning via Dual Text-Image Prompting</title>
      <link>https://paperswithcode.com/paper/multimodal-procedural-planning-via-dual-text</link>
      <description><![CDATA[The key challenges of MPP are to ensure the informativeness, temporal coherence, and accuracy of plans across modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-procedural-planning-via-dual-text</guid>
    </item>
    <item>
      <title>Tool Learning with Foundation Models</title>
      <link>https://paperswithcode.com/paper/tool-learning-with-foundation-models</link>
      <description><![CDATA[Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tool-learning-with-foundation-models</guid>
    </item>
  </channel>
</rss>
