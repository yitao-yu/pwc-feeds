<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 04 Nov 2022 21:07:53 +0000</lastBuildDate>
    <item>
      <title>High Fidelity Neural Audio Compression</title>
      <link>https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</link>
      <description><![CDATA[We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</guid>
    </item>
    <item>
      <title>Pop2Piano : Pop Audio-based Piano Cover Generation</title>
      <link>https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</link>
      <description><![CDATA[The piano cover of pop music is widely enjoyed by people.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Text-Only Training for Image Captioning using Noise-Injected CLIP</title>
      <link>https://paperswithcode.com/paper/text-only-training-for-image-captioning-using</link>
      <description><![CDATA[We consider the task of image-captioning using only the CLIP model and additional text data at training time, and no additional captioned images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-only-training-for-image-captioning-using</guid>
    </item>
    <item>
      <title>ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization</title>
      <link>https://paperswithcode.com/paper/shapo-implicit-representations-for-multi</link>
      <description><![CDATA[A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shapo-implicit-representations-for-multi</guid>
    </item>
    <item>
      <title>Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform</title>
      <link>https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</link>
      <description><![CDATA[We propose a lightweight end-to-end text-to-speech model using multi-band generation and inverse short-time Fourier transform.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightweight-and-high-fidelity-end-to-end-text</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation</title>
      <link>https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</link>
      <description><![CDATA[In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vox-fusion-dense-tracking-and-mapping-with</guid>
    </item>
    <item>
      <title>Language models enable zero-shot prediction of the effects of mutations on protein function</title>
      <link>https://paperswithcode.com/paper/language-models-enable-zero-shot-prediction</link>
      <description><![CDATA[Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-enable-zero-shot-prediction</guid>
    </item>
    <item>
      <title>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models</title>
      <link>https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</link>
      <description><![CDATA[We analyze prompts in the dataset and discuss key properties of these prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusiondb-a-large-scale-prompt-gallery</guid>
    </item>
    <item>
      <title>Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective</title>
      <link>https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</link>
      <description><![CDATA[We propose a new paradigm for zero-shot learners that is format agnostic, i. e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</guid>
    </item>
    <item>
      <title>UniInst: Unique Representation for End-to-End Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/uniinst-unique-representation-for-end-to-end</link>
      <description><![CDATA[Existing instance segmentation methods have achieved impressive performance but still suffer from a common dilemma: redundant representations (e. g., multiple boxes, grids, and anchor points) are inferred for one instance, which leads to multiple duplicated predictions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uniinst-unique-representation-for-end-to-end</guid>
    </item>
    <item>
      <title>Retrieval-efficiency trade-off of Unsupervised Keyword Extraction</title>
      <link>https://paperswithcode.com/paper/retrieval-efficiency-trade-off-of</link>
      <description><![CDATA[Efficiently identifying keyphrases that represent a given document is a challenging task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-efficiency-trade-off-of</guid>
    </item>
    <item>
      <title>A Surprising Thing: The Application of Machine Learning Ensembles and Signal Theory to Predict Earnings Surprises</title>
      <link>https://paperswithcode.com/paper/a-surprising-thing-the-application-of-machine</link>
      <description><![CDATA[Nonlinear classification models can predict future earnings surprises with a high accuracy by using pricing and earnings input data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-surprising-thing-the-application-of-machine</guid>
    </item>
    <item>
      <title>Unsupervised visualization of image datasets using contrastive learning</title>
      <link>https://paperswithcode.com/paper/unsupervised-visualization-of-image-datasets</link>
      <description><![CDATA[This problem can be circumvented by self-supervised approaches based on contrastive learning, such as SimCLR, relying on data augmentation to generate implicit neighbors, but these methods do not produce two-dimensional embeddings suitable for visualization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-visualization-of-image-datasets</guid>
    </item>
    <item>
      <title>Focal Modulation Networks</title>
      <link>https://paperswithcode.com/paper/focal-modulation-networks</link>
      <description><![CDATA[For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2. 4, and beats Swin at multi-scale (50. 5 v. s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/focal-modulation-networks</guid>
    </item>
    <item>
      <title>DreamFusion: Text-to-3D using 2D Diffusion</title>
      <link>https://paperswithcode.com/paper/dreamfusion-text-to-3d-using-2d-diffusion</link>
      <description><![CDATA[Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamfusion-text-to-3d-using-2d-diffusion</guid>
    </item>
    <item>
      <title>Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models</title>
      <link>https://paperswithcode.com/paper/adan-adaptive-nesterov-momentum-algorithm-for</link>
      <description><![CDATA[Then Adan adopts NME to estimate the first- and second-order moments of the gradient in adaptive gradient algorithms for convergence acceleration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adan-adaptive-nesterov-momentum-algorithm-for</guid>
    </item>
    <item>
      <title>Monocular Dynamic View Synthesis: A Reality Check</title>
      <link>https://paperswithcode.com/paper/monocular-dynamic-view-synthesis-a-reality</link>
      <description><![CDATA[We study the recent progress on dynamic view synthesis (DVS) from monocular video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monocular-dynamic-view-synthesis-a-reality</guid>
    </item>
    <item>
      <title>Adapting Pretrained Text-to-Text Models for Long Text Sequences</title>
      <link>https://paperswithcode.com/paper/adapting-pretrained-text-to-text-models-for</link>
      <description><![CDATA[We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adapting-pretrained-text-to-text-models-for</guid>
    </item>
  </channel>
</rss>
