<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 13 Jun 2025 21:09:46 +0000</lastBuildDate>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>self-prompting analogical reasoning for uav object detection</title>
      <link>https://paperswithcode.com/paper/self-prompting-analogical-reasoning-for-uav</link>
      <description><![CDATA[While for analogical reasoningmodule, graph nodes consist of category-level prompt nodes and pixel-level image feature nodes. Analogical inference is based on graph convolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-prompting-analogical-reasoning-for-uav</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention</title>
      <link>https://paperswithcode.com/paper/direct3d-s2-gigascale-3d-generation-made-easy</link>
      <description><![CDATA[Generating high-resolution 3D shapes using volumetric representations such as Signed Distance Functions (SDFs) presents substantial computational and memory challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/direct3d-s2-gigascale-3d-generation-made-easy</guid>
    </item>
    <item>
      <title>SkyReels-V2: Infinite-length Film Generative Model</title>
      <link>https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</link>
      <description><![CDATA[Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/skyreels-v2-infinite-length-film-generative</guid>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
      <link>https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</link>
      <description><![CDATA[To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</guid>
    </item>
    <item>
      <title>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</title>
      <link>https://paperswithcode.com/paper/kvzip-query-agnostic-kv-cache-compression</link>
      <description><![CDATA[Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kvzip-query-agnostic-kv-cache-compression</guid>
    </item>
    <item>
      <title>Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</title>
      <link>https://paperswithcode.com/paper/darwin-godel-machine-open-ended-evolution-of</link>
      <description><![CDATA[The G\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/darwin-godel-machine-open-ended-evolution-of</guid>
    </item>
    <item>
      <title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
      <link>https://paperswithcode.com/paper/areal-a-large-scale-asynchronous</link>
      <description><![CDATA[Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/areal-a-large-scale-asynchronous</guid>
    </item>
    <item>
      <title>Wings: Learning Multimodal LLMs without Text-only Forgetting</title>
      <link>https://paperswithcode.com/paper/wings-learning-multimodal-llms-without-text</link>
      <description><![CDATA[Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wings-learning-multimodal-llms-without-text</guid>
    </item>
    <item>
      <title>Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers</title>
      <link>https://paperswithcode.com/paper/paper2poster-towards-multimodal-poster</link>
      <description><![CDATA[To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paper2poster-towards-multimodal-poster</guid>
    </item>
    <item>
      <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</link>
      <description><![CDATA[This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</guid>
    </item>
    <item>
      <title>QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos</title>
      <link>https://paperswithcode.com/paper/queen-quantized-efficient-encoding-of-dynamic</link>
      <description><![CDATA[Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/queen-quantized-efficient-encoding-of-dynamic</guid>
    </item>
    <item>
      <title>RWKV-7 "Goose" with Expressive Dynamic State Evolution</title>
      <link>https://paperswithcode.com/paper/rwkv-7-goose-with-expressive-dynamic-state</link>
      <description><![CDATA[We present RWKV-7 "Goose", a new sequence modeling architecture with constant memory usage and constant inference time per token.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rwkv-7-goose-with-expressive-dynamic-state</guid>
    </item>
    <item>
      <title>Representing Long Volumetric Video with Temporal Gaussian Hierarchy</title>
      <link>https://paperswithcode.com/paper/representing-long-volumetric-video-with</link>
      <description><![CDATA[In addition, the tree-like structure of the Gaussian hierarchy allows us to efficiently represent the scene at a particular moment with a subset of Gaussian primitives, leading to nearly constant GPU memory usage during the training or rendering regardless of the video length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/representing-long-volumetric-video-with</guid>
    </item>
    <item>
      <title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
      <link>https://paperswithcode.com/paper/omniaudio-generating-spatial-audio-from-360</link>
      <description><![CDATA[To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omniaudio-generating-spatial-audio-from-360</guid>
    </item>
    <item>
      <title>Advanced long-term earth system forecasting by learning the small-scale nature</title>
      <link>https://paperswithcode.com/paper/advanced-long-term-earth-system-forecasting</link>
      <description><![CDATA[Reliable long-term forecast of Earth system dynamics is heavily hampered by instabilities in current AI models during extended autoregressive simulations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advanced-long-term-earth-system-forecasting</guid>
    </item>
    <item>
      <title>SEW: Self-Evolving Agentic Workflows for Automated Code Generation</title>
      <link>https://paperswithcode.com/paper/sew-self-evolving-agentic-workflows-for</link>
      <description><![CDATA[Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sew-self-evolving-agentic-workflows-for</guid>
    </item>
    <item>
      <title>VACE: All-in-One Video Creation and Editing</title>
      <link>https://paperswithcode.com/paper/vace-all-in-one-video-creation-and-editing</link>
      <description><![CDATA[Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vace-all-in-one-video-creation-and-editing</guid>
    </item>
    <item>
      <title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
      <link>https://paperswithcode.com/paper/which-agent-causes-task-failures-and-when-on</link>
      <description><![CDATA[In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/which-agent-causes-task-failures-and-when-on</guid>
    </item>
  </channel>
</rss>
