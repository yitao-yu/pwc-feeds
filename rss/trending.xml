<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 30 Jun 2024 21:08:12 +0000</lastBuildDate>
    <item>
      <title>ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning</title>
      <link>https://paperswithcode.com/paper/exvideo-extending-video-diffusion-models-via</link>
      <description><![CDATA[To evaluate the efficacy of our proposed post-tuning approach, we conduct extension training on the Stable Video Diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exvideo-extending-video-diffusion-models-via</guid>
    </item>
    <item>
      <title>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</title>
      <link>https://paperswithcode.com/paper/4m-21-an-any-to-any-vision-model-for-tens-of</link>
      <description><![CDATA[In this paper, we expand upon the capabilities of them by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/4m-21-an-any-to-any-vision-model-for-tens-of</guid>
    </item>
    <item>
      <title>Depth Anything V2</title>
      <link>https://paperswithcode.com/paper/depth-anything-v2</link>
      <description><![CDATA[This work presents Depth Anything V2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-v2</guid>
    </item>
    <item>
      <title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</link>
      <description><![CDATA[We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</guid>
    </item>
    <item>
      <title>EvTexture: Event-driven Texture Enhancement for Video Super-Resolution</title>
      <link>https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</link>
      <description><![CDATA[Our method, called EvTexture, leverages high-frequency details of events to better recover texture regions in VSR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</guid>
    </item>
    <item>
      <title>Long Context Transfer from Language to Vision</title>
      <link>https://paperswithcode.com/paper/long-context-transfer-from-language-to-vision</link>
      <description><![CDATA[By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/long-context-transfer-from-language-to-vision</guid>
    </item>
    <item>
      <title>LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language</title>
      <link>https://paperswithcode.com/paper/langgpt-rethinking-structured-reusable-prompt</link>
      <description><![CDATA[We have built a community on LangGPT to facilitate the tuition and sharing of prompt design.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/langgpt-rethinking-structured-reusable-prompt</guid>
    </item>
    <item>
      <title>Meta Learning Text-to-Speech Synthesis in over 7000 Languages</title>
      <link>https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</link>
      <description><![CDATA[In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</guid>
    </item>
    <item>
      <title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title>
      <link>https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</link>
      <description><![CDATA[In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</guid>
    </item>
    <item>
      <title>TextGrad: Automatic "Differentiation" via Text</title>
      <link>https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</link>
      <description><![CDATA[Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</guid>
    </item>
    <item>
      <title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mg-llava-towards-multi-granularity-visual</link>
      <description><![CDATA[We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mg-llava-towards-multi-granularity-visual</guid>
    </item>
    <item>
      <title>DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation</title>
      <link>https://paperswithcode.com/paper/dreambench-a-human-aligned-benchmark-for</link>
      <description><![CDATA[Personalized image generation holds great promise in assisting humans in everyday work and life due to its impressive function in creatively generating personalized content.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambench-a-human-aligned-benchmark-for</guid>
    </item>
    <item>
      <title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title>
      <link>https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</link>
      <description><![CDATA[Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</guid>
    </item>
    <item>
      <title>AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation</title>
      <link>https://paperswithcode.com/paper/autostudio-crafting-consistent-subjects-in</link>
      <description><![CDATA[As cutting-edge Text-to-Image (T2I) generation models already excel at producing remarkable single images, an even more challenging task, i. e., multi-turn interactive image generation begins to attract the attention of related research communities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autostudio-crafting-consistent-subjects-in</guid>
    </item>
    <item>
      <title>MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model</title>
      <link>https://paperswithcode.com/paper/mofa-video-controllable-image-animation-via</link>
      <description><![CDATA[We present MOFA-Video, an advanced controllable image animation method that generates video from the given image using various additional controllable signals (such as human landmarks reference, manual trajectories, and another even provided video) or their combinations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mofa-video-controllable-image-animation-via</guid>
    </item>
    <item>
      <title>MegActor: Harness the Power of Raw Video for Vivid Portrait Animation</title>
      <link>https://paperswithcode.com/paper/megactor-harness-the-power-of-raw-video-for</link>
      <description><![CDATA[Despite raw driving videos contain richer information on facial expressions than intermediate representations such as landmarks in the field of portrait animation, they are seldom the subject of research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/megactor-harness-the-power-of-raw-video-for</guid>
    </item>
    <item>
      <title>Adam-mini: Use Fewer Learning Rates To Gain More</title>
      <link>https://paperswithcode.com/paper/adam-mini-use-fewer-learning-rates-to-gain</link>
      <description><![CDATA[We find that $\geq$ 90% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our proposed principle on Hessian structure; (2) assign a single but good learning rate to each parameter block.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adam-mini-use-fewer-learning-rates-to-gain</guid>
    </item>
    <item>
      <title>DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence</title>
      <link>https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</link>
      <description><![CDATA[Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of</guid>
    </item>
    <item>
      <title>Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text</title>
      <link>https://paperswithcode.com/paper/director3d-real-world-camera-trajectory-and</link>
      <description><![CDATA[To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/director3d-real-world-camera-trajectory-and</guid>
    </item>
    <item>
      <title>Scalable MatMul-free Language Modeling</title>
      <link>https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</link>
      <description><![CDATA[Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2. 7B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</guid>
    </item>
  </channel>
</rss>
