<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 03 Mar 2025 21:09:19 +0000</lastBuildDate>
    <item>
      <title>Fractal Generative Models</title>
      <link>https://paperswithcode.com/paper/fractal-generative-models</link>
      <description><![CDATA[In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fractal-generative-models</guid>
    </item>
    <item>
      <title>From System 1 to System 2: A Survey of Reasoning Large Language Models</title>
      <link>https://paperswithcode.com/paper/from-system-1-to-system-2-a-survey-of</link>
      <description><![CDATA[Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-system-1-to-system-2-a-survey-of</guid>
    </item>
    <item>
      <title>Magma: A Foundation Model for Multimodal AI Agents</title>
      <link>https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</link>
      <description><![CDATA[We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</guid>
    </item>
    <item>
      <title>R1-OnevisionAn Open-Source Multimodal Large Language Model Capable of Deep Reasoning</title>
      <link>https://paperswithcode.com/paper/r1-onevision-an-open-source-multimodal-large</link>
      <description><![CDATA[R1-OneVision is a versatile multimodal reasoning large model, designed to tackle complex visual reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r1-onevision-an-open-source-multimodal-large</guid>
    </item>
    <item>
      <title>PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation</title>
      <link>https://paperswithcode.com/paper/pike-rag-specialized-knowledge-and-rationale</link>
      <description><![CDATA[Despite notable advancements in Retrieval-Augmented Generation (RAG) systems that expand large language model (LLM) capabilities through external retrieval, these systems often struggle to meet the complex and diverse needs of real-world industrial applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pike-rag-specialized-knowledge-and-rationale</guid>
    </item>
    <item>
      <title>Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator</title>
      <link>https://paperswithcode.com/paper/distill-any-depth-distillation-creates-a</link>
      <description><![CDATA[In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/distill-any-depth-distillation-creates-a</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
    <item>
      <title>HybridFlow: A Flexible and Efficient RLHF Framework</title>
      <link>https://paperswithcode.com/paper/hybridflow-a-flexible-and-efficient-rlhf</link>
      <description><![CDATA[Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hybridflow-a-flexible-and-efficient-rlhf</guid>
    </item>
    <item>
      <title>Slamming: Training a Speech Language Model on One GPU in a Day</title>
      <link>https://paperswithcode.com/paper/slamming-training-a-speech-language-model-on</link>
      <description><![CDATA[We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slamming-training-a-speech-language-model-on</guid>
    </item>
    <item>
      <title>Training AI to be Loyal</title>
      <link>https://paperswithcode.com/paper/training-ai-to-be-loyal</link>
      <description><![CDATA[The key scientific question then is: how can we build models that are openly accessible (open source) and yet are owned and governed by the community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-ai-to-be-loyal</guid>
    </item>
    <item>
      <title>PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</title>
      <link>https://paperswithcode.com/paper/photodoodle-learning-artistic-image-editing</link>
      <description><![CDATA[Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photodoodle-learning-artistic-image-editing</guid>
    </item>
    <item>
      <title>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</title>
      <link>https://paperswithcode.com/paper/diception-a-generalist-diffusion-model-for</link>
      <description><![CDATA[We achieve results on par with SAM-vit-h using only 0. 06% of their data (e. g., 600K vs. 1B pixel-level annotated images).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diception-a-generalist-diffusion-model-for</guid>
    </item>
    <item>
      <title>Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction</title>
      <link>https://paperswithcode.com/paper/step-audio-unified-understanding-and</link>
      <description><![CDATA[Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/step-audio-unified-understanding-and</guid>
    </item>
    <item>
      <title>Hawk: Learning to Understand Open-World Video Anomalies</title>
      <link>https://paperswithcode.com/paper/hawk-learning-to-understand-open-world-video</link>
      <description><![CDATA[Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hawk-learning-to-understand-open-world-video</guid>
    </item>
    <item>
      <title>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</title>
      <link>https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</link>
      <description><![CDATA[We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseek-vl2-mixture-of-experts-vision</guid>
    </item>
    <item>
      <title>Audio-FLAN: A Preliminary Release</title>
      <link>https://paperswithcode.com/paper/audio-flan-a-preliminary-release</link>
      <description><![CDATA[Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e. g., transcription, comprehension) and generation (e. g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audio-flan-a-preliminary-release</guid>
    </item>
    <item>
      <title>SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</title>
      <link>https://paperswithcode.com/paper/spargeattn-accurate-sparse-attention</link>
      <description><![CDATA[A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spargeattn-accurate-sparse-attention</guid>
    </item>
    <item>
      <title>Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction</title>
      <link>https://paperswithcode.com/paper/baichuan-audio-a-unified-framework-for-end-to</link>
      <description><![CDATA[To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/baichuan-audio-a-unified-framework-for-end-to</guid>
    </item>
    <item>
      <title>EliGen: Entity-Level Controlled Image Generation with Regional Attention</title>
      <link>https://paperswithcode.com/paper/eligen-entity-level-controlled-image</link>
      <description><![CDATA[Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eligen-entity-level-controlled-image</guid>
    </item>
    <item>
      <title>Free-Form Image Inpainting with Gated Convolution</title>
      <link>https://paperswithcode.com/paper/free-form-image-inpainting-with-gated</link>
      <description><![CDATA[We present a generative image inpainting system to complete images with free-form mask and guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/free-form-image-inpainting-with-gated</guid>
    </item>
  </channel>
</rss>
