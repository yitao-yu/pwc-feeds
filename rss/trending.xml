<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 20 Jun 2022 09:15:21 +0000</lastBuildDate>
    <item>
      <title>CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers</title>
      <link>https://paperswithcode.com/paper/cogview2-faster-and-better-text-to-image</link>
      <description><![CDATA[The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogview2-faster-and-better-text-to-image</guid>
    </item>
    <item>
      <title>Zero-Shot Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/zero-shot-text-to-image-generation</link>
      <description><![CDATA[Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-text-to-image-generation</guid>
    </item>
    <item>
      <title>Multiplying Matrices Without Multiplying</title>
      <link>https://paperswithcode.com/paper/multiplying-matrices-without-multiplying</link>
      <description><![CDATA[Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multiplying-matrices-without-multiplying</guid>
    </item>
    <item>
      <title>Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case</title>
      <link>https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</link>
      <description><![CDATA[In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</guid>
    </item>
    <item>
      <title>Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing</title>
      <link>https://paperswithcode.com/paper/spatially-adaptive-multilayer-selection-for-1</link>
      <description><![CDATA[We propose a new method to invert and edit such complex images in the latent space of GANs, such as StyleGAN2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spatially-adaptive-multilayer-selection-for-1</guid>
    </item>
    <item>
      <title>Implicit Sample Extension for Unsupervised Person Re-Identification</title>
      <link>https://paperswithcode.com/paper/implicit-sample-extension-for-unsupervised</link>
      <description><![CDATA[Specifically, we generate support samples from actual samples and their neighbouring clusters in the embedding space through a progressive linear interpolation (PLI) strategy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/implicit-sample-extension-for-unsupervised</guid>
    </item>
    <item>
      <title>Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging</title>
      <link>https://paperswithcode.com/paper/degradation-aware-unfolding-half-shuffle</link>
      <description><![CDATA[In coded aperture snapshot spectral compressive imaging (CASSI) systems, hyperspectral image (HSI) reconstruction methods are employed to recover the spatial-spectral signal from a compressed measurement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/degradation-aware-unfolding-half-shuffle</guid>
    </item>
    <item>
      <title>BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</title>
      <link>https://paperswithcode.com/paper/bevformer-learning-bird-s-eye-view</link>
      <description><![CDATA[In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bevformer-learning-bird-s-eye-view</guid>
    </item>
    <item>
      <title>Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt</title>
      <link>https://paperswithcode.com/paper/prioritized-training-on-points-that-are-1</link>
      <description><![CDATA[But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prioritized-training-on-points-that-are-1</guid>
    </item>
    <item>
      <title>General-purpose, long-context autoregressive modeling with Perceiver AR</title>
      <link>https://paperswithcode.com/paper/general-purpose-long-context-autoregressive</link>
      <description><![CDATA[Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-purpose-long-context-autoregressive</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Variable Bitrate Neural Fields</title>
      <link>https://paperswithcode.com/paper/variable-bitrate-neural-fields</link>
      <description><![CDATA[Neural approximations of scalar and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/variable-bitrate-neural-fields</guid>
    </item>
    <item>
      <title>Improving GAN Equilibrium by Raising Spatial Awareness</title>
      <link>https://paperswithcode.com/paper/improving-gan-equilibrium-by-raising-spatial</link>
      <description><![CDATA[We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-gan-equilibrium-by-raising-spatial</guid>
    </item>
    <item>
      <title>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
      <link>https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</link>
      <description><![CDATA[We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</guid>
    </item>
    <item>
      <title>PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies</title>
      <link>https://paperswithcode.com/paper/pointnext-revisiting-pointnet-with-improved</link>
      <description><![CDATA[In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pointnext-revisiting-pointnet-with-improved</guid>
    </item>
    <item>
      <title>PaddleRec</title>
      <link>https://github.com/PaddlePaddle/PaddleRec</link>
      <description><![CDATA[Recommendation AlgorithmLRWide&DeepDSSMTDMMINDWord2VecBert4RecDeepWalkSSRAITMDSINSIGNIPRECGRU4RecYoutube_dnnNCFGNNFMFFMDeepFMDCNDINDIENDLRMMMOEPLEESMMESCMM, MAMLxDeepFMDeepFEFMNFMAFMRALMDMRGateNetNAMLDIFMDeep CrossingPNNBSTAutoIntFGCNNFLENFibinetListWiseDeepRecENSFMTiSASAutoFIScriteo movielens]]></description>
      <guid isPermaLink="true">https://github.com/PaddlePaddle/PaddleRec</guid>
    </item>
    <item>
      <title>Online Segmentation of LiDAR Sequences: Dataset and Algorithm</title>
      <link>https://paperswithcode.com/paper/online-segmentation-of-lidar-sequences</link>
      <description><![CDATA[Helix4D operates on acquisition slices that correspond to a fraction of a full rotation of the sensor, significantly reducing the total latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/online-segmentation-of-lidar-sequences</guid>
    </item>
    <item>
      <title>Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline</title>
      <link>https://paperswithcode.com/paper/trajectory-guided-control-prediction-for-end</link>
      <description><![CDATA[The two branches are connected so that the control branch receives corresponding guidance from the trajectory branch at each time step.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/trajectory-guided-control-prediction-for-end</guid>
    </item>
    <item>
      <title>Combining Label Propagation and Simple Models Out-performs Graph Neural Networks</title>
      <link>https://paperswithcode.com/paper/combining-label-propagation-and-simple-models-1</link>
      <description><![CDATA[Graph Neural Networks (GNNs) are the predominant technique for learning over graphs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/combining-label-propagation-and-simple-models-1</guid>
    </item>
    <item>
      <title>GLIPv2: Unifying Localization and Vision-Language Understanding</title>
      <link>https://paperswithcode.com/paper/glipv2-unifying-localization-and-vision</link>
      <description><![CDATA[We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e. g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e. g., VQA, image captioning).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glipv2-unifying-localization-and-vision</guid>
    </item>
  </channel>
</rss>
