<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 03 Jul 2022 09:14:21 +0000</lastBuildDate>
    <item>
      <title>Pen and Paper Exercises in Machine Learning</title>
      <link>https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</link>
      <description><![CDATA[This is a collection of (mostly) pen-and-paper exercises in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</guid>
    </item>
    <item>
      <title>LViT: Language meets Vision Transformer in Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/lvit-language-meets-vision-transformer-in</link>
      <description><![CDATA[In our model, medical text annotation is introduced to compensate for the quality deficiency in image data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lvit-language-meets-vision-transformer-in</guid>
    </item>
    <item>
      <title>Multi-Graph Fusion Networks for Urban Region Embedding</title>
      <link>https://paperswithcode.com/paper/multi-graph-fusion-networks-for-urban-region</link>
      <description><![CDATA[Human mobility data contains rich but abundant information, which yields to the comprehensive region embeddings for cross domain tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-graph-fusion-networks-for-urban-region</guid>
    </item>
    <item>
      <title>Denoised MDPs: Learning World Models Better Than the World Itself</title>
      <link>https://paperswithcode.com/paper/denoised-mdps-learning-world-models-better</link>
      <description><![CDATA[The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/denoised-mdps-learning-world-models-better</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Forecasting Future World Events with Neural Networks</title>
      <link>https://paperswithcode.com/paper/forecasting-future-world-events-with-neural</link>
      <description><![CDATA[We test language models on our forecasting task and find that performance is far below a human expert baseline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/forecasting-future-world-events-with-neural</guid>
    </item>
    <item>
      <title>ProGen2: Exploring the Boundaries of Protein Language Models</title>
      <link>https://paperswithcode.com/paper/progen2-exploring-the-boundaries-of-protein</link>
      <description><![CDATA[Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/progen2-exploring-the-boundaries-of-protein</guid>
    </item>
    <item>
      <title>BokehMe: When Neural Rendering Meets Classical Rendering</title>
      <link>https://paperswithcode.com/paper/bokehme-when-neural-rendering-meets-classical-1</link>
      <description><![CDATA[Based on this formulation, we implement the classical renderer by a scattering-based method and propose a two-stage neural renderer to fix the erroneous areas from the classical renderer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bokehme-when-neural-rendering-meets-classical-1</guid>
    </item>
    <item>
      <title>BoT-SORT: Robust Associations Multi-Pedestrian Tracking</title>
      <link>https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</link>
      <description><![CDATA[The goal of multi-object tracking (MOT) is detecting and tracking all the objects in a scene, while keeping a unique identifier for each object.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bot-sort-robust-associations-multi-pedestrian</guid>
    </item>
    <item>
      <title>PolarFormer: Multi-camera 3D Object Detection with Polar Transformer</title>
      <link>https://paperswithcode.com/paper/polarformer-multi-camera-3d-object-detection</link>
      <description><![CDATA[3D object detection in autonomous driving aims to reason "what" and "where" the objects of interest present in a 3D world.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/polarformer-multi-camera-3d-object-detection</guid>
    </item>
    <item>
      <title>Zero-Shot Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/zero-shot-text-to-image-generation</link>
      <description><![CDATA[Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-text-to-image-generation</guid>
    </item>
    <item>
      <title>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</title>
      <link>https://paperswithcode.com/paper/minedojo-building-open-ended-embodied-agents</link>
      <description><![CDATA[Autonomous agents have made great strides in specialist domains like Atari games and Go.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minedojo-building-open-ended-embodied-agents</guid>
    </item>
    <item>
      <title>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</title>
      <link>https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</link>
      <description><![CDATA[The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</guid>
    </item>
    <item>
      <title>HiVT: Hierarchical Vector Transformer for Multi-Agent Motion Prediction</title>
      <link>https://paperswithcode.com/paper/hivt-hierarchical-vector-transformer-for</link>
      <description><![CDATA[To tackle this challenge, we propose Hierarchical Vector Transformer (HiVT) for fast and accurate multi-agent motion prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hivt-hierarchical-vector-transformer-for</guid>
    </item>
    <item>
      <title>Federated Learning with Fair Averaging</title>
      <link>https://paperswithcode.com/paper/federated-learning-with-fair-averaging</link>
      <description><![CDATA[Fairness has emerged as a critical problem in federated learning (FL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/federated-learning-with-fair-averaging</guid>
    </item>
    <item>
      <title>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre</link>
      <description><![CDATA[Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip-bootstrapping-language-image-pre</guid>
    </item>
    <item>
      <title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
      <link>https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</link>
      <description><![CDATA[We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/elucidating-the-design-space-of-diffusion</guid>
    </item>
    <item>
      <title>Preconditioner on Matrix Lie Group for SGD</title>
      <link>https://paperswithcode.com/paper/preconditioner-on-matrix-lie-group-for-sgd</link>
      <description><![CDATA[We study two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods in a unified framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/preconditioner-on-matrix-lie-group-for-sgd</guid>
    </item>
    <item>
      <title>TSM: Temporal Shift Module for Efficient Video Understanding</title>
      <link>https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video</link>
      <description><![CDATA[The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/temporal-shift-module-for-efficient-video</guid>
    </item>
    <item>
      <title>Augmented Shortcuts for Vision Transformers</title>
      <link>https://paperswithcode.com/paper/augmented-shortcuts-for-vision-transformers</link>
      <description><![CDATA[Transformer models have achieved great progress on computer vision tasks recently.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/augmented-shortcuts-for-vision-transformers</guid>
    </item>
  </channel>
</rss>
