<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 06 May 2024 09:14:28 +0000</lastBuildDate>
    <item>
      <title>KAN: Kolmogorov-Arnold Networks</title>
      <link>https://paperswithcode.com/paper/kan-kolmogorov-arnold-networks</link>
      <description><![CDATA[Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kan-kolmogorov-arnold-networks</guid>
    </item>
    <item>
      <title>StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</title>
      <link>https://paperswithcode.com/paper/storydiffusion-consistent-self-attention-for</link>
      <description><![CDATA[This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/storydiffusion-consistent-self-attention-for</guid>
    </item>
    <item>
      <title>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</title>
      <link>https://paperswithcode.com/paper/prometheus-2-an-open-source-language-model</link>
      <description><![CDATA[Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prometheus-2-an-open-source-language-model</guid>
    </item>
    <item>
      <title>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</title>
      <link>https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</link>
      <description><![CDATA[Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</guid>
    </item>
    <item>
      <title>Improving Diffusion Models for Virtual Try-on</title>
      <link>https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</link>
      <description><![CDATA[Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</guid>
    </item>
    <item>
      <title>Lightplane: Highly-Scalable Components for Neural 3D Fields</title>
      <link>https://paperswithcode.com/paper/lightplane-highly-scalable-components-for</link>
      <description><![CDATA[Contemporary 3D research, particularly in reconstruction and generation, heavily relies on 2D images for inputs or supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightplane-highly-scalable-components-for</guid>
    </item>
    <item>
      <title>ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving</title>
      <link>https://paperswithcode.com/paper/consistentid-portrait-generation-with</link>
      <description><![CDATA[ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/consistentid-portrait-generation-with</guid>
    </item>
    <item>
      <title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</title>
      <link>https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</link>
      <description><![CDATA[Compared to both open-source and proprietary models, InternVL 1. 5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</guid>
    </item>
    <item>
      <title>PuLID: Pure and Lightning ID Customization via Contrastive Alignment</title>
      <link>https://paperswithcode.com/paper/pulid-pure-and-lightning-id-customization-via</link>
      <description><![CDATA[We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pulid-pure-and-lightning-id-customization-via</guid>
    </item>
    <item>
      <title>RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</title>
      <link>https://paperswithcode.com/paper/rag-and-rau-a-survey-on-retrieval-augmented</link>
      <description><![CDATA[Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rag-and-rau-a-survey-on-retrieval-augmented</guid>
    </item>
    <item>
      <title>AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains Into One</title>
      <link>https://paperswithcode.com/paper/am-radio-agglomerative-model-reduce-all</link>
      <description><![CDATA[A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/am-radio-agglomerative-model-reduce-all</guid>
    </item>
    <item>
      <title>Spectrally Pruned Gaussian Fields with Neural Compensation</title>
      <link>https://paperswithcode.com/paper/spectrally-pruned-gaussian-fields-with-neural</link>
      <description><![CDATA[However, this comes with high memory consumption, e. g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spectrally-pruned-gaussian-fields-with-neural</guid>
    </item>
    <item>
      <title>WavCraft: Audio Editing and Generation with Natural Language Prompts</title>
      <link>https://paperswithcode.com/paper/wavcraft-audio-editing-and-generation-with</link>
      <description><![CDATA[We introduce WavCraft, a collective system that leverages large language models (LLMs) to connect diverse task-specific models for audio content creation and editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wavcraft-audio-editing-and-generation-with</guid>
    </item>
    <item>
      <title>PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning</title>
      <link>https://paperswithcode.com/paper/pllava-parameter-free-llava-extension-from-1</link>
      <description><![CDATA[PLLaVA achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pllava-parameter-free-llava-extension-from-1</guid>
    </item>
    <item>
      <title>Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding</title>
      <link>https://paperswithcode.com/paper/meta-prompting-enhancing-language-models-with</link>
      <description><![CDATA[This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-prompting-enhancing-language-models-with</guid>
    </item>
    <item>
      <title>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent</title>
      <link>https://paperswithcode.com/paper/flowmap-high-quality-camera-poses-intrinsics</link>
      <description><![CDATA[This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flowmap-high-quality-camera-poses-intrinsics</guid>
    </item>
    <item>
      <title>MicroDreamer: Zero-shot 3D Generation in $\sim$20 Seconds by Score-based Iterative Reconstruction</title>
      <link>https://paperswithcode.com/paper/microdreamer-zero-shot-3d-generation-in-sim</link>
      <description><![CDATA[In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/microdreamer-zero-shot-3d-generation-in-sim</guid>
    </item>
    <item>
      <title>CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</title>
      <link>https://paperswithcode.com/paper/catlip-clip-level-visual-recognition-accuracy</link>
      <description><![CDATA[Contrastive learning has emerged as a transformative method for learning effective visual representations through the alignment of image and text embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/catlip-clip-level-visual-recognition-accuracy</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>MemGPT: Towards LLMs as Operating Systems</title>
      <link>https://paperswithcode.com/paper/memgpt-towards-llms-as-operating-systems</link>
      <description><![CDATA[Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/memgpt-towards-llms-as-operating-systems</guid>
    </item>
  </channel>
</rss>
