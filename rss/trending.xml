<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 05 Mar 2024 21:07:04 +0000</lastBuildDate>
    <item>
      <title>Transparent Image Layer Diffusion using Latent Transparency</title>
      <link>https://paperswithcode.com/paper/transparent-image-layer-diffusion-using</link>
      <description><![CDATA[We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transparent-image-layer-diffusion-using</guid>
    </item>
    <item>
      <title>Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases</title>
      <link>https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing</link>
      <description><![CDATA[Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intent-based-prompt-calibration-enhancing</guid>
    </item>
    <item>
      <title>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</title>
      <link>https://paperswithcode.com/paper/sora-a-review-on-background-technology</link>
      <description><![CDATA[Sora is a text-to-video generative AI model, released by OpenAI in February 2024.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sora-a-review-on-background-technology</guid>
    </item>
    <item>
      <title>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</title>
      <link>https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using</link>
      <description><![CDATA[It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov9-learning-what-you-want-to-learn-using</guid>
    </item>
    <item>
      <title>Datasets for Large Language Models: A Comprehensive Survey</title>
      <link>https://paperswithcode.com/paper/datasets-for-large-language-models-a</link>
      <description><![CDATA[Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datasets-for-large-language-models-a</guid>
    </item>
    <item>
      <title>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</title>
      <link>https://paperswithcode.com/paper/learning-to-generate-instruction-tuning</link>
      <description><![CDATA[Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-generate-instruction-tuning</guid>
    </item>
    <item>
      <title>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</title>
      <link>https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight</link>
      <description><![CDATA["Bigger the better" has been the predominant trend in recent Large Language Models (LLMs) development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobillama-towards-accurate-and-lightweight</guid>
    </item>
    <item>
      <title>Training-Free Long-Context Scaling of Large Language Models</title>
      <link>https://paperswithcode.com/paper/training-free-long-context-scaling-of-large</link>
      <description><![CDATA[The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-free-long-context-scaling-of-large</guid>
    </item>
    <item>
      <title>BitNet: Scaling 1-bit Transformers for Large Language Models</title>
      <link>https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</link>
      <description><![CDATA[The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</guid>
    </item>
    <item>
      <title>The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA</title>
      <link>https://paperswithcode.com/paper/the-first-place-solution-of-wsdm-cup-2024</link>
      <description><![CDATA[Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-first-place-solution-of-wsdm-cup-2024</guid>
    </item>
    <item>
      <title>UFO: A UI-Focused Agent for Windows OS Interaction</title>
      <link>https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</link>
      <description><![CDATA[We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</guid>
    </item>
    <item>
      <title>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</title>
      <link>https://paperswithcode.com/paper/efficientsam-leveraged-masked-image</link>
      <description><![CDATA[On segment anything task such as zero-shot instance segmentation, our EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably with a significant gain (e. g., ~4 AP on COCO/LVIS) over other fast SAM models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficientsam-leveraged-masked-image</guid>
    </item>
    <item>
      <title>Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing</title>
      <link>https://paperswithcode.com/paper/where-visual-speech-meets-language-vsp-llm</link>
      <description><![CDATA[In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/where-visual-speech-meets-language-vsp-llm</guid>
    </item>
    <item>
      <title>Language Agents as Optimizable Graphs</title>
      <link>https://paperswithcode.com/paper/language-agents-as-optimizable-graphs</link>
      <description><![CDATA[Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-agents-as-optimizable-graphs</guid>
    </item>
    <item>
      <title>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</title>
      <link>https://paperswithcode.com/paper/discrete-diffusion-language-modeling-by</link>
      <description><![CDATA[Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/discrete-diffusion-language-modeling-by</guid>
    </item>
    <item>
      <title>Diffusion Model-Based Image Editing: A Survey</title>
      <link>https://paperswithcode.com/paper/diffusion-model-based-image-editing-a-survey</link>
      <description><![CDATA[In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-model-based-image-editing-a-survey</guid>
    </item>
    <item>
      <title>Neural Network Diffusion</title>
      <link>https://paperswithcode.com/paper/neural-network-diffusion</link>
      <description><![CDATA[The autoencoder extracts latent representations of a subset of the trained network parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-network-diffusion</guid>
    </item>
    <item>
      <title>Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/craftax-a-lightning-fast-benchmark-for-open</link>
      <description><![CDATA[Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/craftax-a-lightning-fast-benchmark-for-open</guid>
    </item>
    <item>
      <title>Scalable Diffusion Models with Transformers</title>
      <link>https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</link>
      <description><![CDATA[We explore a new class of diffusion models based on the transformer architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</guid>
    </item>
    <item>
      <title>OMG-Seg: Is One Model Good Enough For All Segmentation?</title>
      <link>https://paperswithcode.com/paper/omg-seg-is-one-model-good-enough-for-all</link>
      <description><![CDATA[In this work, we address various segmentation tasks, each traditionally tackled by distinct or partially unified models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omg-seg-is-one-model-good-enough-for-all</guid>
    </item>
  </channel>
</rss>
