<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 27 Jul 2022 21:07:24 +0000</lastBuildDate>
    <item>
      <title>Multiface: A Dataset for Neural Face Rendering</title>
      <link>https://paperswithcode.com/paper/multiface-a-dataset-for-neural-face-rendering</link>
      <description><![CDATA[Along with the release of the dataset, we conduct ablation studies on the influence of different model architectures toward the model's interpolation capacity of novel viewpoint and expressions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multiface-a-dataset-for-neural-face-rendering</guid>
    </item>
    <item>
      <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
      <link>https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</link>
      <description><![CDATA[This paper deals with the problem of audio source separation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-scale-multi-band-densenets-for-audio</guid>
    </item>
    <item>
      <title>CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</title>
      <link>https://paperswithcode.com/paper/celebv-hq-a-large-scale-video-facial</link>
      <description><![CDATA[Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/celebv-hq-a-large-scale-video-facial</guid>
    </item>
    <item>
      <title>An Improved One millisecond Mobile Backbone</title>
      <link>https://paperswithcode.com/paper/an-improved-one-millisecond-mobile-backbone</link>
      <description><![CDATA[Furthermore, we show that our model generalizes to multiple tasks - image classification, object detection, and semantic segmentation with significant improvements in latency and accuracy as compared to existing efficient architectures when deployed on a mobile device.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-improved-one-millisecond-mobile-backbone</guid>
    </item>
    <item>
      <title>When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition</title>
      <link>https://paperswithcode.com/paper/when-counting-meets-hmer-counting-aware</link>
      <description><![CDATA[Recently, most handwritten mathematical expression recognition (HMER) methods adopt the encoder-decoder networks, which directly predict the markup sequences from formula images with the attention mechanism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/when-counting-meets-hmer-counting-aware</guid>
    </item>
    <item>
      <title>DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection</title>
      <link>https://paperswithcode.com/paper/deviant-depth-equivariant-network-for</link>
      <description><![CDATA[As a result, DEVIANT is equivariant to the depth translations in the projective manifold whereas vanilla networks are not.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deviant-depth-equivariant-network-for</guid>
    </item>
    <item>
      <title>RePaint: Inpainting using Denoising Diffusion Probabilistic Models</title>
      <link>https://paperswithcode.com/paper/repaint-inpainting-using-denoising-diffusion</link>
      <description><![CDATA[In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repaint-inpainting-using-denoising-diffusion</guid>
    </item>
    <item>
      <title>Neural-Sim: Learning to Generate Training Data with NeRF</title>
      <link>https://paperswithcode.com/paper/neural-sim-learning-to-generate-training-data</link>
      <description><![CDATA[However, existing approaches either require human experts to manually tune each scene property or use automatic methods that provide little to no control; this requires rendering large amounts of random data variations, which is slow and is often suboptimal for the target domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-sim-learning-to-generate-training-data</guid>
    </item>
    <item>
      <title>Generative Multiplane Images: Making a 2D GAN 3D-Aware</title>
      <link>https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</link>
      <description><![CDATA[What is really needed to make an existing 2D GAN 3D-aware?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</guid>
    </item>
    <item>
      <title>Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild</title>
      <link>https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</link>
      <description><![CDATA[Omni3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 97 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</guid>
    </item>
    <item>
      <title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
      <link>https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</link>
      <description><![CDATA[YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56. 8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov7-trainable-bag-of-freebies-sets-new</guid>
    </item>
    <item>
      <title>SpA-Former: Transformer image shadow detection and removal via spatial attention</title>
      <link>https://paperswithcode.com/paper/spa-former-transformer-image-shadow-detection</link>
      <description><![CDATA[In this paper, we propose an end-to-end SpA-Former to recover a shadow-free image from a single shaded image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spa-former-transformer-image-shadow-detection</guid>
    </item>
    <item>
      <title>AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/adanerf-adaptive-sampling-for-real-time</link>
      <description><![CDATA[However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adanerf-adaptive-sampling-for-real-time</guid>
    </item>
    <item>
      <title>In Defense of Online Models for Video Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/in-defense-of-online-models-for-video</link>
      <description><![CDATA[In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-defense-of-online-models-for-video</guid>
    </item>
    <item>
      <title>OCR-free Document Understanding Transformer</title>
      <link>https://paperswithcode.com/paper/donut-document-understanding-transformer</link>
      <description><![CDATA[Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/donut-document-understanding-transformer</guid>
    </item>
    <item>
      <title>Panoptic Scene Graph Generation</title>
      <link>https://paperswithcode.com/paper/panoptic-scene-graph-generation</link>
      <description><![CDATA[Existing research addresses scene graph generation (SGG) -- a critical technology for scene understanding in images -- from a detection perspective, i. e., objects are detected using bounding boxes followed by prediction of their pairwise relationships.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/panoptic-scene-graph-generation</guid>
    </item>
    <item>
      <title>Adversarial Sample Detection for Speaker Verification by Neural Vocoders</title>
      <link>https://paperswithcode.com/paper/spotting-adversarial-samples-for-speaker</link>
      <description><![CDATA[We also show that the neural vocoder adopted in the detection framework is dataset-independent.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spotting-adversarial-samples-for-speaker</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>Dive into Big Model Training</title>
      <link>https://paperswithcode.com/paper/dive-into-big-model-training</link>
      <description><![CDATA[We summarize the existing training methodologies into three main categories: training parallelism, memory-saving technologies, and model sparsity design.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dive-into-big-model-training</guid>
    </item>
    <item>
      <title>Patchwork++: Fast and Robust Ground Segmentation Solving Partial Under-Segmentation Using 3D Point Cloud</title>
      <link>https://paperswithcode.com/paper/patchwork-fast-and-robust-ground-segmentation</link>
      <description><![CDATA[Moreover, even if the parameters are well adjusted, a partial under-segmentation problem can still emerge, which implies ground segmentation failures in some regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/patchwork-fast-and-robust-ground-segmentation</guid>
    </item>
  </channel>
</rss>
