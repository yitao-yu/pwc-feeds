<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 15 Sep 2022 09:17:59 +0000</lastBuildDate>
    <item>
      <title>Diffusion Models: A Comprehensive Survey of Methods and Applications</title>
      <link>https://paperswithcode.com/paper/diffusion-models-a-comprehensive-survey-of</link>
      <description><![CDATA[Diffusion models are a class of deep generative models that have shown impressive results on various tasks with dense theoretical founding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-a-comprehensive-survey-of</guid>
    </item>
    <item>
      <title>CenterFormer: Center-based Transformer for 3D Object Detection</title>
      <link>https://paperswithcode.com/paper/centerformer-center-based-transformer-for-3d</link>
      <description><![CDATA[It then uses the feature of the center candidate as the query embedding in the transformer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/centerformer-center-based-transformer-for-3d</guid>
    </item>
    <item>
      <title>StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation</title>
      <link>https://paperswithcode.com/paper/storydall-e-adapting-pretrained-text-to-image</link>
      <description><![CDATA[Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/storydall-e-adapting-pretrained-text-to-image</guid>
    </item>
    <item>
      <title>TEACH: Temporal Action Composition for 3D Humans</title>
      <link>https://paperswithcode.com/paper/teach-temporal-action-composition-for-3d</link>
      <description><![CDATA[In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/teach-temporal-action-composition-for-3d</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots</title>
      <link>https://paperswithcode.com/paper/genloco-generalized-locomotion-controllers</link>
      <description><![CDATA[In this work, we introduce a framework for training generalized locomotion (GenLoco) controllers for quadrupedal robots.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/genloco-generalized-locomotion-controllers</guid>
    </item>
    <item>
      <title>Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories</title>
      <link>https://paperswithcode.com/paper/particle-videos-revisited-tracking-through</link>
      <description><![CDATA[In this paper, we revisit Sand and Teller's "particle video" approach, and study pixel tracking as a long-range motion estimation problem, where every pixel is described with a trajectory that locates it in multiple future frames.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/particle-videos-revisited-tracking-through</guid>
    </item>
    <item>
      <title>Thin-Plate Spline Motion Model for Image Animation</title>
      <link>https://paperswithcode.com/paper/thin-plate-spline-motion-model-for-image</link>
      <description><![CDATA[Firstly, we propose thin-plate spline motion estimation to produce a more flexible optical flow, which warps the feature maps of the source image to the feature domain of the driving image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/thin-plate-spline-motion-model-for-image</guid>
    </item>
    <item>
      <title>ESFPNet: efficient deep learning architecture for real-time lesion segmentation in autofluorescence bronchoscopic video</title>
      <link>https://paperswithcode.com/paper/esfpnet-efficient-deep-learning-architecture</link>
      <description><![CDATA[These values are superior to results achieved by other competing architectures that use Mix transformers or CNN-based encoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/esfpnet-efficient-deep-learning-architecture</guid>
    </item>
    <item>
      <title>Surface Representation for Point Clouds</title>
      <link>https://paperswithcode.com/paper/surface-representation-for-point-clouds</link>
      <description><![CDATA[Based on a simple baseline of PointNet++ (SSG version), Umbrella RepSurf surpasses the previous state-of-the-art by a large margin for classification, segmentation and detection on various benchmarks in terms of performance and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/surface-representation-for-point-clouds</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
    <item>
      <title>Parameter-Free Style Projection for Arbitrary Style Transfer</title>
      <link>https://paperswithcode.com/paper/parameter-free-style-projection-for-arbitrary</link>
      <description><![CDATA[This paper further presents a real-time feed-forward model to leverage Style Projection for arbitrary image style transfer, which includes a regularization term for matching the semantics between input contents and stylized outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/parameter-free-style-projection-for-arbitrary</guid>
    </item>
    <item>
      <title>CLIP-Mesh: Generating textured meshes from text using pretrained image-text models</title>
      <link>https://paperswithcode.com/paper/text-to-mesh-without-3d-supervision-using</link>
      <description><![CDATA[We present a technique for zero-shot generation of a 3D model using only a target text prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-to-mesh-without-3d-supervision-using</guid>
    </item>
    <item>
      <title>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</title>
      <link>https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</link>
      <description><![CDATA[Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-is-worth-one-word-personalizing-text</guid>
    </item>
    <item>
      <title>FedML: A Research Library and Benchmark for Federated Machine Learning</title>
      <link>https://paperswithcode.com/paper/fedml-a-research-library-and-benchmark-for</link>
      <description><![CDATA[Federated learning (FL) is a rapidly growing research field in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fedml-a-research-library-and-benchmark-for</guid>
    </item>
    <item>
      <title>Neural Architectures for Named Entity Recognition</title>
      <link>https://paperswithcode.com/paper/neural-architectures-for-named-entity</link>
      <description><![CDATA[State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-architectures-for-named-entity</guid>
    </item>
    <item>
      <title>Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models</title>
      <link>https://paperswithcode.com/paper/text-guided-synthesis-of-artistic-images-with</link>
      <description><![CDATA[In RDMs, a set of nearest neighbors is retrieved from an external database during training for each training instance, and the diffusion model is conditioned on these informative samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text-guided-synthesis-of-artistic-images-with</guid>
    </item>
    <item>
      <title>ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech</title>
      <link>https://paperswithcode.com/paper/prodiff-progressive-fast-diffusion-model-for</link>
      <description><![CDATA[Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prodiff-progressive-fast-diffusion-model-for</guid>
    </item>
    <item>
      <title>General Place Recognition Survey: Towards the Real-world Autonomy Age</title>
      <link>https://paperswithcode.com/paper/general-place-recognition-survey-towards-the</link>
      <description><![CDATA[A summary of this work and our datasets and evaluation API is publicly available to the robotics community at: https://github. com/MetaSLAM/GPRS.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-place-recognition-survey-towards-the</guid>
    </item>
    <item>
      <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification</title>
      <link>https://paperswithcode.com/paper/a-gentle-introduction-to-conformal-prediction</link>
      <description><![CDATA[Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-gentle-introduction-to-conformal-prediction</guid>
    </item>
  </channel>
</rss>
