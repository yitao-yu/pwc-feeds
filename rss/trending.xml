<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 30 Apr 2024 09:13:56 +0000</lastBuildDate>
    <item>
      <title>CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</title>
      <link>https://paperswithcode.com/paper/catlip-clip-level-visual-recognition-accuracy</link>
      <description><![CDATA[Contrastive learning has emerged as a transformative method for learning effective visual representations through the alignment of image and text embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/catlip-clip-level-visual-recognition-accuracy</guid>
    </item>
    <item>
      <title>Improving Diffusion Models for Virtual Try-on</title>
      <link>https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</link>
      <description><![CDATA[Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-diffusion-models-for-virtual-try-on</guid>
    </item>
    <item>
      <title>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</title>
      <link>https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</link>
      <description><![CDATA[Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</guid>
    </item>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent</title>
      <link>https://paperswithcode.com/paper/flowmap-high-quality-camera-poses-intrinsics</link>
      <description><![CDATA[This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flowmap-high-quality-camera-poses-intrinsics</guid>
    </item>
    <item>
      <title>Make Your LLM Fully Utilize the Context</title>
      <link>https://paperswithcode.com/paper/make-your-llm-fully-utilize-the-context</link>
      <description><![CDATA[While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/make-your-llm-fully-utilize-the-context</guid>
    </item>
    <item>
      <title>InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models</title>
      <link>https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</link>
      <description><![CDATA[We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantmesh-efficient-3d-mesh-generation-from</guid>
    </item>
    <item>
      <title>QLoRA: Efficient Finetuning of Quantized LLMs</title>
      <link>https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</link>
      <description><![CDATA[Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99. 3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms</guid>
    </item>
    <item>
      <title>ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving</title>
      <link>https://paperswithcode.com/paper/consistentid-portrait-generation-with</link>
      <description><![CDATA[ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/consistentid-portrait-generation-with</guid>
    </item>
    <item>
      <title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title>
      <link>https://paperswithcode.com/paper/id-animator-zero-shot-identity-preserving</link>
      <description><![CDATA[Based on this pipeline, a random face reference training method is further devised to precisely capture the ID-relevant embeddings from reference images, thus improving the fidelity and generalization capacity of our model for ID-specific video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/id-animator-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</title>
      <link>https://paperswithcode.com/paper/seed-x-multimodal-models-with-unified-multi</link>
      <description><![CDATA[We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-x-multimodal-models-with-unified-multi</guid>
    </item>
    <item>
      <title>Learning Visuotactile Skills with Two Multifingered Hands</title>
      <link>https://paperswithcode.com/paper/learning-visuotactile-skills-with-two</link>
      <description><![CDATA[Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-visuotactile-skills-with-two</guid>
    </item>
    <item>
      <title>AgentScope: A Flexible yet Robust Multi-Agent Platform</title>
      <link>https://paperswithcode.com/paper/agentscope-a-flexible-yet-robust-multi-agent</link>
      <description><![CDATA[With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentscope-a-flexible-yet-robust-multi-agent</guid>
    </item>
    <item>
      <title>Rethinking Inductive Biases for Surface Normal Estimation</title>
      <link>https://paperswithcode.com/paper/rethinking-inductive-biases-for-surface</link>
      <description><![CDATA[Despite the growing demand for accurate surface normal estimation models, existing methods use general-purpose dense prediction models, adopting the same inductive biases as other tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-inductive-biases-for-surface</guid>
    </item>
    <item>
      <title>SnapKV: LLM Knows What You are Looking for Before Generation</title>
      <link>https://paperswithcode.com/paper/snapkv-llm-knows-what-you-are-looking-for</link>
      <description><![CDATA[Specifically, SnapKV achieves a consistent decoding speed with a 3. 6x increase in generation speed and an 8. 2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/snapkv-llm-knows-what-you-are-looking-for</guid>
    </item>
    <item>
      <title>Dynamic Generation of Personalities with Large Language Models</title>
      <link>https://paperswithcode.com/paper/dynamic-generation-of-personalities-with</link>
      <description><![CDATA[We propose a new metric to assess personality generation capability based on this evaluation method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dynamic-generation-of-personalities-with</guid>
    </item>
    <item>
      <title>ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation</title>
      <link>https://paperswithcode.com/paper/esrl-efficient-sampling-based-reinforcement</link>
      <description><![CDATA[Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\textit{e. g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/esrl-efficient-sampling-based-reinforcement</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</title>
      <link>https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</link>
      <description><![CDATA[Compared to both open-source and proprietary models, InternVL 1. 5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</guid>
    </item>
    <item>
      <title>List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/list-items-one-by-one-a-new-data-source-and</link>
      <description><![CDATA[Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/list-items-one-by-one-a-new-data-source-and</guid>
    </item>
  </channel>
</rss>
