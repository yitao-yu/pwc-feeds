<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 19 Jun 2025 09:19:23 +0000</lastBuildDate>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning</title>
      <link>https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</link>
      <description><![CDATA[Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</guid>
    </item>
    <item>
      <title>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</title>
      <link>https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</link>
      <description><![CDATA[Drawing inspiration from the orchestration of diverse specialized agents collaborating to tackle intricate tasks, we propose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that harnesses multi-agent collaboration to bolster the faithfulness and causality of foundation models, involving a set of reasoners and evaluators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</guid>
    </item>
    <item>
      <title>MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</title>
      <link>https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</link>
      <description><![CDATA[To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</guid>
    </item>
    <item>
      <title>MUSt3R: Multi-view Network for Stereo 3D Reconstruction</title>
      <link>https://paperswithcode.com/paper/must3r-multi-view-network-for-stereo-3d</link>
      <description><![CDATA[DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/must3r-multi-view-network-for-stereo-3d</guid>
    </item>
    <item>
      <title>VGGT: Visual Geometry Grounded Transformer</title>
      <link>https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</link>
      <description><![CDATA[We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</guid>
    </item>
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://paperswithcode.com/paper/ming-omni-a-unified-multimodal-model-for</link>
      <description><![CDATA[We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ming-omni-a-unified-multimodal-model-for</guid>
    </item>
    <item>
      <title>MagCache: Fast Video Generation with Magnitude-Aware Cache</title>
      <link>https://paperswithcode.com/paper/magcache-fast-video-generation-with-magnitude</link>
      <description><![CDATA[Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magcache-fast-video-generation-with-magnitude</guid>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
      <link>https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</link>
      <description><![CDATA[To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</guid>
    </item>
    <item>
      <title>Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders</title>
      <link>https://paperswithcode.com/paper/efficient-speech-enhancement-via-embeddings</link>
      <description><![CDATA[Recent research has delved into speech enhancement (SE) approaches that leverage audio embeddings from pre-trained models, diverging from time-frequency masking or signal prediction techniques.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-speech-enhancement-via-embeddings</guid>
    </item>
    <item>
      <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</link>
      <description><![CDATA[This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</guid>
    </item>
    <item>
      <title>SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing</title>
      <link>https://paperswithcode.com/paper/surveyforge-on-the-outline-heuristics-memory</link>
      <description><![CDATA[Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/surveyforge-on-the-outline-heuristics-memory</guid>
    </item>
    <item>
      <title>Efficient Part-level 3D Object Generation via Dual Volume Packing</title>
      <link>https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</link>
      <description><![CDATA[Recent progress in 3D object generation has greatly improved both the quality and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</guid>
    </item>
    <item>
      <title>PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service Levels and Prices</title>
      <link>https://paperswithcode.com/paper/pixelsdb-serverless-and-natural-language</link>
      <description><![CDATA[The queries are then executed by a serverless query engine that offers varying prices for different performance service levels (SLAs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixelsdb-serverless-and-natural-language</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention</title>
      <link>https://paperswithcode.com/paper/direct3d-s2-gigascale-3d-generation-made-easy</link>
      <description><![CDATA[Generating high-resolution 3D shapes using volumetric representations such as Signed Distance Functions (SDFs) presents substantial computational and memory challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/direct3d-s2-gigascale-3d-generation-made-easy</guid>
    </item>
    <item>
      <title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
      <link>https://paperswithcode.com/paper/omniaudio-generating-spatial-audio-from-360</link>
      <description><![CDATA[To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omniaudio-generating-spatial-audio-from-360</guid>
    </item>
    <item>
      <title>RFUAV: A Benchmark Dataset for Unmanned Aerial Vehicle Detection and Identification</title>
      <link>https://paperswithcode.com/paper/rfuav-a-benchmark-dataset-for-unmanned-aerial</link>
      <description><![CDATA[In addition to the dataset, RFUAV provides a baseline preprocessing method and model evaluation tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rfuav-a-benchmark-dataset-for-unmanned-aerial</guid>
    </item>
    <item>
      <title>MAGREF: Masked Guidance for Any-Reference Video Generation</title>
      <link>https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</link>
      <description><![CDATA[Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</guid>
    </item>
  </channel>
</rss>
