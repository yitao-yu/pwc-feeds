<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 23 Jun 2025 09:21:11 +0000</lastBuildDate>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>Mirage: A Multi-Level Superoptimizer for Tensor Programs</title>
      <link>https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</link>
      <description><![CDATA[We introduce Mirage, the first multi-level superoptimizer for tensor programs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-multi-level-superoptimizer-for-tensor</guid>
    </item>
    <item>
      <title>LeVo: High-Quality Song Generation with Multi-Preference Alignment</title>
      <link>https://paperswithcode.com/paper/levo-high-quality-song-generation-with-multi</link>
      <description><![CDATA[To further enhance musicality and instruction following, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/levo-high-quality-song-generation-with-multi</guid>
    </item>
    <item>
      <title>Do Large Language Models Need a Content Delivery Network?</title>
      <link>https://paperswithcode.com/paper/do-large-language-models-need-a-content</link>
      <description><![CDATA[As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-large-language-models-need-a-content</guid>
    </item>
    <item>
      <title>PixelsDB: Serverless and NL-Aided Data Analytics with Flexible Service Levels and Prices</title>
      <link>https://paperswithcode.com/paper/pixelsdb-serverless-and-natural-language</link>
      <description><![CDATA[The queries are then executed by a serverless query engine that offers varying prices for different performance service levels (SLAs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixelsdb-serverless-and-natural-language</guid>
    </item>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</title>
      <link>https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</link>
      <description><![CDATA[To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</guid>
    </item>
    <item>
      <title>Visual Causal Scene Refinement for Video Question Answering</title>
      <link>https://paperswithcode.com/paper/visual-causal-scene-refinement-for-video</link>
      <description><![CDATA[Our VCSR involves two essential modules: i) the Question-Guided Refiner (QGR) module, which refines consecutive video frames guided by the question semantics to obtain more representative segment features for causal front-door intervention; ii) the Causal Scene Separator (CSS) module, which discovers a collection of visual causal and non-causal scenes based on the visual-linguistic causal relevance and estimates the causal effect of the scene-separating intervention in a contrastive learning manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-causal-scene-refinement-for-video</guid>
    </item>
    <item>
      <title>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning</title>
      <link>https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</link>
      <description><![CDATA[Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</guid>
    </item>
    <item>
      <title>Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</title>
      <link>https://paperswithcode.com/paper/stream-omni-simultaneous-multimodal</link>
      <description><![CDATA[The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stream-omni-simultaneous-multimodal</guid>
    </item>
    <item>
      <title>Efficient Part-level 3D Object Generation via Dual Volume Packing</title>
      <link>https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</link>
      <description><![CDATA[Recent progress in 3D object generation has greatly improved both the quality and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</guid>
    </item>
    <item>
      <title>AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents</title>
      <link>https://paperswithcode.com/paper/autoagent-a-fully-automated-and-zero-code</link>
      <description><![CDATA[To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoagent-a-fully-automated-and-zero-code</guid>
    </item>
    <item>
      <title>EdgeTAM: On-Device Track Anything Model</title>
      <link>https://paperswithcode.com/paper/edgetam-on-device-track-anything-model</link>
      <description><![CDATA[Given that video segmentation is a dense prediction task, we find preserving the spatial structure of the memories is essential so that the queries are split into global-level and patch-level groups.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/edgetam-on-device-track-anything-model</guid>
    </item>
    <item>
      <title>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</title>
      <link>https://paperswithcode.com/paper/soundmind-rl-incentivized-logic-reasoning-for</link>
      <description><![CDATA[While large language models have shown reasoning capabilities, their application to the audio modality, particularly in large audio-language models (ALMs), remains significantly underdeveloped.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soundmind-rl-incentivized-logic-reasoning-for</guid>
    </item>
    <item>
      <title>MAGREF: Masked Guidance for Any-Reference Video Generation</title>
      <link>https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</link>
      <description><![CDATA[Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</guid>
    </item>
    <item>
      <title>VGGT: Visual Geometry Grounded Transformer</title>
      <link>https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</link>
      <description><![CDATA[We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</guid>
    </item>
    <item>
      <title>VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</title>
      <link>https://paperswithcode.com/paper/vtool-r1-vlms-learn-to-think-with-images-via</link>
      <description><![CDATA[Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vtool-r1-vlms-learn-to-think-with-images-via</guid>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
      <link>https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</link>
      <description><![CDATA[To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
  </channel>
</rss>
