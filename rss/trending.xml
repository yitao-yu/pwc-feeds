<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 15 Jun 2023 09:11:45 +0000</lastBuildDate>
    <item>
      <title>Simple and Controllable Music Generation</title>
      <link>https://paperswithcode.com/paper/simple-and-controllable-music-generation</link>
      <description><![CDATA[We tackle the task of conditional music generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simple-and-controllable-music-generation</guid>
    </item>
    <item>
      <title>AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</title>
      <link>https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</link>
      <description><![CDATA[In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/altclip-altering-the-language-encoder-in-clip</guid>
    </item>
    <item>
      <title>Matting Anything</title>
      <link>https://paperswithcode.com/paper/matting-anything</link>
      <description><![CDATA[In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matting-anything</guid>
    </item>
    <item>
      <title>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</title>
      <link>https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</link>
      <description><![CDATA[Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spqr-a-sparse-quantized-representation-for</guid>
    </item>
    <item>
      <title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</link>
      <description><![CDATA[We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mimic-it-multi-modal-in-context-instruction</guid>
    </item>
    <item>
      <title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title>
      <link>https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</link>
      <description><![CDATA[For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-llama-an-instruction-tuned-audio-visual</guid>
    </item>
    <item>
      <title>Matte Anything: Interactive Natural Image Matting with Segment Anything Models</title>
      <link>https://paperswithcode.com/paper/matte-anything-interactive-natural-image</link>
      <description><![CDATA[We leverage task-specific vision models to enhance the performance of natural image matting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matte-anything-interactive-natural-image</guid>
    </item>
    <item>
      <title>Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models</title>
      <link>https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</link>
      <description><![CDATA[Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-chatgpt-towards-detailed-video</guid>
    </item>
    <item>
      <title>Scene as Occupancy</title>
      <link>https://paperswithcode.com/paper/scene-as-occupancy</link>
      <description><![CDATA[Human driver can easily describe the complex traffic scene by visual system.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scene-as-occupancy</guid>
    </item>
    <item>
      <title>Gorilla: Large Language Model Connected with Massive APIs</title>
      <link>https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</link>
      <description><![CDATA[Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gorilla-large-language-model-connected-with</guid>
    </item>
    <item>
      <title>Recognize Anything: A Strong Image Tagging Model</title>
      <link>https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</link>
      <description><![CDATA[We are releasing the RAM at \url{https://recognize-anything. github. io/} to foster the advancements of large models in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/recognize-anything-a-strong-image-tagging</guid>
    </item>
    <item>
      <title>DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement</title>
      <link>https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</link>
      <description><![CDATA[Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepfilternet-perceptually-motivated-real</guid>
    </item>
    <item>
      <title>HQ-50K: A Large-scale, High-quality Dataset for Image Restoration</title>
      <link>https://paperswithcode.com/paper/hq-50k-a-large-scale-high-quality-dataset-for</link>
      <description><![CDATA[This paper introduces a new large-scale image restoration dataset, called HQ-50K, which contains 50, 000 high-quality images with rich texture details and semantic diversity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hq-50k-a-large-scale-high-quality-dataset-for</guid>
    </item>
    <item>
      <title>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</title>
      <link>https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of</link>
      <description><![CDATA[We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of</guid>
    </item>
    <item>
      <title>LEACE: Perfect linear concept erasure in closed form</title>
      <link>https://paperswithcode.com/paper/leace-perfect-linear-concept-erasure-in</link>
      <description><![CDATA[Concept erasure aims to remove specified features from a representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leace-perfect-linear-concept-erasure-in</guid>
    </item>
    <item>
      <title>Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?</title>
      <link>https://paperswithcode.com/paper/are-random-decompositions-all-we-need-in-high</link>
      <description><![CDATA[Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation (BO) to high-dimensional problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/are-random-decompositions-all-we-need-in-high</guid>
    </item>
    <item>
      <title>HUMAP: Hierarchical Uniform Manifold Approximation and Projection</title>
      <link>https://paperswithcode.com/paper/humap-hierarchical-uniform-manifold</link>
      <description><![CDATA[Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humap-hierarchical-uniform-manifold</guid>
    </item>
    <item>
      <title>CLEAR: A Fully User-side Image Search System</title>
      <link>https://paperswithcode.com/paper/clear-a-fully-user-side-image-search-system</link>
      <description><![CDATA[In this framework, each user builds their own search system that meets their preference with a user-defined scoring function and user-defined interface.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/clear-a-fully-user-side-image-search-system</guid>
    </item>
    <item>
      <title>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
      <link>https://paperswithcode.com/paper/blip-2-bootstrapping-language-image-pre</link>
      <description><![CDATA[The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip-2-bootstrapping-language-image-pre</guid>
    </item>
    <item>
      <title>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</title>
      <link>https://paperswithcode.com/paper/layoutgpt-compositional-visual-planning-and</link>
      <description><![CDATA[When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layoutgpt-compositional-visual-planning-and</guid>
    </item>
  </channel>
</rss>
