<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 17 May 2025 21:08:58 +0000</lastBuildDate>
    <item>
      <title>FastVLM: Efficient Vision Encoding for Vision Language Models</title>
      <link>https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</link>
      <description><![CDATA[At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastvlm-efficient-vision-encoding-for-vision</guid>
    </item>
    <item>
      <title>Continuous Thought Machines</title>
      <link>https://paperswithcode.com/paper/continuous-thought-machines</link>
      <description><![CDATA[The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/continuous-thought-machines</guid>
    </item>
    <item>
      <title>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</title>
      <link>https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</link>
      <description><![CDATA[Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/blip3-o-a-family-of-fully-open-unified</guid>
    </item>
    <item>
      <title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
      <link>https://paperswithcode.com/paper/generating-physically-stable-and-buildable</link>
      <description><![CDATA[Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generating-physically-stable-and-buildable</guid>
    </item>
    <item>
      <title>Flow-GRPO: Training Flow Matching Models via Online RL</title>
      <link>https://paperswithcode.com/paper/flow-grpo-training-flow-matching-models-via</link>
      <description><![CDATA[We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flow-grpo-training-flow-matching-models-via</guid>
    </item>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</link>
      <description><![CDATA[Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/absolute-zero-reinforced-self-play-reasoning</guid>
    </item>
    <item>
      <title>HealthBench: Evaluating Large Language Models Towards Improved Human Health</title>
      <link>https://paperswithcode.com/paper/healthbench-evaluating-large-language-models</link>
      <description><![CDATA[We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/healthbench-evaluating-large-language-models</guid>
    </item>
    <item>
      <title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
      <link>https://paperswithcode.com/paper/perception-reason-think-and-plan-a-survey-on</link>
      <description><![CDATA[Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/perception-reason-think-and-plan-a-survey-on</guid>
    </item>
    <item>
      <title>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</title>
      <link>https://paperswithcode.com/paper/univla-learning-to-act-anywhere-with-task</link>
      <description><![CDATA[Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/univla-learning-to-act-anywhere-with-task</guid>
    </item>
    <item>
      <title>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</title>
      <link>https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</link>
      <description><![CDATA[Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vita-audio-fast-interleaved-cross-modal-token</guid>
    </item>
    <item>
      <title>LTX-Video: Realtime Video Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</link>
      <description><![CDATA[To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ltx-video-realtime-video-latent-diffusion</guid>
    </item>
    <item>
      <title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/openthinkimg-learning-to-think-with-images</link>
      <description><![CDATA[We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openthinkimg-learning-to-think-with-images</guid>
    </item>
    <item>
      <title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
      <link>https://paperswithcode.com/paper/paper2code-automating-code-generation-from</link>
      <description><![CDATA[Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paper2code-automating-code-generation-from</guid>
    </item>
    <item>
      <title>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</title>
      <link>https://paperswithcode.com/paper/nexus-gen-a-unified-model-for-image</link>
      <description><![CDATA[To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nexus-gen-a-unified-model-for-image</guid>
    </item>
    <item>
      <title>Unified Continuous Generative Models</title>
      <link>https://paperswithcode.com/paper/unified-continuous-generative-models</link>
      <description><![CDATA[We introduce a unified framework for training, sampling, and analyzing these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unified-continuous-generative-models</guid>
    </item>
    <item>
      <title>3D Scene Generation: A Survey</title>
      <link>https://paperswithcode.com/paper/3d-scene-generation-a-survey</link>
      <description><![CDATA[Recent advances in deep generative models (e. g., GANs, diffusion models) and 3D representations (e. g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-scene-generation-a-survey</guid>
    </item>
    <item>
      <title>WebThinker: Empowering Large Reasoning Models with Deep Research Capability</title>
      <link>https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</link>
      <description><![CDATA[Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webthinker-empowering-large-reasoning-models</guid>
    </item>
    <item>
      <title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
      <link>https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</link>
      <description><![CDATA[Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s2-a-compositional-generalist</guid>
    </item>
    <item>
      <title>Human-like Episodic Memory for Infinite Context LLMs</title>
      <link>https://paperswithcode.com/paper/human-like-episodic-memory-for-infinite</link>
      <description><![CDATA[Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-like-episodic-memory-for-infinite</guid>
    </item>
    <item>
      <title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
      <link>https://paperswithcode.com/paper/openhelix-a-short-survey-empirical-analysis</link>
      <description><![CDATA[Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openhelix-a-short-survey-empirical-analysis</guid>
    </item>
  </channel>
</rss>
