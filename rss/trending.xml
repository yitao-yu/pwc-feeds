<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 27 Apr 2023 21:06:29 +0000</lastBuildDate>
    <item>
      <title>Track Anything: Segment Anything Meets Videos</title>
      <link>https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</link>
      <description><![CDATA[Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/track-anything-segment-anything-meets-videos</guid>
    </item>
    <item>
      <title>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head</title>
      <link>https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</link>
      <description><![CDATA[In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i. e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audiogpt-understanding-and-generating-speech</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</guid>
    </item>
    <item>
      <title>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</title>
      <link>https://paperswithcode.com/paper/adafactor-adaptive-learning-rates-with</link>
      <description><![CDATA[In several recently proposed stochastic optimization methods (e. g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adafactor-adaptive-learning-rates-with</guid>
    </item>
    <item>
      <title>WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
      <link>https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</link>
      <description><![CDATA[By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wizardlm-empowering-large-language-models-to</guid>
    </item>
    <item>
      <title>Segment Anything in Medical Images</title>
      <link>https://paperswithcode.com/paper/segment-anything-in-medical-images</link>
      <description><![CDATA[Comprehensive experiments on 21 3D segmentation tasks and 9 2D segmentation tasks demonstrate that MedSAM outperforms the default SAM model with an average Dice Similarity Coefficient (DSC) of 22. 5% and 17. 6% on 3D and 2D segmentation tasks, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-in-medical-images</guid>
    </item>
    <item>
      <title>Inpaint Anything: Segment Anything Meets Image Inpainting</title>
      <link>https://paperswithcode.com/paper/inpaint-anything-segment-anything-meets-image</link>
      <description><![CDATA[We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/inpaint-anything-segment-anything-meets-image</guid>
    </item>
    <item>
      <title>Tool Learning with Foundation Models</title>
      <link>https://paperswithcode.com/paper/tool-learning-with-foundation-models</link>
      <description><![CDATA[Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tool-learning-with-foundation-models</guid>
    </item>
    <item>
      <title>Segment Everything Everywhere All at Once</title>
      <link>https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</link>
      <description><![CDATA[Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-everything-everywhere-all-at-once</guid>
    </item>
    <item>
      <title>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</title>
      <link>https://paperswithcode.com/paper/chameleon-plug-and-play-compositional</link>
      <description><![CDATA[Large language models (LLMs) have achieved remarkable progress in various natural language processing tasks with emergent abilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chameleon-plug-and-play-compositional</guid>
    </item>
    <item>
      <title>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</title>
      <link>https://paperswithcode.com/paper/minigpt-4-enhancing-vision-language</link>
      <description><![CDATA[To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minigpt-4-enhancing-vision-language</guid>
    </item>
    <item>
      <title>Phoenix: Democratizing ChatGPT across Languages</title>
      <link>https://paperswithcode.com/paper/phoenix-democratizing-chatgpt-across</link>
      <description><![CDATA[This paper presents our efforts to democratize ChatGPT across language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/phoenix-democratizing-chatgpt-across</guid>
    </item>
    <item>
      <title>DINOv2: Learning Robust Visual Features without Supervision</title>
      <link>https://paperswithcode.com/paper/dinov2-learning-robust-visual-features</link>
      <description><![CDATA[The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dinov2-learning-robust-visual-features</guid>
    </item>
    <item>
      <title>Anything-3D: Towards Single-view Anything Reconstruction in the Wild</title>
      <link>https://paperswithcode.com/paper/anything-3d-towards-single-view-anything</link>
      <description><![CDATA[3D reconstruction from a single-RGB image in unconstrained real-world scenarios presents numerous challenges due to the inherent diversity and complexity of objects and environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anything-3d-towards-single-view-anything</guid>
    </item>
    <item>
      <title>F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories</title>
      <link>https://paperswithcode.com/paper/f-2-nerf-fast-neural-radiance-field-training</link>
      <description><![CDATA[Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/f-2-nerf-fast-neural-radiance-field-training</guid>
    </item>
    <item>
      <title>Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes</title>
      <link>https://paperswithcode.com/paper/language-models-enable-simple-systems-for</link>
      <description><![CDATA[Code synthesis is cheap, but far less accurate than directly processing each document with the LLM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-enable-simple-systems-for</guid>
    </item>
    <item>
      <title>Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness</title>
      <link>https://paperswithcode.com/paper/evaluating-chatgpt-s-information-extraction</link>
      <description><![CDATA[The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-chatgpt-s-information-extraction</guid>
    </item>
    <item>
      <title>SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</title>
      <link>https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</link>
      <description><![CDATA[We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</guid>
    </item>
    <item>
      <title>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</title>
      <link>https://paperswithcode.com/paper/gptq-accurate-post-training-quantization-for</link>
      <description><![CDATA[In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gptq-accurate-post-training-quantization-for</guid>
    </item>
    <item>
      <title>Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/zip-nerf-anti-aliased-grid-based-neural</link>
      <description><![CDATA[Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zip-nerf-anti-aliased-grid-based-neural</guid>
    </item>
  </channel>
</rss>
