<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 03 Sep 2024 21:08:22 +0000</lastBuildDate>
    <item>
      <title>WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling</title>
      <link>https://paperswithcode.com/paper/wavtokenizer-an-efficient-acoustic-discrete</link>
      <description><![CDATA[Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wavtokenizer-an-efficient-acoustic-discrete</guid>
    </item>
    <item>
      <title>Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</title>
      <link>https://paperswithcode.com/paper/eagle-exploring-the-design-space-for</link>
      <description><![CDATA[We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eagle-exploring-the-design-space-for</guid>
    </item>
    <item>
      <title>Sapiens: Foundation for Human Vision Models</title>
      <link>https://paperswithcode.com/paper/sapiens-foundation-for-human-vision-models</link>
      <description><![CDATA[We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sapiens-foundation-for-human-vision-models</guid>
    </item>
    <item>
      <title>Text2SQL is Not Enough: Unifying AI and Databases with TAG</title>
      <link>https://paperswithcode.com/paper/text2sql-is-not-enough-unifying-ai-and</link>
      <description><![CDATA[Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2sql-is-not-enough-unifying-ai-and</guid>
    </item>
    <item>
      <title>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</title>
      <link>https://paperswithcode.com/paper/sam2point-segment-any-3d-as-videos-in-zero</link>
      <description><![CDATA[We introduce SAM2Point, a preliminary exploration adapting Segment Anything Model 2 (SAM 2) for zero-shot and promptable 3D segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sam2point-segment-any-3d-as-videos-in-zero</guid>
    </item>
    <item>
      <title>LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</title>
      <link>https://paperswithcode.com/paper/lmsys-chat-1m-a-large-scale-real-world-llm</link>
      <description><![CDATA[Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lmsys-chat-1m-a-large-scale-real-world-llm</guid>
    </item>
    <item>
      <title>Beyond Alignment: Blind Video Face Restoration via Parsing-Guided Temporal-Coherent Transformer</title>
      <link>https://paperswithcode.com/paper/beyond-alignment-blind-video-face-restoration</link>
      <description><![CDATA[Multiple complex degradations are coupled in low-quality video faces in the real world.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/beyond-alignment-blind-video-face-restoration</guid>
    </item>
    <item>
      <title>OctFusion: Octree-based Diffusion Models for 3D Shape Generation</title>
      <link>https://paperswithcode.com/paper/octfusion-octree-based-diffusion-models-for</link>
      <description><![CDATA[Diffusion models have emerged as a popular method for 3D generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/octfusion-octree-based-diffusion-models-for</guid>
    </item>
    <item>
      <title>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</title>
      <link>https://paperswithcode.com/paper/the-mamba-in-the-llama-distilling-and</link>
      <description><![CDATA[Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-mamba-in-the-llama-distilling-and</guid>
    </item>
    <item>
      <title>Writing in the Margins: Better Inference Pattern for Long Context Retrieval</title>
      <link>https://paperswithcode.com/paper/writing-in-the-margins-better-inference</link>
      <description><![CDATA[In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/writing-in-the-margins-better-inference</guid>
    </item>
    <item>
      <title>Training-Free Activation Sparsity in Large Language Models</title>
      <link>https://paperswithcode.com/paper/training-free-activation-sparsity-in-large</link>
      <description><![CDATA[Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-free-activation-sparsity-in-large</guid>
    </item>
    <item>
      <title>CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer</title>
      <link>https://paperswithcode.com/paper/cogvideox-text-to-video-diffusion-models-with</link>
      <description><![CDATA[To improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogvideox-text-to-video-diffusion-models-with</guid>
    </item>
    <item>
      <title>OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer</title>
      <link>https://paperswithcode.com/paper/omagent-a-multi-modal-agent-framework-for</link>
      <description><![CDATA[Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omagent-a-multi-modal-agent-framework-for</guid>
    </item>
    <item>
      <title>FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance</title>
      <link>https://paperswithcode.com/paper/fancyvideo-towards-dynamic-and-consistent</link>
      <description><![CDATA[Then, TAR refines the correlation matrix between cross-frame textual conditions and latent features along the time dimension.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fancyvideo-towards-dynamic-and-consistent</guid>
    </item>
    <item>
      <title>Automated Design of Agentic Systems</title>
      <link>https://paperswithcode.com/paper/automated-design-of-agentic-systems</link>
      <description><![CDATA[Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e. g. Chain-of-Thought, Self-Reflection, Toolformer).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automated-design-of-agentic-systems</guid>
    </item>
    <item>
      <title>Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/show-o-one-single-transformer-to-unify</link>
      <description><![CDATA[We present a unified transformer, i. e., Show-o, that unifies multimodal understanding and generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-o-one-single-transformer-to-unify</guid>
    </item>
    <item>
      <title>Explainable Person Re-Identification with Attribute-guided Metric Distillation</title>
      <link>https://paperswithcode.com/paper/attrimeter-an-attribute-guided-metric</link>
      <description><![CDATA[In this paper, we propose a post-hoc method, named Attribute-guided Metric Distillation (AMD), to explain existing ReID models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/attrimeter-an-attribute-guided-metric</guid>
    </item>
    <item>
      <title>Law of Vision Representation in MLLMs</title>
      <link>https://paperswithcode.com/paper/law-of-vision-representation-in-mllms</link>
      <description><![CDATA[We present the "Law of Vision Representation" in multimodal large language models (MLLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/law-of-vision-representation-in-mllms</guid>
    </item>
    <item>
      <title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
      <link>https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</link>
      <description><![CDATA[To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamingt2v-consistent-dynamic-and</guid>
    </item>
    <item>
      <title>LLM4CP: Adapting Large Language Models for Channel Prediction</title>
      <link>https://paperswithcode.com/paper/llm4cp-adapting-large-language-models-for</link>
      <description><![CDATA[Channel prediction is an effective approach for reducing the feedback or estimation overhead in massive multi-input multi-output (m-MIMO) systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm4cp-adapting-large-language-models-for</guid>
    </item>
  </channel>
</rss>
