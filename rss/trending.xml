<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 05 Dec 2024 21:09:35 +0000</lastBuildDate>
    <item>
      <title>DeMo: Decoupled Momentum Optimization</title>
      <link>https://paperswithcode.com/paper/demo-decoupled-momentum-optimization</link>
      <description><![CDATA[Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demo-decoupled-momentum-optimization</guid>
    </item>
    <item>
      <title>Scaling Transformers for Low-Bitrate High-Quality Speech Coding</title>
      <link>https://paperswithcode.com/paper/scaling-transformers-for-low-bitrate-high</link>
      <description><![CDATA[The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-transformers-for-low-bitrate-high</guid>
    </item>
    <item>
      <title>MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions</title>
      <link>https://paperswithcode.com/paper/mossformer-pushing-the-performance-limit-of</link>
      <description><![CDATA[To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mossformer-pushing-the-performance-limit-of</guid>
    </item>
    <item>
      <title>Open-Sora Plan: Open-Source Large Video Generation Model</title>
      <link>https://paperswithcode.com/paper/open-sora-plan-open-source-large-video</link>
      <description><![CDATA[We introduce Open-Sora Plan, an open-source project that aims to contribute a large generation model for generating desired high-resolution videos with long durations based on various user inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/open-sora-plan-open-source-large-video</guid>
    </item>
    <item>
      <title>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</title>
      <link>https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</link>
      <description><![CDATA[We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-autoregressive-modeling-scalable-image</guid>
    </item>
    <item>
      <title>SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting</title>
      <link>https://paperswithcode.com/paper/switchlight-co-design-of-physics-driven</link>
      <description><![CDATA[We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/switchlight-co-design-of-physics-driven</guid>
    </item>
    <item>
      <title>Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models</title>
      <link>https://paperswithcode.com/paper/auto-rag-autonomous-retrieval-augmented</link>
      <description><![CDATA[Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/auto-rag-autonomous-retrieval-augmented</guid>
    </item>
    <item>
      <title>Multimodal Whole Slide Foundation Model for Pathology</title>
      <link>https://paperswithcode.com/paper/multimodal-whole-slide-foundation-model-for</link>
      <description><![CDATA[The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-whole-slide-foundation-model-for</guid>
    </item>
    <item>
      <title>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</title>
      <link>https://paperswithcode.com/paper/showui-one-vision-language-action-model-for</link>
      <description><![CDATA[In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/showui-one-vision-language-action-model-for</guid>
    </item>
    <item>
      <title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title>
      <link>https://paperswithcode.com/paper/mooncake-a-kvcache-centric-disaggregated</link>
      <description><![CDATA[Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mooncake-a-kvcache-centric-disaggregated</guid>
    </item>
    <item>
      <title>TextSSR: Diffusion-based Data Synthesis for Scene Text Recognition</title>
      <link>https://paperswithcode.com/paper/textssr-diffusion-based-data-synthesis-for</link>
      <description><![CDATA[Experiments show that models trained on added TextSSR-F data exhibit better accuracy compared to models trained on 4 million existing synthetic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textssr-diffusion-based-data-synthesis-for</guid>
    </item>
    <item>
      <title>Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance</title>
      <link>https://paperswithcode.com/paper/proactive-agent-shifting-llm-agents-from</link>
      <description><![CDATA[The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/proactive-agent-shifting-llm-agents-from</guid>
    </item>
    <item>
      <title>EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation</title>
      <link>https://paperswithcode.com/paper/echomimicv2-towards-striking-simplified-and</link>
      <description><![CDATA[Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/echomimicv2-towards-striking-simplified-and</guid>
    </item>
    <item>
      <title>SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</title>
      <link>https://paperswithcode.com/paper/svtrv2-ctc-beats-encoder-decoder-models-in</link>
      <description><![CDATA[In this paper, we propose SVTRv2, a CTC model that beats leading EDTRs in both accuracy and inference speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/svtrv2-ctc-beats-encoder-decoder-models-in</guid>
    </item>
    <item>
      <title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title>
      <link>https://paperswithcode.com/paper/ominicontrol-minimal-and-universal-control</link>
      <description><![CDATA[In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ominicontrol-minimal-and-universal-control</guid>
    </item>
    <item>
      <title>StableAnimator: High-Quality Identity-Preserving Human Image Animation</title>
      <link>https://paperswithcode.com/paper/stableanimator-high-quality-identity</link>
      <description><![CDATA[During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stableanimator-high-quality-identity</guid>
    </item>
    <item>
      <title>MinerU: An Open-Source Solution for Precise Document Content Extraction</title>
      <link>https://paperswithcode.com/paper/mineru-an-open-source-solution-for-precise</link>
      <description><![CDATA[Document content analysis has been a crucial research area in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mineru-an-open-source-solution-for-precise</guid>
    </item>
    <item>
      <title>MARS: Unleashing the Power of Variance Reduction for Training Large Models</title>
      <link>https://paperswithcode.com/paper/mars-unleashing-the-power-of-variance</link>
      <description><![CDATA[Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mars-unleashing-the-power-of-variance</guid>
    </item>
    <item>
      <title>GraphCast: Learning skillful medium-range global weather forecasting</title>
      <link>https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</link>
      <description><![CDATA[Global medium-range weather forecasting is critical to decision-making across many social and economic domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</guid>
    </item>
    <item>
      <title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title>
      <link>https://paperswithcode.com/paper/samurai-adapting-segment-anything-model-for-1</link>
      <description><![CDATA[The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/samurai-adapting-segment-anything-model-for-1</guid>
    </item>
  </channel>
</rss>
