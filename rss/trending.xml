<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 16 May 2023 21:06:03 +0000</lastBuildDate>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title>
      <link>https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</link>
      <description><![CDATA[We recruit annotators to search for relevant information using our interface and then answer questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</guid>
    </item>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <link>https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</link>
      <description><![CDATA[We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</guid>
    </item>
    <item>
      <title>InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language</title>
      <link>https://paperswithcode.com/paper/internchat-solving-vision-centric-tasks-by</link>
      <description><![CDATA[Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internchat-solving-vision-centric-tasks-by</guid>
    </item>
    <item>
      <title>Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</title>
      <link>https://paperswithcode.com/paper/principle-driven-self-alignment-of-language</link>
      <description><![CDATA[Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/principle-driven-self-alignment-of-language</guid>
    </item>
    <item>
      <title>Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</link>
      <description><![CDATA[A medical image adapted SAM, which we have dubbed Medical SAM Adapter (MSA), shows superior performance on 19 medical image segmentation tasks with various image modalities including CT, MRI, ultrasound image, fundus image, and dermoscopic images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</guid>
    </item>
    <item>
      <title>TidyBot: Personalized Robot Assistance with Large Language Models</title>
      <link>https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</link>
      <description><![CDATA[For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</guid>
    </item>
    <item>
      <title>Active Retrieval Augmented Generation</title>
      <link>https://paperswithcode.com/paper/active-retrieval-augmented-generation</link>
      <description><![CDATA[We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/active-retrieval-augmented-generation</guid>
    </item>
    <item>
      <title>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</link>
      <description><![CDATA[In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</guid>
    </item>
    <item>
      <title>iDisc: Internal Discretization for Monocular Depth Estimation</title>
      <link>https://paperswithcode.com/paper/idisc-internal-discretization-for-monocular</link>
      <description><![CDATA[Our method sets the new state of the art with significant improvements on NYU-Depth v2 and KITTI, outperforming all published methods on the official KITTI benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/idisc-internal-discretization-for-monocular</guid>
    </item>
    <item>
      <title>An Inverse Scaling Law for CLIP Training</title>
      <link>https://paperswithcode.com/paper/an-inverse-scaling-law-for-clip-training</link>
      <description><![CDATA[CLIP, the first foundation model that connects images and text, has enabled many recent breakthroughs in computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-inverse-scaling-law-for-clip-training</guid>
    </item>
    <item>
      <title>Progressive-Hint Prompting Improves Reasoning in Large Language Models</title>
      <link>https://paperswithcode.com/paper/progressive-hint-prompting-improves-reasoning</link>
      <description><![CDATA[The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/progressive-hint-prompting-improves-reasoning</guid>
    </item>
    <item>
      <title>PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
      <link>https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</link>
      <description><![CDATA[Real-world applications have high demands for semantic segmentation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</guid>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <link>https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</link>
      <description><![CDATA[We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans</title>
      <link>https://paperswithcode.com/paper/multimodal-gpt-a-vision-and-language-model</link>
      <description><![CDATA[To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-gpt-a-vision-and-language-model</guid>
    </item>
    <item>
      <title>VideoChat: Chat-Centric Video Understanding</title>
      <link>https://paperswithcode.com/paper/videochat-chat-centric-video-understanding</link>
      <description><![CDATA[In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videochat-chat-centric-video-understanding</guid>
    </item>
    <item>
      <title>Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title>
      <link>https://paperswithcode.com/paper/efficient-and-effective-text-encoding-for</link>
      <description><![CDATA[Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-and-effective-text-encoding-for</guid>
    </item>
    <item>
      <title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</guid>
    </item>
  </channel>
</rss>
