<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 05 Jul 2023 21:07:31 +0000</lastBuildDate>
    <item>
      <title>ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases</title>
      <link>https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</link>
      <description><![CDATA[Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatlaw-open-source-legal-large-language</guid>
    </item>
    <item>
      <title>Faster Segment Anything: Towards Lightweight SAM for Mobile Applications</title>
      <link>https://paperswithcode.com/paper/faster-segment-anything-towards-lightweight</link>
      <description><![CDATA[Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/faster-segment-anything-towards-lightweight</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>Fast Segment Anything</title>
      <link>https://paperswithcode.com/paper/fast-segment-anything</link>
      <description><![CDATA[In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-segment-anything</guid>
    </item>
    <item>
      <title>End-to-end Autonomous Driving: Challenges and Frontiers</title>
      <link>https://paperswithcode.com/paper/end-to-end-autonomous-driving-challenges-and</link>
      <description><![CDATA[The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/end-to-end-autonomous-driving-challenges-and</guid>
    </item>
    <item>
      <title>Enhancing Chat Language Models by Scaling High-quality Instructional Conversations</title>
      <link>https://paperswithcode.com/paper/enhancing-chat-language-models-by-scaling</link>
      <description><![CDATA[Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/enhancing-chat-language-models-by-scaling</guid>
    </item>
    <item>
      <title>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for</link>
      <description><![CDATA[Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mme-a-comprehensive-evaluation-benchmark-for</guid>
    </item>
    <item>
      <title>Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language</title>
      <link>https://paperswithcode.com/paper/towards-language-models-that-can-see-computer</link>
      <description><![CDATA[We propose LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-language-models-that-can-see-computer</guid>
    </item>
    <item>
      <title>Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</title>
      <link>https://paperswithcode.com/paper/drag-your-gan-interactive-point-based</link>
      <description><![CDATA[Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/drag-your-gan-interactive-point-based</guid>
    </item>
    <item>
      <title>Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic</title>
      <link>https://paperswithcode.com/paper/shikra-unleashing-multimodal-llm-s</link>
      <description><![CDATA[Referential dialogue is a superset of various vision-language (VL) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shikra-unleashing-multimodal-llm-s</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>LightGlue: Local Feature Matching at Light Speed</title>
      <link>https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</link>
      <description><![CDATA[We introduce LightGlue, a deep neural network that learns to match local features across images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightglue-local-feature-matching-at-light</guid>
    </item>
    <item>
      <title>PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$</title>
      <link>https://paperswithcode.com/paper/panohead-geometry-aware-3d-full-head</link>
      <description><![CDATA[We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in $360^\circ$ with diverse appearance and detailed geometry using only in-the-wild unstructured images for training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/panohead-geometry-aware-3d-full-head</guid>
    </item>
    <item>
      <title>MotionGPT: Human Motion as a Foreign Language</title>
      <link>https://paperswithcode.com/paper/motiongpt-human-motion-as-a-foreign-language</link>
      <description><![CDATA[Building upon this "motion vocabulary", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/motiongpt-human-motion-as-a-foreign-language</guid>
    </item>
    <item>
      <title>Voice Conversion With Just Nearest Neighbors</title>
      <link>https://paperswithcode.com/paper/voice-conversion-with-just-nearest-neighbors</link>
      <description><![CDATA[Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voice-conversion-with-just-nearest-neighbors</guid>
    </item>
    <item>
      <title>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</title>
      <link>https://paperswithcode.com/paper/leandojo-theorem-proving-with-retrieval</link>
      <description><![CDATA[Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leandojo-theorem-proving-with-retrieval</guid>
    </item>
    <item>
      <title>Inheriting the Wisdom of Predecessors: A Multiplex Cascade Framework for Unified Aspect-based Sentiment Analysis</title>
      <link>https://paperswithcode.com/paper/inheriting-the-wisdom-of-predecessors-a</link>
      <description><![CDATA[So far, aspect-based sentiment analysis (ABSA) has involved with total seven subtasks, in which, however the interactions among them have been left unexplored sufficiently.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/inheriting-the-wisdom-of-predecessors-a</guid>
    </item>
    <item>
      <title>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</title>
      <link>https://paperswithcode.com/paper/rsprompter-learning-to-prompt-for-remote</link>
      <description><![CDATA[Leveraging vast training data (SA-1B), the foundation Segment Anything Model (SAM) proposed by Meta AI Research exhibits remarkable generalization and zero-shot capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rsprompter-learning-to-prompt-for-remote</guid>
    </item>
    <item>
      <title>StickyLand: Breaking the Linear Presentation of Computational Notebooks</title>
      <link>https://paperswithcode.com/paper/stickyland-breaking-the-linear-presentation</link>
      <description><![CDATA[We present StickyLand, a notebook extension for empowering users to freely organize their code in non-linear ways.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stickyland-breaking-the-linear-presentation</guid>
    </item>
    <item>
      <title>Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias</title>
      <link>https://paperswithcode.com/paper/large-language-model-as-attributed-training</link>
      <description><![CDATA[Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\% of the querying cost of ChatGPT associated with the latter.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-model-as-attributed-training</guid>
    </item>
  </channel>
</rss>
