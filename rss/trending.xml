<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 11 Jul 2024 09:15:23 +0000</lastBuildDate>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title>
      <link>https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</link>
      <description><![CDATA[With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</title>
      <link>https://paperswithcode.com/paper/diffusion-forcing-next-token-prediction-meets</link>
      <description><![CDATA[This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-forcing-next-token-prediction-meets</guid>
    </item>
    <item>
      <title>Agentless: Demystifying LLM-based Software Engineering Agents</title>
      <link>https://paperswithcode.com/paper/agentless-demystifying-llm-based-software</link>
      <description><![CDATA[However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentless-demystifying-llm-based-software</guid>
    </item>
    <item>
      <title>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation</title>
      <link>https://paperswithcode.com/paper/be-your-outpainter-mastering-video</link>
      <description><![CDATA[We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/be-your-outpainter-mastering-video</guid>
    </item>
    <item>
      <title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title>
      <link>https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</link>
      <description><![CDATA[In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</guid>
    </item>
    <item>
      <title>Occupancy as Set of Points</title>
      <link>https://paperswithcode.com/paper/occupancy-as-set-of-points</link>
      <description><![CDATA[Owing to the inherent flexibility of the point-based representation, OSP achieves strong performance compared with existing methods and excels in terms of training and inference adaptability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/occupancy-as-set-of-points</guid>
    </item>
    <item>
      <title>Self-Play Preference Optimization for Language Model Alignment</title>
      <link>https://paperswithcode.com/paper/self-play-preference-optimization-for</link>
      <description><![CDATA[Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-play-preference-optimization-for</guid>
    </item>
    <item>
      <title>Scaling Synthetic Data Creation with 1,000,000,000 Personas</title>
      <link>https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</link>
      <description><![CDATA[We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</guid>
    </item>
    <item>
      <title>Isomorphic Pruning for Vision Models</title>
      <link>https://paperswithcode.com/paper/isomorphic-pruning-for-vision-models</link>
      <description><![CDATA[For instance, we improve the accuracy of DeiT-Tiny from 74. 52% to 77. 50% by pruning an off-the-shelf DeiT-Base model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/isomorphic-pruning-for-vision-models</guid>
    </item>
    <item>
      <title>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</title>
      <link>https://paperswithcode.com/paper/learning-to-learn-at-test-time-rnns-with</link>
      <description><![CDATA[We evaluate our instantiations at the scale of 125M to 1. 3B parameters, comparing with a strong Transformer and Mamba, a modern RNN.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-learn-at-test-time-rnns-with</guid>
    </item>
    <item>
      <title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title>
      <link>https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</link>
      <description><![CDATA[In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</guid>
    </item>
    <item>
      <title>FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds</title>
      <link>https://paperswithcode.com/paper/foleycrafter-bring-silent-videos-to-life-with</link>
      <description><![CDATA[Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foleycrafter-bring-silent-videos-to-life-with</guid>
    </item>
    <item>
      <title>Model Predictive Optimized Path Integral Strategies</title>
      <link>https://paperswithcode.com/paper/model-predictive-optimized-path-integral</link>
      <description><![CDATA[We generalize the derivation of model predictive path integral control (MPPI) to allow for a single joint distribution across controls in the control sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/model-predictive-optimized-path-integral</guid>
    </item>
    <item>
      <title>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</link>
      <description><![CDATA[We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</guid>
    </item>
    <item>
      <title>InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</title>
      <link>https://paperswithcode.com/paper/internlm-xcomposer-2-5-a-versatile-large</link>
      <description><![CDATA[This long-context capability allows IXC-2. 5 to excel in tasks requiring extensive input and output contexts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internlm-xcomposer-2-5-a-versatile-large</guid>
    </item>
    <item>
      <title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</title>
      <link>https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</link>
      <description><![CDATA[In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</guid>
    </item>
    <item>
      <title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title>
      <link>https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</link>
      <description><![CDATA[Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</guid>
    </item>
    <item>
      <title>BM25S: Orders of magnitude faster lexical search via eager sparse scoring</title>
      <link>https://paperswithcode.com/paper/bm25s-orders-of-magnitude-faster-lexical</link>
      <description><![CDATA[We introduce BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy and Scipy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bm25s-orders-of-magnitude-faster-lexical</guid>
    </item>
  </channel>
</rss>
