<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 21 May 2023 09:10:33 +0000</lastBuildDate>
    <item>
      <title>Sparks of Artificial General Intelligence: Early experiments with GPT-4</title>
      <link>https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence</link>
      <description><![CDATA[We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence</guid>
    </item>
    <item>
      <title>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</title>
      <link>https://paperswithcode.com/paper/visionllm-large-language-model-is-also-an</link>
      <description><![CDATA[We hope this model can set a new baseline for generalist vision and language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visionllm-large-language-model-is-also-an</guid>
    </item>
    <item>
      <title>FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention</title>
      <link>https://paperswithcode.com/paper/fastcomposer-tuning-free-multi-subject-image</link>
      <description><![CDATA[FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fastcomposer-tuning-free-multi-subject-image</guid>
    </item>
    <item>
      <title>SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</title>
      <link>https://paperswithcode.com/paper/speechgpt-empowering-large-language-models</link>
      <description><![CDATA[Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/speechgpt-empowering-large-language-models</guid>
    </item>
    <item>
      <title>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</link>
      <description><![CDATA[To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</guid>
    </item>
    <item>
      <title>ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities</title>
      <link>https://paperswithcode.com/paper/one-peace-exploring-one-general</link>
      <description><![CDATA[In this work, we explore a scalable way for building a general representation model toward unlimited modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-peace-exploring-one-general</guid>
    </item>
    <item>
      <title>Going Denser with Open-Vocabulary Part Segmentation</title>
      <link>https://paperswithcode.com/paper/going-denser-with-open-vocabulary-part</link>
      <description><![CDATA[In this paper, we propose a detector with the ability to predict both open-vocabulary objects and their part segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/going-denser-with-open-vocabulary-part</guid>
    </item>
    <item>
      <title>Reflexion: an autonomous agent with dynamic memory and self-reflection</title>
      <link>https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</link>
      <description><![CDATA[To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reflexion-an-autonomous-agent-with-dynamic</guid>
    </item>
    <item>
      <title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title>
      <link>https://paperswithcode.com/paper/decentralization-and-acceleration-enables</link>
      <description><![CDATA[In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decentralization-and-acceleration-enables</guid>
    </item>
    <item>
      <title>Listen, Think, and Understand</title>
      <link>https://paperswithcode.com/paper/listen-think-and-understand</link>
      <description><![CDATA[In this paper, we propose a novel audio foundation model, called LTU (Listen, Think, and Understand).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/listen-think-and-understand</guid>
    </item>
    <item>
      <title>StructGPT: A General Framework for Large Language Model to Reason over Structured Data</title>
      <link>https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</link>
      <description><![CDATA[Specially, we propose an \emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structgpt-a-general-framework-for-large</guid>
    </item>
    <item>
      <title>Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability</title>
      <link>https://paperswithcode.com/paper/seeing-is-believing-brain-inspired-modular</link>
      <description><![CDATA[We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seeing-is-believing-brain-inspired-modular</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>EasySpider: A No-Code Visual System for Crawling the Web</title>
      <link>https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</link>
      <description><![CDATA[As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyspider-a-no-code-visual-system-for</guid>
    </item>
    <item>
      <title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title>
      <link>https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</link>
      <description><![CDATA[We recruit annotators to search for relevant information using our interface and then answer questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</guid>
    </item>
    <item>
      <title>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</title>
      <link>https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</link>
      <description><![CDATA[We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</guid>
    </item>
    <item>
      <title>HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</title>
      <link>https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</link>
      <description><![CDATA[To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humanrf-high-fidelity-neural-radiance-fields</guid>
    </item>
    <item>
      <title>SoundStorm: Efficient Parallel Audio Generation</title>
      <link>https://paperswithcode.com/paper/soundstorm-efficient-parallel-audio</link>
      <description><![CDATA[We present SoundStorm, a model for efficient, non-autoregressive audio generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soundstorm-efficient-parallel-audio</guid>
    </item>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback</title>
      <link>https://paperswithcode.com/paper/improving-language-model-negotiation-with</link>
      <description><![CDATA[We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-language-model-negotiation-with</guid>
    </item>
  </channel>
</rss>
