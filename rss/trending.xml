<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 04 Dec 2023 21:06:13 +0000</lastBuildDate>
    <item>
      <title>MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</title>
      <link>https://paperswithcode.com/paper/meditron-70b-scaling-medical-pretraining-for</link>
      <description><![CDATA[Large language models (LLMs) can potentially democratize access to medical knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meditron-70b-scaling-medical-pretraining-for</guid>
    </item>
    <item>
      <title>TaskWeaver: A Code-First Agent Framework</title>
      <link>https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</link>
      <description><![CDATA[TaskWeaver provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, and leverages LLM coding capabilities for complex logic.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taskweaver-a-code-first-agent-framework</guid>
    </item>
    <item>
      <title>Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</title>
      <link>https://paperswithcode.com/paper/improving-sample-quality-of-diffusion-model</link>
      <description><![CDATA[Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-sample-quality-of-diffusion-model</guid>
    </item>
    <item>
      <title>GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation</title>
      <link>https://paperswithcode.com/paper/geodream-disentangling-2d-and-geometric</link>
      <description><![CDATA[We justify that the refined 3D geometric priors aid in the 3D-aware capability of 2D diffusion priors, which in turn provides superior guidance for the refinement of 3D geometric priors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/geodream-disentangling-2d-and-geometric</guid>
    </item>
    <item>
      <title>On Bringing Robots Home</title>
      <link>https://paperswithcode.com/paper/on-bringing-robots-home</link>
      <description><![CDATA[We use the Stick to collect 13 hours of data in 22 homes of New York City, and train Home Pretrained Representations (HPR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-bringing-robots-home</guid>
    </item>
    <item>
      <title>SeamlessM4T: Massively Multilingual &amp; Multimodal Machine Translation</title>
      <link>https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</link>
      <description><![CDATA[What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</guid>
    </item>
    <item>
      <title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</title>
      <link>https://paperswithcode.com/paper/lightgaussian-unbounded-3d-gaussian</link>
      <description><![CDATA[Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightgaussian-unbounded-3d-gaussian</guid>
    </item>
    <item>
      <title>Qwen Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen-technical-report</link>
      <description><![CDATA[Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-technical-report</guid>
    </item>
    <item>
      <title>Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models</title>
      <link>https://paperswithcode.com/paper/qwen-audio-advancing-universal-audio</link>
      <description><![CDATA[Recently, instruction-following audio-language models have received broad attention for audio interaction with humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen-audio-advancing-universal-audio</guid>
    </item>
    <item>
      <title>YUAN 2.0: A Large Language Model with Localized Filtering-based Attention</title>
      <link>https://paperswithcode.com/paper/yuan-2-0-a-large-language-model-with</link>
      <description><![CDATA[In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yuan-2-0-a-large-language-model-with</guid>
    </item>
    <item>
      <title>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</title>
      <link>https://paperswithcode.com/paper/synctalk-the-devil-is-in-the-synchronization</link>
      <description><![CDATA[A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/synctalk-the-devil-is-in-the-synchronization</guid>
    </item>
    <item>
      <title>OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/occworld-learning-a-3d-occupancy-world-model</link>
      <description><![CDATA[In this paper, we explore a new framework of learning a world model, OccWorld, in the 3D Occupancy space to simultaneously predict the movement of the ego car and the evolution of the surrounding scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/occworld-learning-a-3d-occupancy-world-model</guid>
    </item>
    <item>
      <title>Initializing Models with Larger Ones</title>
      <link>https://paperswithcode.com/paper/initializing-models-with-larger-ones</link>
      <description><![CDATA[Weight selection offers a new approach to leverage the power of pretrained models in resource-constrained settings, and we hope it can be a useful tool for training small models in the large-model era.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/initializing-models-with-larger-ones</guid>
    </item>
    <item>
      <title>UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition</title>
      <link>https://paperswithcode.com/paper/unireplknet-a-universal-perception-large</link>
      <description><![CDATA[1) We propose four architectural guidelines for designing large-kernel ConvNets, the core of which is to exploit the essential characteristics of large kernels that distinguish them from small kernels - they can see wide without going deep.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unireplknet-a-universal-perception-large</guid>
    </item>
    <item>
      <title>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</title>
      <link>https://paperswithcode.com/paper/llama-vid-an-image-is-worth-2-tokens-in-large</link>
      <description><![CDATA[Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-vid-an-image-is-worth-2-tokens-in-large</guid>
    </item>
    <item>
      <title>Adversarial Diffusion Distillation</title>
      <link>https://paperswithcode.com/paper/adversarial-diffusion-distillation</link>
      <description><![CDATA[We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1-4 steps while maintaining high image quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adversarial-diffusion-distillation</guid>
    </item>
    <item>
      <title>GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?</title>
      <link>https://paperswithcode.com/paper/gpt4vis-what-can-gpt-4-do-for-zero-shot</link>
      <description><![CDATA[Our study centers on the evaluation of GPT-4's linguistic and visual capabilities in zero-shot visual recognition tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt4vis-what-can-gpt-4-do-for-zero-shot</guid>
    </item>
    <item>
      <title>RETVec: Resilient and Efficient Text Vectorizer</title>
      <link>https://paperswithcode.com/paper/retvec-resilient-and-efficient-text</link>
      <description><![CDATA[The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retvec-resilient-and-efficient-text</guid>
    </item>
    <item>
      <title>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</title>
      <link>https://paperswithcode.com/paper/animatable-gaussians-learning-pose-dependent</link>
      <description><![CDATA[Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatable-gaussians-learning-pose-dependent</guid>
    </item>
    <item>
      <title>Instruction Tuning with Human Curriculum</title>
      <link>https://paperswithcode.com/paper/instruction-tuning-with-human-curriculum</link>
      <description><![CDATA[The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instruction-tuning-with-human-curriculum</guid>
    </item>
  </channel>
</rss>
