<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 19 Jul 2024 09:14:15 +0000</lastBuildDate>
    <item>
      <title>Qwen2-Audio Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-audio-technical-report</link>
      <description><![CDATA[We introduce the latest progress of Qwen-Audio, a large-scale audio-language model called Qwen2-Audio, which is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-audio-technical-report</guid>
    </item>
    <item>
      <title>IMAGDressing-v1: Customizable Virtual Dressing</title>
      <link>https://paperswithcode.com/paper/imagdressing-v1-customizable-virtual-dressing</link>
      <description><![CDATA[Latest advances have achieved realistic virtual try-on (VTON) through localized garment inpainting using latent diffusion models, significantly enhancing consumers' online shopping experience.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagdressing-v1-customizable-virtual-dressing</guid>
    </item>
    <item>
      <title>SEED-Story: Multimodal Long Story Generation with Large Language Model</title>
      <link>https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</link>
      <description><![CDATA[We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>Scaling Diffusion Transformers to 16 Billion Parameters</title>
      <link>https://paperswithcode.com/paper/scaling-diffusion-transformers-to-16-billion</link>
      <description><![CDATA[In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-diffusion-transformers-to-16-billion</guid>
    </item>
    <item>
      <title>FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title>
      <link>https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</link>
      <description><![CDATA[This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</guid>
    </item>
    <item>
      <title>Cradle: Empowering Foundation Agents Towards General Computer Control</title>
      <link>https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</link>
      <description><![CDATA[To handle this issue, we propose the General Computer Control (GCC) setting to restrict foundation agents to interact with software through the most unified and standardized interface, i. e., using screenshots as input and keyboard and mouse actions as output.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</guid>
    </item>
    <item>
      <title>GIM: A Million-scale Benchmark for Generative Image Manipulation Detection and Localization</title>
      <link>https://paperswithcode.com/paper/gim-a-million-scale-benchmark-for-generative</link>
      <description><![CDATA[The extraordinary ability of generative models emerges as a new trend in image editing and generating realistic images, posing a serious threat to the trustworthiness of multimedia data and driving the research of image manipulation detection and location(IMDL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gim-a-million-scale-benchmark-for-generative</guid>
    </item>
    <item>
      <title>Grounding Image Matching in 3D with MASt3R</title>
      <link>https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</link>
      <description><![CDATA[Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</guid>
    </item>
    <item>
      <title>MambaVision: A Hybrid Mamba-Transformer Vision Backbone</title>
      <link>https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</link>
      <description><![CDATA[We propose a novel hybrid Mamba-Transformer backbone, denoted as MambaVision, which is specifically tailored for vision applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</guid>
    </item>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>GRUtopia: Dream General Robots in a City at Scale</title>
      <link>https://paperswithcode.com/paper/grutopia-dream-general-robots-in-a-city-at</link>
      <description><![CDATA[Recent works have been exploring the scaling laws in the field of Embodied AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grutopia-dream-general-robots-in-a-city-at</guid>
    </item>
    <item>
      <title>VISA: Reasoning Video Object Segmentation via Large Language Models</title>
      <link>https://paperswithcode.com/paper/visa-reasoning-video-object-segmentation-via</link>
      <description><![CDATA[In this paper, we introduce a new task, Reasoning Video Object Segmentation (ReasonVOS).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visa-reasoning-video-object-segmentation-via</guid>
    </item>
    <item>
      <title>Deep-TEMPEST: Using Deep Learning to Eavesdrop on HDMI from its Unintended Electromagnetic Emanations</title>
      <link>https://paperswithcode.com/paper/deep-tempest-using-deep-learning-to-eavesdrop</link>
      <description><![CDATA[As a result, eavesdropping systems designed for the analog case obtain unclear and difficult-to-read images when applied to digital video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-tempest-using-deep-learning-to-eavesdrop</guid>
    </item>
    <item>
      <title>E5-V: Universal Embeddings with Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/e5-v-universal-embeddings-with-multimodal</link>
      <description><![CDATA[We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/e5-v-universal-embeddings-with-multimodal</guid>
    </item>
    <item>
      <title>Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</title>
      <link>https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</link>
      <description><![CDATA[The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers</title>
      <link>https://paperswithcode.com/paper/hydra-bidirectional-state-space-models</link>
      <description><![CDATA[We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hydra-bidirectional-state-space-models</guid>
    </item>
    <item>
      <title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title>
      <link>https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</link>
      <description><![CDATA[With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</guid>
    </item>
    <item>
      <title>A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights</title>
      <link>https://paperswithcode.com/paper/a-comprehensive-survey-on-human-video</link>
      <description><![CDATA[The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-comprehensive-survey-on-human-video</guid>
    </item>
  </channel>
</rss>
