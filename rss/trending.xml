<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 04 Jul 2024 09:14:45 +0000</lastBuildDate>
    <item>
      <title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</link>
      <description><![CDATA[We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</guid>
    </item>
    <item>
      <title>ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning</title>
      <link>https://paperswithcode.com/paper/exvideo-extending-video-diffusion-models-via</link>
      <description><![CDATA[To evaluate the efficacy of our proposed post-tuning approach, we conduct extension training on the Stable Video Diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exvideo-extending-video-diffusion-models-via</guid>
    </item>
    <item>
      <title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title>
      <link>https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</link>
      <description><![CDATA[Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</guid>
    </item>
    <item>
      <title>EvTexture: Event-driven Texture Enhancement for Video Super-Resolution</title>
      <link>https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</link>
      <description><![CDATA[Our method, called EvTexture, leverages high-frequency details of events to better recover texture regions in VSR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</guid>
    </item>
    <item>
      <title>Self-Play Preference Optimization for Language Model Alignment</title>
      <link>https://paperswithcode.com/paper/self-play-preference-optimization-for</link>
      <description><![CDATA[Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-play-preference-optimization-for</guid>
    </item>
    <item>
      <title>On Scaling Up 3D Gaussian Splatting Training</title>
      <link>https://paperswithcode.com/paper/on-scaling-up-3d-gaussian-splatting-training</link>
      <description><![CDATA[Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-scaling-up-3d-gaussian-splatting-training</guid>
    </item>
    <item>
      <title>MegActor: Harness the Power of Raw Video for Vivid Portrait Animation</title>
      <link>https://paperswithcode.com/paper/megactor-harness-the-power-of-raw-video-for</link>
      <description><![CDATA[Despite raw driving videos contain richer information on facial expressions than intermediate representations such as landmarks in the field of portrait animation, they are seldom the subject of research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/megactor-harness-the-power-of-raw-video-for</guid>
    </item>
    <item>
      <title>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</title>
      <link>https://paperswithcode.com/paper/4m-21-an-any-to-any-vision-model-for-tens-of</link>
      <description><![CDATA[In this paper, we expand upon the capabilities of them by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/4m-21-an-any-to-any-vision-model-for-tens-of</guid>
    </item>
    <item>
      <title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title>
      <link>https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</link>
      <description><![CDATA[In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</guid>
    </item>
    <item>
      <title>HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale</title>
      <link>https://paperswithcode.com/paper/huatuogpt-vision-towards-injecting-medical</link>
      <description><![CDATA[The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuogpt-vision-towards-injecting-medical</guid>
    </item>
    <item>
      <title>Meta Learning Text-to-Speech Synthesis in over 7000 Languages</title>
      <link>https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</link>
      <description><![CDATA[In this work, we take on the challenging task of building a single text-to-speech synthesis system that is capable of generating speech in over 7000 languages, many of which lack sufficient data for traditional TTS development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-learning-text-to-speech-synthesis-in</guid>
    </item>
    <item>
      <title>DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</title>
      <link>https://paperswithcode.com/paper/dsp-dynamic-sequence-parallelism-for-multi</link>
      <description><![CDATA[Scaling multi-dimensional transformers to long sequences is indispensable across various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dsp-dynamic-sequence-parallelism-for-multi</guid>
    </item>
    <item>
      <title>Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs</title>
      <link>https://paperswithcode.com/paper/step-dpo-step-wise-preference-optimization</link>
      <description><![CDATA[Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/step-dpo-step-wise-preference-optimization</guid>
    </item>
    <item>
      <title>TextGrad: Automatic "Differentiation" via Text</title>
      <link>https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</link>
      <description><![CDATA[Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</guid>
    </item>
    <item>
      <title>Depth Anything V2</title>
      <link>https://paperswithcode.com/paper/depth-anything-v2</link>
      <description><![CDATA[This work presents Depth Anything V2.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-v2</guid>
    </item>
    <item>
      <title>Scalable MatMul-free Language Modeling</title>
      <link>https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</link>
      <description><![CDATA[Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2. 7B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</guid>
    </item>
    <item>
      <title>Adam-mini: Use Fewer Learning Rates To Gain More</title>
      <link>https://paperswithcode.com/paper/adam-mini-use-fewer-learning-rates-to-gain</link>
      <description><![CDATA[We find that $\geq$ 90% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our proposed principle on Hessian structure; (2) assign a single but good learning rate to each parameter block.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adam-mini-use-fewer-learning-rates-to-gain</guid>
    </item>
    <item>
      <title>Efficient World Models with Context-Aware Tokenization</title>
      <link>https://paperswithcode.com/paper/efficient-world-models-with-context-aware</link>
      <description><![CDATA[Scaling up deep Reinforcement Learning (RL) methods presents a significant challenge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-world-models-with-context-aware</guid>
    </item>
    <item>
      <title>Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text</title>
      <link>https://paperswithcode.com/paper/director3d-real-world-camera-trajectory-and</link>
      <description><![CDATA[To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the Cinematographer, to model the distribution of camera trajectories based on textual descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/director3d-real-world-camera-trajectory-and</guid>
    </item>
    <item>
      <title>Long Context Transfer from Language to Vision</title>
      <link>https://paperswithcode.com/paper/long-context-transfer-from-language-to-vision</link>
      <description><![CDATA[By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/long-context-transfer-from-language-to-vision</guid>
    </item>
  </channel>
</rss>
