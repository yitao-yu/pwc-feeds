<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 15 Sep 2023 21:06:04 +0000</lastBuildDate>
    <item>
      <title>InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/instaflow-one-step-is-enough-for-high-quality</link>
      <description><![CDATA[Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of $23. 3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37. 2$ $\rightarrow$ $23. 3$ in FID).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instaflow-one-step-is-enough-for-high-quality</guid>
    </item>
    <item>
      <title>DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior</title>
      <link>https://paperswithcode.com/paper/diffbir-towards-blind-image-restoration-with</link>
      <description><![CDATA[We present DiffBIR, which leverages pretrained text-to-image diffusion models for blind image restoration problem.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffbir-towards-blind-image-restoration-with</guid>
    </item>
    <item>
      <title>Break-A-Scene: Extracting Multiple Concepts from a Single Image</title>
      <link>https://paperswithcode.com/paper/break-a-scene-extracting-multiple-concepts</link>
      <description><![CDATA[Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/break-a-scene-extracting-multiple-concepts</guid>
    </item>
    <item>
      <title>Communicative Agents for Software Development</title>
      <link>https://paperswithcode.com/paper/communicative-agents-for-software-development</link>
      <description><![CDATA[At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/communicative-agents-for-software-development</guid>
    </item>
    <item>
      <title>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents</title>
      <link>https://paperswithcode.com/paper/agentverse-facilitating-multi-agent</link>
      <description><![CDATA[Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentverse-facilitating-multi-agent</guid>
    </item>
    <item>
      <title>Nougat: Neural Optical Understanding for Academic Documents</title>
      <link>https://paperswithcode.com/paper/nougat-neural-optical-understanding-for</link>
      <description><![CDATA[Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nougat-neural-optical-understanding-for</guid>
    </item>
    <item>
      <title>Tracking Anything with Decoupled Video Segmentation</title>
      <link>https://paperswithcode.com/paper/tracking-anything-with-decoupled-video</link>
      <description><![CDATA[To 'track anything' without training on video data for every individual task, we develop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tracking-anything-with-decoupled-video</guid>
    </item>
    <item>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
      <link>https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</link>
      <description><![CDATA[Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</guid>
    </item>
    <item>
      <title>PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips</title>
      <link>https://paperswithcode.com/paper/2309-03685</link>
      <description><![CDATA[In some data-sensitive fields such as education or medicine, access to public datasets is even more limited.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2309-03685</guid>
    </item>
    <item>
      <title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title>
      <link>https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</link>
      <description><![CDATA[With the advance of text-to-image models (e. g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</guid>
    </item>
    <item>
      <title>Assessing Neural Network Representations During Training Using Data Diffusion Spectra</title>
      <link>https://paperswithcode.com/paper/assessing-neural-network-representations</link>
      <description><![CDATA[We also see that there is an increase in DSMI with the class label over time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assessing-neural-network-representations</guid>
    </item>
    <item>
      <title>Evaluating Explanation Methods for Multivariate Time Series Classification</title>
      <link>https://paperswithcode.com/paper/evaluating-explanation-methods-for</link>
      <description><![CDATA[In many applications classification alone is not enough, we often need to classify but also understand what the model learns (e. g., why was a prediction given, based on what information in the data).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-explanation-methods-for</guid>
    </item>
    <item>
      <title>GPT Can Solve Mathematical Problems Without a Calculator</title>
      <link>https://paperswithcode.com/paper/gpt-can-solve-mathematical-problems-without-a</link>
      <description><![CDATA[Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-can-solve-mathematical-problems-without-a</guid>
    </item>
    <item>
      <title>Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications</title>
      <link>https://paperswithcode.com/paper/kani-a-lightweight-and-highly-hackable</link>
      <description><![CDATA[Language model applications are becoming increasingly popular and complex, often including features like tool usage and retrieval augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kani-a-lightweight-and-highly-hackable</guid>
    </item>
    <item>
      <title>Cognitive Architectures for Language Agents</title>
      <link>https://paperswithcode.com/paper/cognitive-architectures-for-language-agents</link>
      <description><![CDATA[Recent efforts have incorporated large language models (LLMs) with external resources (e. g., the Internet) or internal control flows (e. g., prompt chaining) for tasks requiring grounding or reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cognitive-architectures-for-language-agents</guid>
    </item>
    <item>
      <title>CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X</title>
      <link>https://paperswithcode.com/paper/codegeex-a-pre-trained-model-for-code</link>
      <description><![CDATA[Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codegeex-a-pre-trained-model-for-code</guid>
    </item>
    <item>
      <title>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</title>
      <link>https://paperswithcode.com/paper/relay-diffusion-unifying-diffusion-process-1</link>
      <description><![CDATA[Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/relay-diffusion-unifying-diffusion-process-1</guid>
    </item>
    <item>
      <title>Extending Context Window of Large Language Models via Positional Interpolation</title>
      <link>https://paperswithcode.com/paper/extending-context-window-of-large-language</link>
      <description><![CDATA[We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/extending-context-window-of-large-language</guid>
    </item>
    <item>
      <title>EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/efficientvit-enhanced-linear-attention-for</link>
      <description><![CDATA[Unlike prior semantic segmentation models that rely on heavy self-attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our lightweight multi-scale attention achieves a global receptive field and multi-scale learning (two critical features for semantic segmentation models) with only lightweight and hardware-efficient operations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficientvit-enhanced-linear-attention-for</guid>
    </item>
    <item>
      <title>FaceChain: A Playground for Identity-Preserving Portrait Generation</title>
      <link>https://paperswithcode.com/paper/facechain-a-playground-for-identity</link>
      <description><![CDATA[In this paper, we present FaceChain, a personalized portrait generation framework that combines a series of customized image-generation model and a rich set of face-related perceptual understanding models (\eg, face detection, deep face embedding extraction, and facial attribute recognition), to tackle aforementioned challenges and to generate truthful personalized portraits, with only a handful of portrait images as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/facechain-a-playground-for-identity</guid>
    </item>
  </channel>
</rss>
