<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 30 Dec 2024 21:08:35 +0000</lastBuildDate>
    <item>
      <title>KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation</title>
      <link>https://paperswithcode.com/paper/2409-13731</link>
      <description><![CDATA[The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2409-13731</guid>
    </item>
    <item>
      <title>CogAgent: A Visual Language Model for GUI Agents</title>
      <link>https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui</link>
      <description><![CDATA[People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e. g., computer or smartphone screens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui</guid>
    </item>
    <item>
      <title>Automating the Search for Artificial Life with Foundation Models</title>
      <link>https://paperswithcode.com/paper/automating-the-search-for-artificial-life</link>
      <description><![CDATA[With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automating-the-search-for-artificial-life</guid>
    </item>
    <item>
      <title>The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning</title>
      <link>https://paperswithcode.com/paper/the-well-a-large-scale-collection-of-diverse</link>
      <description><![CDATA[Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-well-a-large-scale-collection-of-diverse</guid>
    </item>
    <item>
      <title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title>
      <link>https://paperswithcode.com/paper/ditctrl-exploring-attention-control-in-multi</link>
      <description><![CDATA[Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ditctrl-exploring-attention-control-in-multi</guid>
    </item>
    <item>
      <title>OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/openemma-open-source-multimodal-model-for-end</link>
      <description><![CDATA[Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openemma-open-source-multimodal-model-for-end</guid>
    </item>
    <item>
      <title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title>
      <link>https://paperswithcode.com/paper/drt-o1-optimized-deep-reasoning-translation</link>
      <description><![CDATA[To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/drt-o1-optimized-deep-reasoning-translation</guid>
    </item>
    <item>
      <title>Large Concept Models: Language Modeling in a Sentence Representation Space</title>
      <link>https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</link>
      <description><![CDATA[In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-concept-models-language-modeling-in-a</guid>
    </item>
    <item>
      <title>Learning Flow Fields in Attention for Controllable Person Image Generation</title>
      <link>https://paperswithcode.com/paper/learning-flow-fields-in-attention-for</link>
      <description><![CDATA[Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-flow-fields-in-attention-for</guid>
    </item>
    <item>
      <title>VidTwin: Video VAE with Decoupled Structure and Dynamics</title>
      <link>https://paperswithcode.com/paper/vidtwin-video-vae-with-decoupled-structure</link>
      <description><![CDATA[Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vidtwin-video-vae-with-decoupled-structure</guid>
    </item>
    <item>
      <title>CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</title>
      <link>https://paperswithcode.com/paper/clear-conv-like-linearization-revs-pre</link>
      <description><![CDATA[Diffusion Transformers (DiT) have become a leading architecture in image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/clear-conv-like-linearization-revs-pre</guid>
    </item>
    <item>
      <title>Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders</title>
      <link>https://paperswithcode.com/paper/dora-sampling-and-benchmarking-for-3d-shape-1</link>
      <description><![CDATA[However, the widely adopted uniform point sampling strategy in Shape VAE training often leads to a significant loss of geometric details, limiting the quality of shape reconstruction and downstream generation tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dora-sampling-and-benchmarking-for-3d-shape-1</guid>
    </item>
    <item>
      <title>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference</title>
      <link>https://paperswithcode.com/paper/smarter-better-faster-longer-a-modern</link>
      <description><![CDATA[Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/smarter-better-faster-longer-a-modern</guid>
    </item>
    <item>
      <title>Autoregressive Video Generation without Vector Quantization</title>
      <link>https://paperswithcode.com/paper/autoregressive-video-generation-without</link>
      <description><![CDATA[This paper presents a novel approach that enables autoregressive video generation with high efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoregressive-video-generation-without</guid>
    </item>
    <item>
      <title>YuLan-Mini: An Open Data-efficient Language Model</title>
      <link>https://paperswithcode.com/paper/yulan-mini-an-open-data-efficient-language</link>
      <description><![CDATA[Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yulan-mini-an-open-data-efficient-language</guid>
    </item>
    <item>
      <title>Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition</title>
      <link>https://paperswithcode.com/paper/lyra-an-efficient-and-speech-centric</link>
      <description><![CDATA[As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lyra-an-efficient-and-speech-centric</guid>
    </item>
    <item>
      <title>APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts</title>
      <link>https://paperswithcode.com/paper/appl-a-prompt-programming-language-for</link>
      <description><![CDATA[Large Language Models (LLMs) have become increasingly capable of handling diverse tasks with the aid of well-crafted prompts and integration of external tools, but as task complexity rises, the workflow involving LLMs can be complicated and thus challenging to implement and maintain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/appl-a-prompt-programming-language-for</guid>
    </item>
    <item>
      <title>Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems</title>
      <link>https://paperswithcode.com/paper/imitate-explore-and-self-improve-a</link>
      <description><![CDATA[We introduce an ``imitate, explore, and self-improve'' framework, denoted as \textbf{STILL-2}, as our primary technical approach to train the reasoning model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imitate-explore-and-self-improve-a</guid>
    </item>
    <item>
      <title>PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models</title>
      <link>https://paperswithcode.com/paper/pulse-self-supervised-photo-upsampling-via</link>
      <description><![CDATA[We present an algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pulse-self-supervised-photo-upsampling-via</guid>
    </item>
    <item>
      <title>Byte Latent Transformer: Patches Scale Better Than Tokens</title>
      <link>https://paperswithcode.com/paper/byte-latent-transformer-patches-scale-better</link>
      <description><![CDATA[We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/byte-latent-transformer-patches-scale-better</guid>
    </item>
  </channel>
</rss>
