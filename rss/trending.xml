<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 26 Mar 2024 09:12:32 +0000</lastBuildDate>
    <item>
      <title>Evolutionary Optimization of Model Merging Recipes</title>
      <link>https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</link>
      <description><![CDATA[Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging</guid>
    </item>
    <item>
      <title>Mora: Enabling Generalist Video Generation via A Multi-Agent Framework</title>
      <link>https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</link>
      <description><![CDATA[Sora is the first large-scale generalist video generation model that garnered significant attention across society.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mora-enabling-generalist-video-generation-via</guid>
    </item>
    <item>
      <title>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</title>
      <link>https://paperswithcode.com/paper/fresco-spatial-temporal-correspondence-for</link>
      <description><![CDATA[In this paper, we introduce FRESCO, intra-frame correspondence alongside inter-frame correspondence to establish a more robust spatial-temporal constraint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fresco-spatial-temporal-correspondence-for</guid>
    </item>
    <item>
      <title>One-Step Image Translation with Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</link>
      <description><![CDATA[In this work, we address two limitations of existing conditional diffusion models: their slow inference speed due to the iterative denoising process and their reliance on paired data for model fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-step-image-translation-with-text-to-image</guid>
    </item>
    <item>
      <title>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</title>
      <link>https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</link>
      <description><![CDATA[We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mvsplat-efficient-3d-gaussian-splatting-from</guid>
    </item>
    <item>
      <title>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</title>
      <link>https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</link>
      <description><![CDATA[We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0. 1s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grm-large-gaussian-reconstruction-model-for</guid>
    </item>
    <item>
      <title>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</title>
      <link>https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</link>
      <description><![CDATA[Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for</guid>
    </item>
    <item>
      <title>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</title>
      <link>https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</link>
      <description><![CDATA[Recognizing the complementary strengths and weaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes both prompts within a single model through contrastive learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/t-rex2-towards-generic-object-detection-via</guid>
    </item>
    <item>
      <title>Aggregated Contextual Transformations for High-Resolution Image Inpainting</title>
      <link>https://paperswithcode.com/paper/aggregated-contextual-transformations-for</link>
      <description><![CDATA[For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aggregated-contextual-transformations-for</guid>
    </item>
    <item>
      <title>StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control</title>
      <link>https://paperswithcode.com/paper/streammultidiffusion-real-time-interactive</link>
      <description><![CDATA[The enormous success of diffusion models in text-to-image synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streammultidiffusion-real-time-interactive</guid>
    </item>
    <item>
      <title>APISR: Anime Production Inspired Real-World Anime Super-Resolution</title>
      <link>https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</link>
      <description><![CDATA[In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</guid>
    </item>
    <item>
      <title>Chronos: Learning the Language of Time Series</title>
      <link>https://paperswithcode.com/paper/chronos-learning-the-language-of-time-series</link>
      <description><![CDATA[We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chronos-learning-the-language-of-time-series</guid>
    </item>
    <item>
      <title>OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models</title>
      <link>https://paperswithcode.com/paper/omg-occlusion-friendly-personalized-multi</link>
      <description><![CDATA[We also observe that the initiation denoising timestep for noise blending is the key to identity preservation and layout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omg-occlusion-friendly-personalized-multi</guid>
    </item>
    <item>
      <title>LLM4Decompile: Decompiling Binary Code with Large Language Models</title>
      <link>https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</link>
      <description><![CDATA[Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with</guid>
    </item>
    <item>
      <title>General Object Foundation Model for Images and Videos at Scale</title>
      <link>https://paperswithcode.com/paper/general-object-foundation-model-for-images</link>
      <description><![CDATA[We present GLEE in this work, an object-level foundation model for locating and identifying objects in images and videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/general-object-foundation-model-for-images</guid>
    </item>
    <item>
      <title>Analyzing and Improving the Training Dynamics of Diffusion Models</title>
      <link>https://paperswithcode.com/paper/analyzing-and-improving-the-training-dynamics</link>
      <description><![CDATA[Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/analyzing-and-improving-the-training-dynamics</guid>
    </item>
    <item>
      <title>RewardBench: Evaluating Reward Models for Language Modeling</title>
      <link>https://paperswithcode.com/paper/rewardbench-evaluating-reward-models-for</link>
      <description><![CDATA[In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rewardbench-evaluating-reward-models-for</guid>
    </item>
    <item>
      <title>When Do We Not Need Larger Vision Models?</title>
      <link>https://paperswithcode.com/paper/when-do-we-not-need-larger-vision-models</link>
      <description><![CDATA[Our results show that a multi-scale smaller model has comparable learning capacity to a larger model, and pre-training smaller models with S$^2$ can match or even exceed the advantage of larger models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/when-do-we-not-need-larger-vision-models</guid>
    </item>
    <item>
      <title>VmambaIR: Visual State Space Model for Image Restoration</title>
      <link>https://paperswithcode.com/paper/vmambair-visual-state-space-model-for-image</link>
      <description><![CDATA[To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vmambair-visual-state-space-model-for-image</guid>
    </item>
    <item>
      <title>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title>
      <link>https://paperswithcode.com/paper/bge-m3-embedding-multi-lingual-multi</link>
      <description><![CDATA[It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bge-m3-embedding-multi-lingual-multi</guid>
    </item>
  </channel>
</rss>
