<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 18 May 2023 09:11:36 +0000</lastBuildDate>
    <item>
      <title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment</title>
      <link>https://paperswithcode.com/paper/decentralization-and-acceleration-enables</link>
      <description><![CDATA[In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decentralization-and-acceleration-enables</guid>
    </item>
    <item>
      <title>HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge</title>
      <link>https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</link>
      <description><![CDATA[Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/huatuo-tuning-llama-model-with-chinese</guid>
    </item>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <link>https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</link>
      <description><![CDATA[We present Shap-E, a conditional generative model for 3D assets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shap-e-generating-conditional-3d-implicit</guid>
    </item>
    <item>
      <title>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</link>
      <description><![CDATA[To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codet5-open-code-large-language-models-for</guid>
    </item>
    <item>
      <title>Tool Learning with Foundation Models</title>
      <link>https://paperswithcode.com/paper/tool-learning-with-foundation-models</link>
      <description><![CDATA[Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tool-learning-with-foundation-models</guid>
    </item>
    <item>
      <title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title>
      <link>https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</link>
      <description><![CDATA[We recruit annotators to search for relevant information using our interface and then answer questions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webcpm-interactive-web-search-for-chinese</guid>
    </item>
    <item>
      <title>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</link>
      <description><![CDATA[In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instructblip-towards-general-purpose-vision</guid>
    </item>
    <item>
      <title>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</title>
      <link>https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</link>
      <description><![CDATA[This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a</guid>
    </item>
    <item>
      <title>TidyBot: Personalized Robot Assistance with Large Language Models</title>
      <link>https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</link>
      <description><![CDATA[For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tidybot-personalized-robot-assistance-with</guid>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <link>https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</link>
      <description><![CDATA[We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagebind-one-embedding-space-to-bind-them</guid>
    </item>
    <item>
      <title>Progressive-Hint Prompting Improves Reasoning in Large Language Models</title>
      <link>https://paperswithcode.com/paper/progressive-hint-prompting-improves-reasoning</link>
      <description><![CDATA[The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/progressive-hint-prompting-improves-reasoning</guid>
    </item>
    <item>
      <title>Active Retrieval Augmented Generation</title>
      <link>https://paperswithcode.com/paper/active-retrieval-augmented-generation</link>
      <description><![CDATA[We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/active-retrieval-augmented-generation</guid>
    </item>
    <item>
      <title>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</title>
      <link>https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</link>
      <description><![CDATA[We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/c-eval-a-multi-level-multi-discipline-chinese</guid>
    </item>
    <item>
      <title>PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model</title>
      <link>https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</link>
      <description><![CDATA[Real-world applications have high demands for semantic segmentation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pp-liteseg-a-superior-real-time-semantic</guid>
    </item>
    <item>
      <title>Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</link>
      <description><![CDATA[A medical image adapted SAM, which we have dubbed Medical SAM Adapter (MSA), shows superior performance on 19 medical image segmentation tasks with various image modalities including CT, MRI, ultrasound image, fundus image, and dermoscopic images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medical-sam-adapter-adapting-segment-anything</guid>
    </item>
    <item>
      <title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/otter-a-multi-modal-model-with-in-context</guid>
    </item>
    <item>
      <title>U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection</title>
      <link>https://paperswithcode.com/paper/u-2-net-going-deeper-with-nested-u-structure</link>
      <description><![CDATA[In this paper, we design a simple yet powerful deep network architecture, U$^2$-Net, for salient object detection (SOD).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/u-2-net-going-deeper-with-nested-u-structure</guid>
    </item>
    <item>
      <title>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</title>
      <link>https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</link>
      <description><![CDATA[In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-codec-language-models-are-zero-shot</guid>
    </item>
    <item>
      <title>Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision</title>
      <link>https://paperswithcode.com/paper/principle-driven-self-alignment-of-language</link>
      <description><![CDATA[Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/principle-driven-self-alignment-of-language</guid>
    </item>
    <item>
      <title>On the Hidden Mystery of OCR in Large Multimodal Models</title>
      <link>https://paperswithcode.com/paper/on-the-hidden-mystery-of-ocr-in-large</link>
      <description><![CDATA[Large models have recently played a dominant role in natural language processing and multimodal vision-language learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-hidden-mystery-of-ocr-in-large</guid>
    </item>
  </channel>
</rss>
