<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 12 Feb 2024 21:06:15 +0000</lastBuildDate>
    <item>
      <title>Guiding Instruction-based Image Editing via Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/guiding-instruction-based-image-editing-via</link>
      <description><![CDATA[Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/guiding-instruction-based-image-editing-via</guid>
    </item>
    <item>
      <title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title>
      <link>https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</link>
      <description><![CDATA[The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</guid>
    </item>
    <item>
      <title>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</title>
      <link>https://paperswithcode.com/paper/self-play-fine-tuning-converts-weak-language</link>
      <description><![CDATA[In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-play-fine-tuning-converts-weak-language</guid>
    </item>
    <item>
      <title>DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors</title>
      <link>https://paperswithcode.com/paper/dynamicrafter-animating-open-domain-images</link>
      <description><![CDATA[Animating a still image offers an engaging visual experience.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dynamicrafter-animating-open-domain-images</guid>
    </item>
    <item>
      <title>Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time</link>
      <description><![CDATA[Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time</guid>
    </item>
    <item>
      <title>Learning to Fly in Seconds</title>
      <link>https://paperswithcode.com/paper/learning-to-fly-in-seconds</link>
      <description><![CDATA[Our framework enables Simulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18 seconds of training on a consumer-grade laptop as well as its deployment on microcontrollers to control a multirotor under real-time guarantees.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-fly-in-seconds</guid>
    </item>
    <item>
      <title>Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</title>
      <link>https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</link>
      <description><![CDATA[To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</guid>
    </item>
    <item>
      <title>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title>
      <link>https://paperswithcode.com/paper/deepseekmath-pushing-the-limits-of</link>
      <description><![CDATA[Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseekmath-pushing-the-limits-of</guid>
    </item>
    <item>
      <title>Extreme Compression of Large Language Models via Additive Quantization</title>
      <link>https://paperswithcode.com/paper/extreme-compression-of-large-language-models</link>
      <description><![CDATA[The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/extreme-compression-of-large-language-models</guid>
    </item>
    <item>
      <title>Fast Timing-Conditioned Latent Audio Diffusion</title>
      <link>https://paperswithcode.com/paper/fast-timing-conditioned-latent-audio</link>
      <description><![CDATA[Generating long-form 44. 1kHz stereo audio from text prompts can be computationally demanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-timing-conditioned-latent-audio</guid>
    </item>
    <item>
      <title>Self-Discover: Large Language Models Self-Compose Reasoning Structures</title>
      <link>https://paperswithcode.com/paper/self-discover-large-language-models-self</link>
      <description><![CDATA[We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-discover-large-language-models-self</guid>
    </item>
    <item>
      <title>OLMo: Accelerating the Science of Language Models</title>
      <link>https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</link>
      <description><![CDATA[Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/olmo-accelerating-the-science-of-language</guid>
    </item>
    <item>
      <title>Learning a Decision Tree Algorithm with Transformers</title>
      <link>https://paperswithcode.com/paper/learning-a-decision-tree-algorithm-with</link>
      <description><![CDATA[We then train MetaTree to produce the trees that achieve strong generalization performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-a-decision-tree-algorithm-with</guid>
    </item>
    <item>
      <title>BiLLM: Pushing the Limit of Post-Training Quantization for LLMs</title>
      <link>https://paperswithcode.com/paper/billm-pushing-the-limit-of-post-training</link>
      <description><![CDATA[Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/billm-pushing-the-limit-of-post-training</guid>
    </item>
    <item>
      <title>EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss</title>
      <link>https://paperswithcode.com/paper/efficientvit-sam-accelerated-segment-anything</link>
      <description><![CDATA[For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficientvit-sam-accelerated-segment-anything</guid>
    </item>
    <item>
      <title>PokéLLMon: A Human-Parity Agent for Pokémon Battles with Large Language Models</title>
      <link>https://paperswithcode.com/paper/pokellmon-a-human-parity-agent-for-pokemon</link>
      <description><![CDATA[We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pokellmon-a-human-parity-agent-for-pokemon</guid>
    </item>
    <item>
      <title>Forgedit: Text Guided Image Editing via Learning and Forgetting</title>
      <link>https://paperswithcode.com/paper/forgedit-text-guided-image-editing-via</link>
      <description><![CDATA[Text guided image editing on real images given only the image and the target text prompt as inputs, is a very general and challenging problem, which requires the editing model to reason by itself which part of the image should be edited, to preserve the characteristics of original image, and also to perform complicated non-rigid editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/forgedit-text-guided-image-editing-via</guid>
    </item>
    <item>
      <title>InstantID: Zero-shot Identity-Preserving Generation in Seconds</title>
      <link>https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</link>
      <description><![CDATA[There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/oasim-an-open-and-adaptive-simulator-based-on</link>
      <description><![CDATA[With the development of implicit rendering technology and in-depth research on using generative models to produce data at scale, we propose OASim, an open and adaptive simulator and autonomous driving data generator based on implicit neural rendering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/oasim-an-open-and-adaptive-simulator-based-on</guid>
    </item>
    <item>
      <title>MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</title>
      <link>https://paperswithcode.com/paper/multihop-rag-benchmarking-retrieval-augmented</link>
      <description><![CDATA[We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multihop-rag-benchmarking-retrieval-augmented</guid>
    </item>
  </channel>
</rss>
