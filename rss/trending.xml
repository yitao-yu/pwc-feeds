<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 07 Jan 2023 09:11:37 +0000</lastBuildDate>
    <item>
      <title>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</title>
      <link>https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets</link>
      <description><![CDATA[This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets</guid>
    </item>
    <item>
      <title>Cramming: Training a Language Model on a Single GPU in One Day</title>
      <link>https://paperswithcode.com/paper/cramming-training-a-language-model-on-a</link>
      <description><![CDATA[Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cramming-training-a-language-model-on-a</guid>
    </item>
    <item>
      <title>Muse: Text-To-Image Generation via Masked Generative Transformers</title>
      <link>https://paperswithcode.com/paper/muse-text-to-image-generation-via-masked</link>
      <description><![CDATA[Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/muse-text-to-image-generation-via-masked</guid>
    </item>
    <item>
      <title>A Survey for In-context Learning</title>
      <link>https://paperswithcode.com/paper/a-survey-for-in-context-learning</link>
      <description><![CDATA[With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-for-in-context-learning</guid>
    </item>
    <item>
      <title>Attention Is All You Need</title>
      <link>https://paperswithcode.com/paper/attention-is-all-you-need</link>
      <description><![CDATA[The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/attention-is-all-you-need</guid>
    </item>
    <item>
      <title>Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal</title>
      <link>https://paperswithcode.com/paper/reasoning-over-different-types-of-knowledge</link>
      <description><![CDATA[The early works in this domain mainly focus on static KGR and tend to directly apply general knowledge graph embedding models to the reasoning task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reasoning-over-different-types-of-knowledge</guid>
    </item>
    <item>
      <title>Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data</title>
      <link>https://paperswithcode.com/paper/diffusion-probabilistic-models-for-scene</link>
      <description><![CDATA[To the best of our knowledge, our work is the first to apply discrete and latent diffusion for 3D categorical data on a scene-scale.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-probabilistic-models-for-scene</guid>
    </item>
    <item>
      <title>Diffusion Models in Vision: A Survey</title>
      <link>https://paperswithcode.com/paper/diffusion-models-in-vision-a-survey</link>
      <description><![CDATA[Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-in-vision-a-survey</guid>
    </item>
    <item>
      <title>MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding</title>
      <link>https://paperswithcode.com/paper/maud-an-expert-annotated-legal-nlp-dataset</link>
      <description><![CDATA[Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/maud-an-expert-annotated-legal-nlp-dataset</guid>
    </item>
    <item>
      <title>Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection</title>
      <link>https://paperswithcode.com/paper/cross-modal-transformer-via-coordinates</link>
      <description><![CDATA[In this paper, we propose a robust 3D detector, named Cross Modal Transformer (CMT), for end-to-end 3D multi-modal detection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cross-modal-transformer-via-coordinates</guid>
    </item>
    <item>
      <title>TextBox 2.0: A Text Generation Library with Pre-trained Language Models</title>
      <link>https://paperswithcode.com/paper/textbox-2-0-a-text-generation-library-with</link>
      <description><![CDATA[To facilitate research on text generation, this paper presents a comprehensive and unified library, TextBox 2. 0, focusing on the use of pre-trained language models (PLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textbox-2-0-a-text-generation-library-with</guid>
    </item>
    <item>
      <title>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</title>
      <link>https://paperswithcode.com/paper/point-e-a-system-for-generating-3d-point</link>
      <description><![CDATA[This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/point-e-a-system-for-generating-3d-point</guid>
    </item>
    <item>
      <title>Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution</title>
      <link>https://paperswithcode.com/paper/box2mask-box-supervised-instance-segmentation</link>
      <description><![CDATA[In contrast to fully supervised methods using pixel-wise mask labels, box-supervised instance segmentation takes advantage of simple box annotations, which has recently attracted increasing research attention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/box2mask-box-supervised-instance-segmentation</guid>
    </item>
    <item>
      <title>Large Language Models as Corporate Lobbyists</title>
      <link>https://paperswithcode.com/paper/large-language-models-as-corporate-lobbyists</link>
      <description><![CDATA[We use hundreds of ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model, which outperforms the baseline of predicting the most common outcome of irrelevance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-models-as-corporate-lobbyists</guid>
    </item>
    <item>
      <title>Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training</title>
      <link>https://paperswithcode.com/paper/colossal-ai-a-unified-deep-learning-system</link>
      <description><![CDATA[The success of Transformer models has pushed the deep learning model scale to billions of parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/colossal-ai-a-unified-deep-learning-system</guid>
    </item>
    <item>
      <title>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</title>
      <link>https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</link>
      <description><![CDATA[In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</guid>
    </item>
    <item>
      <title>Rethinking Mobile Block for Efficient Neural Models</title>
      <link>https://paperswithcode.com/paper/rethinking-mobile-block-for-efficient-neural</link>
      <description><![CDATA[This paper focuses on designing efficient models with low parameters and FLOPs for dense predictions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-mobile-block-for-efficient-neural</guid>
    </item>
    <item>
      <title>TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models</title>
      <link>https://paperswithcode.com/paper/tinymim-an-empirical-study-of-distilling-mim</link>
      <description><![CDATA[Our TinyMIM model of tiny size achieves 79. 6% top-1 accuracy on ImageNet-1K image classification, which sets a new record for small vision models of the same size and computation budget.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinymim-an-empirical-study-of-distilling-mim</guid>
    </item>
    <item>
      <title>NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/nerf-slam-real-time-dense-monocular-slam-with</link>
      <description><![CDATA[We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nerf-slam-real-time-dense-monocular-slam-with</guid>
    </item>
    <item>
      <title>Scaling Language-Image Pre-training via Masking</title>
      <link>https://paperswithcode.com/paper/scaling-language-image-pre-training-via</link>
      <description><![CDATA[We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-language-image-pre-training-via</guid>
    </item>
  </channel>
</rss>
