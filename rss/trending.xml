<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 19 Jul 2023 21:06:27 +0000</lastBuildDate>
    <item>
      <title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title>
      <link>https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</link>
      <description><![CDATA[With the advance of text-to-image models (e. g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</guid>
    </item>
    <item>
      <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title>
      <link>https://paperswithcode.com/paper/flashattention-2-faster-attention-with-better</link>
      <description><![CDATA[We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-2-faster-attention-with-better</guid>
    </item>
    <item>
      <title>Semantic-SAM: Segment and Recognize Anything at Any Granularity</title>
      <link>https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</link>
      <description><![CDATA[In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</guid>
    </item>
    <item>
      <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</link>
      <description><![CDATA[We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</guid>
    </item>
    <item>
      <title>Planting a SEED of Vision in Large Language Model</title>
      <link>https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language</link>
      <description><![CDATA[Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.)]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language</guid>
    </item>
    <item>
      <title>Llama 2: Open Foundation and Fine-Tuned Chat Models</title>
      <link>https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat</link>
      <description><![CDATA[In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat</guid>
    </item>
    <item>
      <title>Copy Is All You Need</title>
      <link>https://paperswithcode.com/paper/copy-is-all-you-need</link>
      <description><![CDATA[The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/copy-is-all-you-need</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
      <link>https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</link>
      <description><![CDATA[We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-neox-20b-an-open-source-autoregressive-1</guid>
    </item>
    <item>
      <title>FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing</title>
      <link>https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</link>
      <description><![CDATA[To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/freedrag-point-tracking-is-not-you-need-for</guid>
    </item>
    <item>
      <title>Generative Pretraining in Multimodality</title>
      <link>https://paperswithcode.com/paper/generative-pretraining-in-multimodality</link>
      <description><![CDATA[We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-pretraining-in-multimodality</guid>
    </item>
    <item>
      <title>MedLSAM: Localize and Segment Anything Model for 3D Medical Images</title>
      <link>https://paperswithcode.com/paper/medlsam-localize-and-segment-anything-model</link>
      <description><![CDATA[Nevertheless, both the original SAM and its medical adaptations necessitate slice-by-slice annotations, which directly increase the annotation workload with the size of the dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medlsam-localize-and-segment-anything-model</guid>
    </item>
    <item>
      <title>Petals: Collaborative Inference and Fine-tuning of Large Models</title>
      <link>https://paperswithcode.com/paper/petals-collaborative-inference-and-fine</link>
      <description><![CDATA[However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/petals-collaborative-inference-and-fine</guid>
    </item>
    <item>
      <title>Secrets of RLHF in Large Language Models Part I: PPO</title>
      <link>https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</link>
      <description><![CDATA[Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/secrets-of-rlhf-in-large-language-models-part</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>MIS-FM: 3D Medical Image Segmentation using Foundation Models Pretrained on a Large-Scale Unannotated Dataset</title>
      <link>https://paperswithcode.com/paper/mis-fm-3d-medical-image-segmentation-using</link>
      <description><![CDATA[The proposed model was pretrained with 110k unannotated 3D CT volumes, and experiments with different downstream segmentation targets including head and neck organs, thoracic/abdominal organs showed that our pretrained model largely outperformed training from scratch and several state-of-the-art self-supervised training methods and segmentation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mis-fm-3d-medical-image-segmentation-using</guid>
    </item>
    <item>
      <title>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</title>
      <link>https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</link>
      <description><![CDATA[In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prolificdreamer-high-fidelity-and-diverse</guid>
    </item>
    <item>
      <title>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</title>
      <link>https://paperswithcode.com/paper/animate-a-story-storytelling-with-retrieval</link>
      <description><![CDATA[For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animate-a-story-storytelling-with-retrieval</guid>
    </item>
    <item>
      <title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates</title>
      <link>https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</link>
      <description><![CDATA[Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stack-more-layers-differently-high-rank</guid>
    </item>
    <item>
      <title>Exploiting Diffusion Prior for Real-World Image Super-Resolution</title>
      <link>https://paperswithcode.com/paper/exploiting-diffusion-prior-for-real-world</link>
      <description><![CDATA[We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploiting-diffusion-prior-for-real-world</guid>
    </item>
  </channel>
</rss>
