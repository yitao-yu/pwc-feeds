<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 28 Jul 2024 21:09:05 +0000</lastBuildDate>
    <item>
      <title>On the Design and Analysis of LLM-Based Algorithms</title>
      <link>https://paperswithcode.com/paper/on-the-design-and-analysis-of-llm-based</link>
      <description><![CDATA[We initiate a formal investigation into the design and analysis of LLM-based algorithms, i. e. algorithms that contain one or multiple calls of large language models (LLMs) as sub-routines and critically rely on the capabilities of LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-design-and-analysis-of-llm-based</guid>
    </item>
    <item>
      <title>DataComp-LM: In search of the next generation of training sets for language models</title>
      <link>https://paperswithcode.com/paper/datacomp-lm-in-search-of-the-next-generation</link>
      <description><![CDATA[We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datacomp-lm-in-search-of-the-next-generation</guid>
    </item>
    <item>
      <title>IMAGDressing-v1: Customizable Virtual Dressing</title>
      <link>https://paperswithcode.com/paper/imagdressing-v1-customizable-virtual-dressing</link>
      <description><![CDATA[Latest advances have achieved realistic virtual try-on (VTON) through localized garment inpainting using latent diffusion models, significantly enhancing consumers' online shopping experience.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagdressing-v1-customizable-virtual-dressing</guid>
    </item>
    <item>
      <title>MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens</title>
      <link>https://paperswithcode.com/paper/mint-1t-scaling-open-source-multimodal-data</link>
      <description><![CDATA[Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mint-1t-scaling-open-source-multimodal-data</guid>
    </item>
    <item>
      <title>Odyssey: Empowering Agents with Open-World Skills</title>
      <link>https://paperswithcode.com/paper/odyssey-empowering-agents-with-open-world</link>
      <description><![CDATA[In this work, we introduce ODYSSEY, a new framework that empowers Large Language Model (LLM)-based agents with open-world skills to explore the vast Minecraft world.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/odyssey-empowering-agents-with-open-world</guid>
    </item>
    <item>
      <title>Neural General Circulation Models for Weather and Climate</title>
      <link>https://paperswithcode.com/paper/neural-general-circulation-models</link>
      <description><![CDATA[Here we present the first GCM that combines a differentiable solver for atmospheric dynamics with ML components, and show that it can generate forecasts of deterministic weather, ensemble weather and climate on par with the best ML and physics-based methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-general-circulation-models</guid>
    </item>
    <item>
      <title>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</title>
      <link>https://paperswithcode.com/paper/humanvid-demystifying-training-data-for</link>
      <description><![CDATA[Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humanvid-demystifying-training-data-for</guid>
    </item>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title>
      <link>https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</link>
      <description><![CDATA[This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>Deep-TEMPEST: Using Deep Learning to Eavesdrop on HDMI from its Unintended Electromagnetic Emanations</title>
      <link>https://paperswithcode.com/paper/deep-tempest-using-deep-learning-to-eavesdrop</link>
      <description><![CDATA[As a result, eavesdropping systems designed for the analog case obtain unclear and difficult-to-read images when applied to digital video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-tempest-using-deep-learning-to-eavesdrop</guid>
    </item>
    <item>
      <title>Stable-Hair: Real-World Hair Transfer via Diffusion Model</title>
      <link>https://paperswithcode.com/paper/stable-hair-real-world-hair-transfer-via</link>
      <description><![CDATA[In the first stage, we train a Bald Converter alongside stable diffusion to remove hair from the user-provided face images, resulting in bald images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stable-hair-real-world-hair-transfer-via</guid>
    </item>
    <item>
      <title>Cradle: Empowering Foundation Agents Towards General Computer Control</title>
      <link>https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</link>
      <description><![CDATA[To handle this issue, we propose the General Computer Control (GCC) setting to restrict foundation agents to interact with software through the most unified and standardized interface, i. e., using screenshots as input and keyboard and mouse actions as output.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>KAN or MLP: A Fairer Comparison</title>
      <link>https://paperswithcode.com/paper/kan-or-mlp-a-fairer-comparison</link>
      <description><![CDATA[This paper does not introduce a novel method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kan-or-mlp-a-fairer-comparison</guid>
    </item>
    <item>
      <title>Compact Language Models via Pruning and Knowledge Distillation</title>
      <link>https://paperswithcode.com/paper/compact-language-models-via-pruning-and</link>
      <description><![CDATA[Large language models (LLMs) targeting different deployment scales and sizes are currently produced by training each variant from scratch; this is extremely compute-intensive.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/compact-language-models-via-pruning-and</guid>
    </item>
    <item>
      <title>GRUtopia: Dream General Robots in a City at Scale</title>
      <link>https://paperswithcode.com/paper/grutopia-dream-general-robots-in-a-city-at</link>
      <description><![CDATA[Recent works have been exploring the scaling laws in the field of Embodied AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grutopia-dream-general-robots-in-a-city-at</guid>
    </item>
    <item>
      <title>VGGSfM: Visual Geometry Grounded Deep Structure From Motion</title>
      <link>https://paperswithcode.com/paper/vggsfm-visual-geometry-grounded-deep</link>
      <description><![CDATA[Finally we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vggsfm-visual-geometry-grounded-deep</guid>
    </item>
    <item>
      <title>Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions</title>
      <link>https://paperswithcode.com/paper/diffusion-models-for-monocular-depth</link>
      <description><![CDATA[We present a novel approach designed to address the complexities posed by challenging, out-of-distribution data in the single-image depth estimation task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-for-monocular-depth</guid>
    </item>
    <item>
      <title>FinanceBench: A New Benchmark for Financial Question Answering</title>
      <link>https://paperswithcode.com/paper/financebench-a-new-benchmark-for-financial</link>
      <description><![CDATA[We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2, 400).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/financebench-a-new-benchmark-for-financial</guid>
    </item>
  </channel>
</rss>
