<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 25 Aug 2023 21:05:16 +0000</lastBuildDate>
    <item>
      <title>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</title>
      <link>https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</link>
      <description><![CDATA[What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seamlessm4t-massively-multilingual-multimodal</guid>
    </item>
    <item>
      <title>CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</title>
      <link>https://paperswithcode.com/paper/codef-content-deformation-fields-for</link>
      <description><![CDATA[We present the content deformation field CoDeF as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i. e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e. g., the object shape) from the video. With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found at https://qiuyu96. github. io/CoDeF/.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codef-content-deformation-fields-for</guid>
    </item>
    <item>
      <title>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</title>
      <link>https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</link>
      <description><![CDATA[In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stablevideo-text-driven-consistency-aware</guid>
    </item>
    <item>
      <title>An Open-Source LoRa Physical Layer Prototype on GNU Radio</title>
      <link>https://paperswithcode.com/paper/an-open-source-lora-physical-layer-prototype</link>
      <description><![CDATA[LoRa is the proprietary physical layer (PHY) of LoRaWAN, which is a popular Internet-of-Things (IoT) protocol enabling low-power devices to communicate over long ranges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-open-source-lora-physical-layer-prototype</guid>
    </item>
    <item>
      <title>ChatHaruhi: Reviving Anime Character in Reality via Large Language Model</title>
      <link>https://paperswithcode.com/paper/chatharuhi-reviving-anime-character-in</link>
      <description><![CDATA[Role-playing chatbots built on large language models have drawn interest, but better techniques are needed to enable mimicking specific fictional characters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chatharuhi-reviving-anime-character-in</guid>
    </item>
    <item>
      <title>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</title>
      <link>https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</link>
      <description><![CDATA[We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graph-of-thoughts-solving-elaborate-problems</guid>
    </item>
    <item>
      <title>3D Gaussian Splatting for Real-Time Radiance Field Rendering</title>
      <link>https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</link>
      <description><![CDATA[Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-gaussian-splatting-for-real-time-radiance</guid>
    </item>
    <item>
      <title>SONAR: Sentence-Level Multimodal and Language-Agnostic Representations</title>
      <link>https://paperswithcode.com/paper/sentence-level-multimodal-and-language</link>
      <description><![CDATA[Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sentence-level-multimodal-and-language</guid>
    </item>
    <item>
      <title>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</title>
      <link>https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</link>
      <description><![CDATA[Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/it3d-improved-text-to-3d-generation-with</guid>
    </item>
    <item>
      <title>NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</title>
      <link>https://paperswithcode.com/paper/neus-learning-neural-implicit-surfaces-by</link>
      <description><![CDATA[In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neus-learning-neural-implicit-surfaces-by</guid>
    </item>
    <item>
      <title>StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views</title>
      <link>https://paperswithcode.com/paper/streetsurf-extending-multi-view-implicit</link>
      <description><![CDATA[We present a novel multi-view implicit surface reconstruction technique, termed StreetSurf, that is readily applicable to street view images in widely-used autonomous driving datasets, such as Waymo-perception sequences, without necessarily requiring LiDAR data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streetsurf-extending-multi-view-implicit</guid>
    </item>
    <item>
      <title>Giraffe: Adventures in Expanding Context Lengths in LLMs</title>
      <link>https://paperswithcode.com/paper/giraffe-adventures-in-expanding-context</link>
      <description><![CDATA[To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/giraffe-adventures-in-expanding-context</guid>
    </item>
    <item>
      <title>Neuralangelo: High-Fidelity Neural Surface Reconstruction</title>
      <link>https://paperswithcode.com/paper/neuralangelo-high-fidelity-neural-surface-1</link>
      <description><![CDATA[Neural surface reconstruction has been shown to be powerful for recovering dense 3D surfaces via image-based neural rendering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neuralangelo-high-fidelity-neural-surface-1</guid>
    </item>
    <item>
      <title>Prompt2Model: Generating Deployable Models from Natural Language Instructions</title>
      <link>https://paperswithcode.com/paper/prompt2model-generating-deployable-models</link>
      <description><![CDATA[In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompt2model-generating-deployable-models</guid>
    </item>
    <item>
      <title>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</title>
      <link>https://paperswithcode.com/paper/platypus-quick-cheap-and-powerful-refinement</link>
      <description><![CDATA[We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/platypus-quick-cheap-and-powerful-refinement</guid>
    </item>
    <item>
      <title>CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X</title>
      <link>https://paperswithcode.com/paper/codegeex-a-pre-trained-model-for-code</link>
      <description><![CDATA[Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codegeex-a-pre-trained-model-for-code</guid>
    </item>
    <item>
      <title>Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction</title>
      <link>https://paperswithcode.com/paper/vanishing-point-estimation-in-uncalibrated</link>
      <description><![CDATA[We tackle the problem of estimating a Manhattan frame, i. e. three orthogonal vanishing points, and the unknown focal length of the camera, leveraging a prior vertical direction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vanishing-point-estimation-in-uncalibrated</guid>
    </item>
    <item>
      <title>Efficient Guided Generation for Large Language Models</title>
      <link>https://paperswithcode.com/paper/efficient-guided-generation-for-llms</link>
      <description><![CDATA[In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-guided-generation-for-llms</guid>
    </item>
    <item>
      <title>Diffusion Models for Image Restoration and Enhancement -- A Comprehensive Survey</title>
      <link>https://paperswithcode.com/paper/diffusion-models-for-image-restoration-and</link>
      <description><![CDATA[Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-for-image-restoration-and</guid>
    </item>
    <item>
      <title>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</title>
      <link>https://paperswithcode.com/paper/plan-and-solve-prompting-improving-zero-shot</link>
      <description><![CDATA[To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/plan-and-solve-prompting-improving-zero-shot</guid>
    </item>
  </channel>
</rss>
