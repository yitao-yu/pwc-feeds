<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 15 Nov 2024 21:08:53 +0000</lastBuildDate>
    <item>
      <title>Qwen2.5-Coder Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-5-coder-technical-report</link>
      <description><![CDATA[In this report, we introduce the Qwen2. 5-Coder series, a significant upgrade from its predecessor, CodeQwen1. 5.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-5-coder-technical-report</guid>
    </item>
    <item>
      <title>Docling Technical Report</title>
      <link>https://paperswithcode.com/paper/docling-technical-report</link>
      <description><![CDATA[This technical report introduces Docling, an easy to use, self-contained, MIT-licensed open-source package for PDF document conversion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/docling-technical-report</guid>
    </item>
    <item>
      <title>OmniGen: Unified Image Generation</title>
      <link>https://paperswithcode.com/paper/omnigen-unified-image-generation</link>
      <description><![CDATA[In this work, we introduce OmniGen, a new diffusion model for unified image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnigen-unified-image-generation</guid>
    </item>
    <item>
      <title>Scaling Mesh Generation via Compressive Tokenization</title>
      <link>https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</link>
      <description><![CDATA[We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</guid>
    </item>
    <item>
      <title>Autoregressive Models in Vision: A Survey</title>
      <link>https://paperswithcode.com/paper/autoregressive-models-in-vision-a-survey</link>
      <description><![CDATA[Autoregressive modeling has been a huge success in the field of natural language processing (NLP).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoregressive-models-in-vision-a-survey</guid>
    </item>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</link>
      <description><![CDATA[While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-lora-for-diffusion-transformers</guid>
    </item>
    <item>
      <title>SplatFormer: Point Transformer for Robust 3D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</link>
      <description><![CDATA[To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</guid>
    </item>
    <item>
      <title>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title>
      <link>https://paperswithcode.com/paper/svdqunat-absorbing-outliers-by-low-rank</link>
      <description><![CDATA[To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/svdqunat-absorbing-outliers-by-low-rank</guid>
    </item>
    <item>
      <title>Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation</title>
      <link>https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</link>
      <description><![CDATA[To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hallo2-long-duration-and-high-resolution</guid>
    </item>
    <item>
      <title>The Surprising Effectiveness of Test-Time Training for Abstract Reasoning</title>
      <link>https://paperswithcode.com/paper/the-surprising-effectiveness-of-test-time</link>
      <description><![CDATA[TTT significantly improves performance on ARC tasks, achieving up to 6x improvement in accuracy compared to base fine-tuned models; applying TTT to an 8B-parameter language model, we achieve 53% accuracy on the ARC's public validation set, improving the state-of-the-art by nearly 25% for public and purely neural approaches.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-surprising-effectiveness-of-test-time</guid>
    </item>
    <item>
      <title>Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement</title>
      <link>https://paperswithcode.com/paper/lingma-swe-gpt-an-open-development-process</link>
      <description><![CDATA[The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30. 20% of the GitHub issues, marking a significant improvement in automatic issue resolution (22. 76% relative improvement compared to Llama 3. 1 405B), approaching the performance of closed-source models (31. 80\% issues of GPT-4o resolved).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lingma-swe-gpt-an-open-development-process</guid>
    </item>
    <item>
      <title>Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings</title>
      <link>https://paperswithcode.com/paper/wavelet-latent-diffusion-wala-billion</link>
      <description><![CDATA[We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wavelet-latent-diffusion-wala-billion</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>ADOPT: Modified Adam Can Converge with Any $Î²_2$ with the Optimal Rate</title>
      <link>https://paperswithcode.com/paper/adopt-modified-adam-can-converge-with-any-b-2</link>
      <description><![CDATA[Adam is one of the most popular optimization algorithms in deep learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adopt-modified-adam-can-converge-with-any-b-2</guid>
    </item>
    <item>
      <title>TableGPT2: A Large Multimodal Model with Tabular Data Integration</title>
      <link>https://paperswithcode.com/paper/tablegpt2-a-large-multimodal-model-with</link>
      <description><![CDATA[In response, we introduce TableGPT2, a model rigorously pre-trained and fine-tuned with over 593. 8K tables and 2. 36M high-quality query-table-output tuples, a scale of table-related data unprecedented in prior research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tablegpt2-a-large-multimodal-model-with</guid>
    </item>
    <item>
      <title>Taming Rectified Flow for Inversion and Editing</title>
      <link>https://paperswithcode.com/paper/taming-rectified-flow-for-inversion-and</link>
      <description><![CDATA[Rectified-flow-based diffusion transformers, such as FLUX and OpenSora, have demonstrated exceptional performance in the field of image and video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taming-rectified-flow-for-inversion-and</guid>
    </item>
    <item>
      <title>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</title>
      <link>https://paperswithcode.com/paper/llm2clip-powerful-language-model-unlock</link>
      <description><![CDATA[In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm2clip-powerful-language-model-unlock</guid>
    </item>
    <item>
      <title>TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control</title>
      <link>https://paperswithcode.com/paper/stylesinger-2-zero-shot-singing-voice</link>
      <description><![CDATA[To address these challenges, we introduce TCSinger, the first zero-shot SVS model for style transfer across cross-lingual speech and singing styles, along with multi-level style control.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stylesinger-2-zero-shot-singing-voice</guid>
    </item>
    <item>
      <title>optillm</title>
      <link>https://github.com/codelion/optillm</link>
      <description><![CDATA[Optimizing inference proxy for LLMs]]></description>
      <guid isPermaLink="true">https://github.com/codelion/optillm</guid>
    </item>
    <item>
      <title>TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks</title>
      <link>https://paperswithcode.com/paper/transformerranker-a-tool-for-efficiently</link>
      <description><![CDATA[Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transformerranker-a-tool-for-efficiently</guid>
    </item>
  </channel>
</rss>
