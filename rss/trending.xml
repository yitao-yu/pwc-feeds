<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 19 Oct 2023 09:12:09 +0000</lastBuildDate>
    <item>
      <title>OpenAgents: An Open Platform for Language Agents in the Wild</title>
      <link>https://paperswithcode.com/paper/openagents-an-open-platform-for-language</link>
      <description><![CDATA[Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openagents-an-open-platform-for-language</guid>
    </item>
    <item>
      <title>PixArt-$Î±$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</title>
      <link>https://paperswithcode.com/paper/pixart-a-fast-training-of-diffusion</link>
      <description><![CDATA[We hope PIXART-$\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixart-a-fast-training-of-diffusion</guid>
    </item>
    <item>
      <title>Llemma: An Open Language Model For Mathematics</title>
      <link>https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</link>
      <description><![CDATA[We present Llemma, a large language model for mathematics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</guid>
    </item>
    <item>
      <title>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</title>
      <link>https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</link>
      <description><![CDATA[AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autogen-enabling-next-gen-llm-applications</guid>
    </item>
    <item>
      <title>From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/from-clip-to-dino-visual-encoders-shout-in</link>
      <description><![CDATA[By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-clip-to-dino-visual-encoders-shout-in</guid>
    </item>
    <item>
      <title>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</link>
      <description><![CDATA[In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/show-1-marrying-pixel-and-latent-diffusion</guid>
    </item>
    <item>
      <title>Separate Anything You Describe</title>
      <link>https://paperswithcode.com/paper/separate-anything-you-describe</link>
      <description><![CDATA[In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/separate-anything-you-describe</guid>
    </item>
    <item>
      <title>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</link>
      <description><![CDATA[Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</guid>
    </item>
    <item>
      <title>GRID: A Platform for General Robot Intelligence Development</title>
      <link>https://paperswithcode.com/paper/grid-a-platform-for-general-robot</link>
      <description><![CDATA[In addition, the modular design enables various deep ML components and existing foundation models to be easily usable in a wider variety of robot-centric problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grid-a-platform-for-general-robot</guid>
    </item>
    <item>
      <title>VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</title>
      <link>https://paperswithcode.com/paper/videoretalking-audio-based-lip</link>
      <description><![CDATA[Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videoretalking-audio-based-lip</guid>
    </item>
    <item>
      <title>Large Language Models Are Zero-Shot Time Series Forecasters</title>
      <link>https://paperswithcode.com/paper/large-language-models-are-zero-shot-time</link>
      <description><![CDATA[By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-language-models-are-zero-shot-time</guid>
    </item>
    <item>
      <title>Humanoid Agents: Platform for Simulating Human-like Generative Agents</title>
      <link>https://paperswithcode.com/paper/humanoid-agents-platform-for-simulating-human</link>
      <description><![CDATA[Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/humanoid-agents-platform-for-simulating-human</guid>
    </item>
    <item>
      <title>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</title>
      <link>https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent</link>
      <description><![CDATA[Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent</guid>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <link>https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</link>
      <description><![CDATA[We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ferret-refer-and-ground-anything-anywhere-at</guid>
    </item>
    <item>
      <title>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</title>
      <link>https://paperswithcode.com/paper/octopus-embodied-vision-language-programmer</link>
      <description><![CDATA[Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/octopus-embodied-vision-language-programmer</guid>
    </item>
    <item>
      <title>A Survey on Video Diffusion Models</title>
      <link>https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</link>
      <description><![CDATA[However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</guid>
    </item>
    <item>
      <title>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</title>
      <link>https://paperswithcode.com/paper/prometheus-inducing-fine-grained-evaluation</link>
      <description><![CDATA[We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prometheus-inducing-fine-grained-evaluation</guid>
    </item>
    <item>
      <title>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title>
      <link>https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</link>
      <description><![CDATA[In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreamgaussian-generative-gaussian-splatting</guid>
    </item>
    <item>
      <title>UniPose: Detecting Any Keypoints</title>
      <link>https://paperswithcode.com/paper/unipose-detecting-any-keypoints</link>
      <description><![CDATA[This work proposes a unified framework called UniPose to detect keypoints of any articulated (e. g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unipose-detecting-any-keypoints</guid>
    </item>
    <item>
      <title>Character-LLM: A Trainable Agent for Role-Playing</title>
      <link>https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</link>
      <description><![CDATA[Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</guid>
    </item>
  </channel>
</rss>
