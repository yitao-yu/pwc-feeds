<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 21 Feb 2024 09:14:05 +0000</lastBuildDate>
    <item>
      <title>World Model on Million-Length Video And Language With RingAttention</title>
      <link>https://paperswithcode.com/paper/world-model-on-million-length-video-and</link>
      <description><![CDATA[This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/world-model-on-million-length-video-and</guid>
    </item>
    <item>
      <title>UFO: A UI-Focused Agent for Windows OS Interaction</title>
      <link>https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</link>
      <description><![CDATA[We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ufo-a-ui-focused-agent-for-windows-os</guid>
    </item>
    <item>
      <title>Revisiting Feature Prediction for Learning Visual Representations from Video</title>
      <link>https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning</link>
      <description><![CDATA[This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning</guid>
    </item>
    <item>
      <title>Scalable Diffusion Models with Transformers</title>
      <link>https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</link>
      <description><![CDATA[We explore a new class of diffusion models based on the transformer architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</guid>
    </item>
    <item>
      <title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title>
      <link>https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</link>
      <description><![CDATA[The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolo-world-real-time-open-vocabulary-object</guid>
    </item>
    <item>
      <title>DoRA: Weight-Decomposed Low-Rank Adaptation</title>
      <link>https://paperswithcode.com/paper/dora-weight-decomposed-low-rank-adaptation</link>
      <description><![CDATA[By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dora-weight-decomposed-low-rank-adaptation</guid>
    </item>
    <item>
      <title>DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows</title>
      <link>https://paperswithcode.com/paper/datadreamer-a-tool-for-synthetic-data</link>
      <description><![CDATA[The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datadreamer-a-tool-for-synthetic-data</guid>
    </item>
    <item>
      <title>OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning</title>
      <link>https://paperswithcode.com/paper/openfedllm-training-large-language-models-on</link>
      <description><![CDATA[Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openfedllm-training-large-language-models-on</guid>
    </item>
    <item>
      <title>Generative Representational Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/generative-representational-instruction</link>
      <description><![CDATA[Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-representational-instruction</guid>
    </item>
    <item>
      <title>GraphCast: Learning skillful medium-range global weather forecasting</title>
      <link>https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</link>
      <description><![CDATA[Global medium-range weather forecasting is critical to decision-making across many social and economic domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graphcast-learning-skillful-medium-range</guid>
    </item>
    <item>
      <title>MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion</title>
      <link>https://paperswithcode.com/paper/magicdance-realistic-human-dance-video</link>
      <description><![CDATA[In this work, we propose MagicPose, a diffusion-based model for 2D human pose and facial expression retargeting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magicdance-realistic-human-dance-video</guid>
    </item>
    <item>
      <title>BitDelta: Your Fine-Tune May Only Be Worth One Bit</title>
      <link>https://paperswithcode.com/paper/bitdelta-your-fine-tune-may-only-be-worth-one</link>
      <description><![CDATA[Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bitdelta-your-fine-tune-may-only-be-worth-one</guid>
    </item>
    <item>
      <title>Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time</link>
      <description><![CDATA[Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time</guid>
    </item>
    <item>
      <title>SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers</title>
      <link>https://paperswithcode.com/paper/sit-exploring-flow-and-diffusion-based</link>
      <description><![CDATA[We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sit-exploring-flow-and-diffusion-based</guid>
    </item>
    <item>
      <title>MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask</title>
      <link>https://paperswithcode.com/paper/masknet-introducing-feature-wise</link>
      <description><![CDATA[We also turn the feed-forward layer in DNN model into a mixture of addictive and multiplicative feature interactions by proposing MaskBlock in this paper.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/masknet-introducing-feature-wise</guid>
    </item>
    <item>
      <title>Magic-Me: Identity-Specific Video Customized Diffusion</title>
      <link>https://paperswithcode.com/paper/magic-me-identity-specific-video-customized</link>
      <description><![CDATA[In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magic-me-identity-specific-video-customized</guid>
    </item>
    <item>
      <title>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</title>
      <link>https://paperswithcode.com/paper/motionctrl-a-unified-and-flexible-motion</link>
      <description><![CDATA[Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/motionctrl-a-unified-and-flexible-motion</guid>
    </item>
    <item>
      <title>Guiding Instruction-based Image Editing via Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/guiding-instruction-based-image-editing-via</link>
      <description><![CDATA[Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/guiding-instruction-based-image-editing-via</guid>
    </item>
    <item>
      <title>Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception</title>
      <link>https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</link>
      <description><![CDATA[To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobile-agent-autonomous-multi-modal-mobile</guid>
    </item>
    <item>
      <title>Ring Attention with Blockwise Transformers for Near-Infinite Context</title>
      <link>https://paperswithcode.com/paper/ring-attention-with-blockwise-transformers</link>
      <description><![CDATA[Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ring-attention-with-blockwise-transformers</guid>
    </item>
  </channel>
</rss>
