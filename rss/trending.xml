<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 13 Jan 2024 21:06:21 +0000</lastBuildDate>
    <item>
      <title>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</title>
      <link>https://paperswithcode.com/paper/open-vocabulary-sam-segment-and-recognize</link>
      <description><![CDATA[The CLIP and Segment Anything Model (SAM) are remarkable vision foundation models (VFMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/open-vocabulary-sam-segment-and-recognize</guid>
    </item>
    <item>
      <title>WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia</title>
      <link>https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</link>
      <description><![CDATA[WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wikichat-a-few-shot-llm-based-chatbot</guid>
    </item>
    <item>
      <title>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</title>
      <link>https://paperswithcode.com/paper/from-audio-to-photoreal-embodiment</link>
      <description><![CDATA[We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-audio-to-photoreal-embodiment</guid>
    </item>
    <item>
      <title>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</title>
      <link>https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</link>
      <description><![CDATA[Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-are-super-mario-absorbing</guid>
    </item>
    <item>
      <title>Retrieval-Augmented Generation for Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</link>
      <description><![CDATA[Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/retrieval-augmented-generation-for-large</guid>
    </item>
    <item>
      <title>BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</title>
      <link>https://paperswithcode.com/paper/bakedavatar-baking-neural-fields-for-real</link>
      <description><![CDATA[Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bakedavatar-baking-neural-fields-for-real</guid>
    </item>
    <item>
      <title>DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</title>
      <link>https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert</link>
      <description><![CDATA[Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert</guid>
    </item>
    <item>
      <title>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</title>
      <link>https://paperswithcode.com/paper/llm-maybe-longlm-self-extend-llm-context</link>
      <description><![CDATA[In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llm-maybe-longlm-self-extend-llm-context</guid>
    </item>
    <item>
      <title>AnyText: Multilingual Visual Text Generation And Editing</title>
      <link>https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</link>
      <description><![CDATA[Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/anytext-multilingual-visual-text-generation</guid>
    </item>
    <item>
      <title>TrustLLM: Trustworthiness in Large Language Models</title>
      <link>https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language</link>
      <description><![CDATA[This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language</guid>
    </item>
    <item>
      <title>LEGO:Language Enhanced Multi-modal Grounding Model</title>
      <link>https://paperswithcode.com/paper/lego-language-enhanced-multi-modal-grounding</link>
      <description><![CDATA[Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lego-language-enhanced-multi-modal-grounding</guid>
    </item>
    <item>
      <title>OpenVoice: Versatile Instant Voice Cloning</title>
      <link>https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</link>
      <description><![CDATA[The voice styles are not directly copied from and constrained by the style of the reference speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning</guid>
    </item>
    <item>
      <title>Fast Inference of Mixture-of-Experts Language Models with Offloading</title>
      <link>https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</link>
      <description><![CDATA[In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language</guid>
    </item>
    <item>
      <title>Instruction Tuning with Human Curriculum</title>
      <link>https://paperswithcode.com/paper/instruction-tuning-with-human-curriculum</link>
      <description><![CDATA[The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instruction-tuning-with-human-curriculum</guid>
    </item>
    <item>
      <title>Mistral 7B</title>
      <link>https://paperswithcode.com/paper/mistral-7b</link>
      <description><![CDATA[We introduce Mistral 7B v0. 1, a 7-billion-parameter language model engineered for superior performance and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mistral-7b</guid>
    </item>
    <item>
      <title>Improving the Stability of Diffusion Models for Content Consistent Super-Resolution</title>
      <link>https://paperswithcode.com/paper/improving-the-stability-of-diffusion-models</link>
      <description><![CDATA[To improve the stability of diffusion prior-based SR, we propose to employ the diffusion models to refine image structures, while employing the generative adversarial training to enhance image fine details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-the-stability-of-diffusion-models</guid>
    </item>
    <item>
      <title>Machine Mindset: An MBTI Exploration of Large Language Models</title>
      <link>https://paperswithcode.com/paper/machine-mindset-an-mbti-exploration-of-large</link>
      <description><![CDATA[We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/machine-mindset-an-mbti-exploration-of-large</guid>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <link>https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</link>
      <description><![CDATA[Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mamba-linear-time-sequence-modeling-with</guid>
    </item>
    <item>
      <title>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</title>
      <link>https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</link>
      <description><![CDATA[To address this problem, we leverage diffusion models trained on billions of standard images to render a chrome ball into the input image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusionlight-light-probes-for-free-by</guid>
    </item>
    <item>
      <title>FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content</title>
      <link>https://paperswithcode.com/paper/facechain-a-playground-for-identity</link>
      <description><![CDATA[In this paper, we present FaceChain, a personalized portrait generation framework that combines a series of customized image-generation model and a rich set of face-related perceptual understanding models (\eg, face detection, deep face embedding extraction, and facial attribute recognition), to tackle aforementioned challenges and to generate truthful personalized portraits, with only a handful of portrait images as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/facechain-a-playground-for-identity</guid>
    </item>
  </channel>
</rss>
