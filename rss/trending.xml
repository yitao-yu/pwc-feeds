<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 19 Jun 2025 21:09:58 +0000</lastBuildDate>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>TradingAgents: Multi-Agents LLM Financial Trading Framework</title>
      <link>https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</link>
      <description><![CDATA[Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tradingagents-multi-agents-llm-financial</guid>
    </item>
    <item>
      <title>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning</title>
      <link>https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</link>
      <description><![CDATA[Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/v-jepa-2-self-supervised-video-models-enable</guid>
    </item>
    <item>
      <title>Efficient Part-level 3D Object Generation via Dual Volume Packing</title>
      <link>https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</link>
      <description><![CDATA[Recent progress in 3D object generation has greatly improved both the quality and efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-part-level-3d-object-generation-via</guid>
    </item>
    <item>
      <title>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</title>
      <link>https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</link>
      <description><![CDATA[Drawing inspiration from the orchestration of diverse specialized agents collaborating to tackle intricate tasks, we propose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that harnesses multi-agent collaboration to bolster the faithfulness and causality of foundation models, involving a set of reasoners and evaluators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-causalgpt-a-multi-agent-approach-for</guid>
    </item>
    <item>
      <title>MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</title>
      <link>https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</link>
      <description><![CDATA[To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-embodied-interactive-agent-for</guid>
    </item>
    <item>
      <title>VGGT: Visual Geometry Grounded Transformer</title>
      <link>https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</link>
      <description><![CDATA[We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</guid>
    </item>
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://paperswithcode.com/paper/ming-omni-a-unified-multimodal-model-for</link>
      <description><![CDATA[We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ming-omni-a-unified-multimodal-model-for</guid>
    </item>
    <item>
      <title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
      <link>https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</link>
      <description><![CDATA[To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-kv-redundancy-aware-kv-cache-compression</guid>
    </item>
    <item>
      <title>OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems</title>
      <link>https://paperswithcode.com/paper/opt-bench-evaluating-llm-agent-on-large-scale</link>
      <description><![CDATA[Large Language Models (LLMs) have shown remarkable capabilities in solving diverse tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opt-bench-evaluating-llm-agent-on-large-scale</guid>
    </item>
    <item>
      <title>MUSt3R: Multi-view Network for Stereo 3D Reconstruction</title>
      <link>https://paperswithcode.com/paper/must3r-multi-view-network-for-stereo-3d</link>
      <description><![CDATA[DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/must3r-multi-view-network-for-stereo-3d</guid>
    </item>
    <item>
      <title>Practical Efficiency of Muon for Pretraining</title>
      <link>https://paperswithcode.com/paper/practical-efficiency-of-muon-for-pretraining</link>
      <description><![CDATA[We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/practical-efficiency-of-muon-for-pretraining</guid>
    </item>
    <item>
      <title>ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering</title>
      <link>https://paperswithcode.com/paper/ale-bench-a-benchmark-for-long-horizon</link>
      <description><![CDATA[How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ale-bench-a-benchmark-for-long-horizon</guid>
    </item>
    <item>
      <title>MAGREF: Masked Guidance for Any-Reference Video Generation</title>
      <link>https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</link>
      <description><![CDATA[Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magref-masked-guidance-for-any-reference</guid>
    </item>
    <item>
      <title>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</title>
      <link>https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</link>
      <description><![CDATA[Audio-driven human animation methods, such as talking head and talking body generation, have made remarkable progress in generating synchronized facial movements and appealing visual quality videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/let-them-talk-audio-driven-multi-person</guid>
    </item>
    <item>
      <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</link>
      <description><![CDATA[This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</guid>
    </item>
    <item>
      <title>MagCache: Fast Video Generation with Magnitude-Aware Cache</title>
      <link>https://paperswithcode.com/paper/magcache-fast-video-generation-with-magnitude</link>
      <description><![CDATA[Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magcache-fast-video-generation-with-magnitude</guid>
    </item>
    <item>
      <title>Logits-Based Finetuning</title>
      <link>https://paperswithcode.com/paper/logits-based-finetuning</link>
      <description><![CDATA[We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/logits-based-finetuning</guid>
    </item>
    <item>
      <title>AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning</title>
      <link>https://paperswithcode.com/paper/autovla-a-vision-language-action-model-for</link>
      <description><![CDATA[Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autovla-a-vision-language-action-model-for</guid>
    </item>
    <item>
      <title>Benchmarking Laparoscopic Surgical Image Restoration and Beyond</title>
      <link>https://paperswithcode.com/paper/benchmarking-laparoscopic-surgical-image</link>
      <description><![CDATA[To systematically investigate and address various forms of surgical scene degradation, we introduce a real-world open-source surgical image restoration dataset covering laparoscopic environments, called SurgClean, which involves multi-type image restoration tasks, e. g., desmoking, defogging, and desplashing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/benchmarking-laparoscopic-surgical-image</guid>
    </item>
  </channel>
</rss>
