<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 31 Jan 2024 09:11:33 +0000</lastBuildDate>
    <item>
      <title>InstantID: Zero-shot Identity-Preserving Generation in Seconds</title>
      <link>https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</link>
      <description><![CDATA[There has been significant progress in personalized image synthesis with methods such as Textual Inversion, DreamBooth, and LoRA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instantid-zero-shot-identity-preserving</guid>
    </item>
    <item>
      <title>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</title>
      <link>https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</link>
      <description><![CDATA[Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slicegpt-compress-large-language-models-by</guid>
    </item>
    <item>
      <title>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</link>
      <description><![CDATA[In this work, we propose a novel training strategy MoE-tuning for LVLMs, which can constructing a sparse model with an outrageous number of parameter but a constant computational cost, and effectively addresses the performance degradation typically associated with multi-modal learning and model sparsity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moe-llava-mixture-of-experts-for-large-vision</guid>
    </item>
    <item>
      <title>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</title>
      <link>https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</link>
      <description><![CDATA[To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-anything-unleashing-the-power-of-large</guid>
    </item>
    <item>
      <title>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</title>
      <link>https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</link>
      <description><![CDATA[Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/code-generation-with-alphacodium-from-prompt</guid>
    </item>
    <item>
      <title>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/mastering-text-to-image-diffusion</link>
      <description><![CDATA[In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mastering-text-to-image-diffusion</guid>
    </item>
    <item>
      <title>Matryoshka Representation Learning</title>
      <link>https://paperswithcode.com/paper/matryoshka-representations-for-adaptive</link>
      <description><![CDATA[The flexibility within the learned Matryoshka Representations offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matryoshka-representations-for-adaptive</guid>
    </item>
    <item>
      <title>In-Context Learning for Extreme Multi-Label Classification</title>
      <link>https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</link>
      <description><![CDATA[Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-for-extreme-multi-label</guid>
    </item>
    <item>
      <title>Benchmarking LLMs via Uncertainty Quantification</title>
      <link>https://paperswithcode.com/paper/benchmarking-llms-via-uncertainty</link>
      <description><![CDATA[The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/benchmarking-llms-via-uncertainty</guid>
    </item>
    <item>
      <title>Flexibly Scaling Large Language Models Contexts Through Extensible Tokenization</title>
      <link>https://paperswithcode.com/paper/flexibly-scaling-large-language-models</link>
      <description><![CDATA[Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flexibly-scaling-large-language-models</guid>
    </item>
    <item>
      <title>Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks</title>
      <link>https://paperswithcode.com/paper/bag-of-tricks-for-long-tailed-visual</link>
      <description><![CDATA[In recent years, visual recognition on challenging long-tailed distributions, where classes often exhibit extremely imbalanced frequencies, has made great progress mostly based on various complex paradigms (e. g., meta learning).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bag-of-tricks-for-long-tailed-visual</guid>
    </item>
    <item>
      <title>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</title>
      <link>https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation</link>
      <description><![CDATA[The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation</guid>
    </item>
    <item>
      <title>Self-Rewarding Language Models</title>
      <link>https://paperswithcode.com/paper/self-rewarding-language-models</link>
      <description><![CDATA[We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-rewarding-language-models</guid>
    </item>
    <item>
      <title>VMamba: Visual State Space Model</title>
      <link>https://paperswithcode.com/paper/vmamba-visual-state-space-model</link>
      <description><![CDATA[Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vmamba-visual-state-space-model</guid>
    </item>
    <item>
      <title>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively</title>
      <link>https://paperswithcode.com/paper/open-vocabulary-sam-segment-and-recognize</link>
      <description><![CDATA[The CLIP and Segment Anything Model (SAM) are remarkable vision foundation models (VFMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/open-vocabulary-sam-segment-and-recognize</guid>
    </item>
    <item>
      <title>Hawkeye: A PyTorch-based Library for Fine-Grained Image Recognition with Deep Learning</title>
      <link>https://paperswithcode.com/paper/hawkeye-a-pytorch-based-library-for-fine</link>
      <description><![CDATA[However, the absence of a unified open-source software library covering various paradigms in FGIR poses a significant challenge for researchers and practitioners in the field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hawkeye-a-pytorch-based-library-for-fine</guid>
    </item>
    <item>
      <title>AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents</title>
      <link>https://paperswithcode.com/paper/agentboard-an-analytical-evaluation-board-of</link>
      <description><![CDATA[Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentboard-an-analytical-evaluation-board-of</guid>
    </item>
    <item>
      <title>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</title>
      <link>https://paperswithcode.com/paper/dspy-compiling-declarative-language-model</link>
      <description><![CDATA[The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dspy-compiling-declarative-language-model</guid>
    </item>
    <item>
      <title>LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model</title>
      <link>https://paperswithcode.com/paper/llava-ph-efficient-multi-modal-assistant-with</link>
      <description><![CDATA[In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-ph-efficient-multi-modal-assistant-with</guid>
    </item>
    <item>
      <title>DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines</title>
      <link>https://paperswithcode.com/paper/dspy-assertions-computational-constraints-for</link>
      <description><![CDATA[Our reference implementation of LM Assertions is integrated into DSPy at https://github. com/stanfordnlp/dspy]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dspy-assertions-computational-constraints-for</guid>
    </item>
  </channel>
</rss>
