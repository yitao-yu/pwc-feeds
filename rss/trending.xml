<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 12 Nov 2022 09:15:30 +0000</lastBuildDate>
    <item>
      <title>TAP-Vid: A Benchmark for Tracking Any Point in a Video</title>
      <link>https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</link>
      <description><![CDATA[Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tap-vid-a-benchmark-for-tracking-any-point-in</guid>
    </item>
    <item>
      <title>Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training</title>
      <link>https://paperswithcode.com/paper/colossal-ai-a-unified-deep-learning-system</link>
      <description><![CDATA[The success of Transformer models has pushed the deep learning model scale to billions of parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/colossal-ai-a-unified-deep-learning-system</guid>
    </item>
    <item>
      <title>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</title>
      <link>https://paperswithcode.com/paper/internimage-exploring-large-scale-vision</link>
      <description><![CDATA[Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internimage-exploring-large-scale-vision</guid>
    </item>
    <item>
      <title>Unifying Flow, Stereo and Depth Estimation</title>
      <link>https://paperswithcode.com/paper/unifying-flow-stereo-and-depth-estimation</link>
      <description><![CDATA[We present a unified formulation and model for three motion and 3D perception tasks: optical flow, rectified stereo matching and unrectified stereo depth estimation from posed images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unifying-flow-stereo-and-depth-estimation</guid>
    </item>
    <item>
      <title>PhaseAug: A Differentiable Augmentation for Speech Synthesis to Simulate One-to-Many Mapping</title>
      <link>https://paperswithcode.com/paper/phaseaug-a-differentiable-augmentation-for</link>
      <description><![CDATA[Previous generative adversarial network (GAN)-based neural vocoders are trained to reconstruct the exact ground truth waveform from the paired mel-spectrogram and do not consider the one-to-many relationship of speech synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/phaseaug-a-differentiable-augmentation-for</guid>
    </item>
    <item>
      <title>StyleNAT: Giving Each Head a New Perspective</title>
      <link>https://paperswithcode.com/paper/stylenat-giving-each-head-a-new-perspective</link>
      <description><![CDATA[Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stylenat-giving-each-head-a-new-perspective</guid>
    </item>
    <item>
      <title>Real-Time Target Sound Extraction</title>
      <link>https://paperswithcode.com/paper/real-time-target-sound-extraction</link>
      <description><![CDATA[We present the first neural network model to achieve real-time and streaming target sound extraction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-time-target-sound-extraction</guid>
    </item>
    <item>
      <title>OneFlow: Redesign the Distributed Deep Learning Framework from Scratch</title>
      <link>https://paperswithcode.com/paper/oneflow-redesign-the-distributed-deep</link>
      <description><![CDATA[Aiming at a simple, neat redesign of distributed deep learning frameworks for various parallelism paradigms, we present OneFlow, a novel distributed training framework based on an SBP (split, broadcast and partial-value) abstraction and the actor model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/oneflow-redesign-the-distributed-deep</guid>
    </item>
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</link>
      <description><![CDATA[The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</guid>
    </item>
    <item>
      <title>MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation</title>
      <link>https://paperswithcode.com/paper/mmdialog-a-large-scale-multi-turn-dialogue</link>
      <description><![CDATA[First, it is the largest multi-modal conversation dataset by the number of dialogues by 8x.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmdialog-a-large-scale-multi-turn-dialogue</guid>
    </item>
    <item>
      <title>High Fidelity Neural Audio Compression</title>
      <link>https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</link>
      <description><![CDATA[We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-fidelity-neural-audio-compression</guid>
    </item>
    <item>
      <title>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</title>
      <link>https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</link>
      <description><![CDATA[In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</guid>
    </item>
    <item>
      <title>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</title>
      <link>https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</link>
      <description><![CDATA[Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a</guid>
    </item>
    <item>
      <title>Focal Modulation Networks</title>
      <link>https://paperswithcode.com/paper/focal-modulation-networks</link>
      <description><![CDATA[For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2. 4, and beats Swin at multi-scale (50. 5 v. s.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/focal-modulation-networks</guid>
    </item>
    <item>
      <title>Example-Based Named Entity Recognition</title>
      <link>https://paperswithcode.com/paper/example-based-named-entity-recognition</link>
      <description><![CDATA[We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/example-based-named-entity-recognition</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Demystify Transformers &amp; Convolutions in Modern Image Deep Networks</title>
      <link>https://paperswithcode.com/paper/demystify-transformers-convolutions-in-modern</link>
      <description><![CDATA[Although the novel feature transformation designs are often claimed as the source of gain, some backbones may benefit from advanced engineering techniques, which makes it hard to identify the real gain from the key feature transformation operators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demystify-transformers-convolutions-in-modern</guid>
    </item>
    <item>
      <title>Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective</title>
      <link>https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</link>
      <description><![CDATA[We propose a new paradigm for zero-shot learners that is format agnostic, i. e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-learners-for-natural-language</guid>
    </item>
    <item>
      <title>Pop2Piano : Pop Audio-based Piano Cover Generation</title>
      <link>https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</link>
      <description><![CDATA[The piano cover of pop music is widely enjoyed by people.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pop2piano-pop-audio-based-piano-cover</guid>
    </item>
    <item>
      <title>Revisiting Sparse Convolutional Model for Visual Recognition</title>
      <link>https://paperswithcode.com/paper/revisiting-sparse-convolutional-model-for</link>
      <description><![CDATA[We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100, and ImageNet datasets when compared to conventional neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-sparse-convolutional-model-for</guid>
    </item>
  </channel>
</rss>
