<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 16 Jun 2024 09:14:49 +0000</lastBuildDate>
    <item>
      <title>Scalable MatMul-free Language Modeling</title>
      <link>https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</link>
      <description><![CDATA[Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2. 7B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-matmul-free-language-modeling</guid>
    </item>
    <item>
      <title>Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</title>
      <link>https://paperswithcode.com/paper/autoregressive-model-beats-diffusion-llama</link>
      <description><![CDATA[(3) A text-conditional image generation model with 775M parameters, from two-stage training on LAION-COCO and high aesthetics quality images, demonstrating competitive performance of visual quality and text alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoregressive-model-beats-diffusion-llama</guid>
    </item>
    <item>
      <title>"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</title>
      <link>https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating</link>
      <description><![CDATA[We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-anything-now-characterizing-and-evaluating</guid>
    </item>
    <item>
      <title>TextGrad: Automatic "Differentiation" via Text</title>
      <link>https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</link>
      <description><![CDATA[Without modifying the framework, TextGrad improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to $55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textgrad-automatic-differentiation-via-text</guid>
    </item>
    <item>
      <title>Matching Anything by Segmenting Anything</title>
      <link>https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</link>
      <description><![CDATA[The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matching-anything-by-segmenting-anything</guid>
    </item>
    <item>
      <title>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</title>
      <link>https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</link>
      <description><![CDATA[Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-lora-mixture-of-low-rank-adapter-experts-a</guid>
    </item>
    <item>
      <title>Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness</title>
      <link>https://paperswithcode.com/paper/less-is-more-removing-text-regions-improves</link>
      <description><![CDATA[In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/less-is-more-removing-text-regions-improves</guid>
    </item>
    <item>
      <title>Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training</title>
      <link>https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy</link>
      <description><![CDATA[In this work, we revisit the settings by adopting step time as a more accurate measure of model complexity, and by determining the total compute budget under the Chinchilla compute-optimal settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-moe-and-dense-speed-accuracy</guid>
    </item>
    <item>
      <title>LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning</title>
      <link>https://paperswithcode.com/paper/libritts-p-a-corpus-with-speaking-style-and</link>
      <description><![CDATA[We employ a hybrid approach to construct prompt annotations: (1) manual annotations that capture human perceptions of speaker characteristics and (2) synthetic annotations on speaking style.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/libritts-p-a-corpus-with-speaking-style-and</guid>
    </item>
    <item>
      <title>OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</title>
      <link>https://paperswithcode.com/paper/omnicorpus-an-unified-multimodal-corpus-of-10</link>
      <description><![CDATA[In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnicorpus-an-unified-multimodal-corpus-of-10</guid>
    </item>
    <item>
      <title>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</title>
      <link>https://paperswithcode.com/paper/buffer-of-thoughts-thought-augmented</link>
      <description><![CDATA[We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/buffer-of-thoughts-thought-augmented</guid>
    </item>
    <item>
      <title>Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning</title>
      <link>https://paperswithcode.com/paper/husky-a-unified-open-source-language-agent</link>
      <description><![CDATA[Despite using 7B models, Husky matches or even exceeds frontier LMs such as GPT-4 on these tasks, showcasing the efficacy of our holistic approach in addressing complex reasoning problems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/husky-a-unified-open-source-language-agent</guid>
    </item>
    <item>
      <title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title>
      <link>https://paperswithcode.com/paper/videollama-2-advancing-spatial-temporal</link>
      <description><![CDATA[In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videollama-2-advancing-spatial-temporal</guid>
    </item>
    <item>
      <title>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</title>
      <link>https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</link>
      <description><![CDATA[In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unique3d-high-quality-and-efficient-3d-mesh</guid>
    </item>
    <item>
      <title>Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis</title>
      <link>https://paperswithcode.com/paper/lighting-every-darkness-with-3dgs-fast</link>
      <description><![CDATA[Volumetric rendering based methods, like NeRF, excel in HDR view synthesis from RAWimages, especially for nighttime scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lighting-every-darkness-with-3dgs-fast</guid>
    </item>
    <item>
      <title>AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising</title>
      <link>https://paperswithcode.com/paper/asyncdiff-parallelizing-diffusion-models-by</link>
      <description><![CDATA[To address this, we introduce AsyncDiff, a universal and plug-and-play acceleration scheme that enables model parallelism across multiple devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/asyncdiff-parallelizing-diffusion-models-by</guid>
    </item>
    <item>
      <title>Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</link>
      <description><![CDATA[Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lumina-t2x-transforming-text-into-any</guid>
    </item>
    <item>
      <title>Seed-TTS: A Family of High-Quality Versatile Speech Generation Models</title>
      <link>https://paperswithcode.com/paper/seed-tts-a-family-of-high-quality-versatile</link>
      <description><![CDATA[Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-tts-a-family-of-high-quality-versatile</guid>
    </item>
    <item>
      <title>Mathematical Supplement for the $\texttt{gsplat}$ Library</title>
      <link>https://paperswithcode.com/paper/mathematical-supplement-for-the-texttt-gsplat</link>
      <description><![CDATA[This report provides the mathematical details of the gsplat library, a modular toolbox for efficient differentiable Gaussian splatting, as proposed by Kerbl et al.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mathematical-supplement-for-the-texttt-gsplat</guid>
    </item>
    <item>
      <title>Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions</title>
      <link>https://paperswithcode.com/paper/image-textualization-an-automatic-framework</link>
      <description><![CDATA[Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/image-textualization-an-automatic-framework</guid>
    </item>
  </channel>
</rss>
