<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 09 Jul 2024 09:14:16 +0000</lastBuildDate>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>VILA: On Pre-training for Visual Language Models</title>
      <link>https://paperswithcode.com/paper/vila-on-pre-training-for-visual-language</link>
      <description><![CDATA[Visual language models (VLMs) rapidly progressed with the recent success of large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vila-on-pre-training-for-visual-language</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>Agentless: Demystifying LLM-based Software Engineering Agents</title>
      <link>https://paperswithcode.com/paper/agentless-demystifying-llm-based-software</link>
      <description><![CDATA[However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentless-demystifying-llm-based-software</guid>
    </item>
    <item>
      <title>Segment Anything without Supervision</title>
      <link>https://paperswithcode.com/paper/segment-anything-without-supervision</link>
      <description><![CDATA[By integrating our unsupervised pseudo masks into SA-1B's ground-truth masks and training UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM's AR by over 6. 7% and AP by 3. 9% on SA-1B.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/segment-anything-without-supervision</guid>
    </item>
    <item>
      <title>Scaling Synthetic Data Creation with 1,000,000,000 Personas</title>
      <link>https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</link>
      <description><![CDATA[We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-synthetic-data-creation-with</guid>
    </item>
    <item>
      <title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title>
      <link>https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</link>
      <description><![CDATA[With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</guid>
    </item>
    <item>
      <title>Measuring Taiwanese Mandarin Language Understanding</title>
      <link>https://paperswithcode.com/paper/measuring-taiwanese-mandarin-language</link>
      <description><![CDATA[We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/measuring-taiwanese-mandarin-language</guid>
    </item>
    <item>
      <title>Self-Play Preference Optimization for Language Model Alignment</title>
      <link>https://paperswithcode.com/paper/self-play-preference-optimization-for</link>
      <description><![CDATA[Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-play-preference-optimization-for</guid>
    </item>
    <item>
      <title>LLaRA: Supercharging Robot Learning Data for Vision-Language Policy</title>
      <link>https://paperswithcode.com/paper/llara-supercharging-robot-learning-data-for</link>
      <description><![CDATA[Large Language Models (LLMs) equipped with extensive world knowledge and strong reasoning skills can tackle diverse tasks across domains, often by posing them as conversation-style instruction-response pairs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llara-supercharging-robot-learning-data-for</guid>
    </item>
    <item>
      <title>TokenPacker: Efficient Visual Projector for Multimodal LLM</title>
      <link>https://paperswithcode.com/paper/tokenpacker-efficient-visual-projector-for</link>
      <description><![CDATA[However, the visual tokens are redundant and can be considerably increased when dealing with high-resolution images, impairing the efficiency of MLLMs significantly.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tokenpacker-efficient-visual-projector-for</guid>
    </item>
    <item>
      <title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</title>
      <link>https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</link>
      <description><![CDATA[In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sharegpt4v-improving-large-multi-modal-models</guid>
    </item>
    <item>
      <title>InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</title>
      <link>https://paperswithcode.com/paper/internlm-xcomposer-2-5-a-versatile-large</link>
      <description><![CDATA[This long-context capability allows IXC-2. 5 to excel in tasks requiring extensive input and output contexts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internlm-xcomposer-2-5-a-versatile-large</guid>
    </item>
    <item>
      <title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title>
      <link>https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</link>
      <description><![CDATA[Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meshanything-artist-created-mesh-generation</guid>
    </item>
    <item>
      <title>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</link>
      <description><![CDATA[We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</guid>
    </item>
    <item>
      <title>Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</title>
      <link>https://paperswithcode.com/paper/mooncake-a-kvcache-centric-disaggregated</link>
      <description><![CDATA[Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mooncake-a-kvcache-centric-disaggregated</guid>
    </item>
    <item>
      <title>EvTexture: Event-driven Texture Enhancement for Video Super-Resolution</title>
      <link>https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</link>
      <description><![CDATA[Our method, called EvTexture, leverages high-frequency details of events to better recover texture regions in VSR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evtexture-event-driven-texture-enhancement</guid>
    </item>
    <item>
      <title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</link>
      <description><![CDATA[We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cambrian-1-a-fully-open-vision-centric</guid>
    </item>
    <item>
      <title>Consistency Flow Matching: Defining Straight Flows with Velocity Consistency</title>
      <link>https://paperswithcode.com/paper/consistency-flow-matching-defining-straight</link>
      <description><![CDATA[Additionally, we propose a multi-segment training approach for Consistency-FM to enhance expressiveness, achieving a better trade-off between sampling quality and speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/consistency-flow-matching-defining-straight</guid>
    </item>
    <item>
      <title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title>
      <link>https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</link>
      <description><![CDATA[In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</guid>
    </item>
  </channel>
</rss>
