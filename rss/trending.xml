<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 18 Jul 2024 21:07:48 +0000</lastBuildDate>
    <item>
      <title>SEED-Story: Multimodal Long Story Generation with Large Language Model</title>
      <link>https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</link>
      <description><![CDATA[We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seed-story-multimodal-long-story-generation</guid>
    </item>
    <item>
      <title>Grounding Image Matching in 3D with MASt3R</title>
      <link>https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</link>
      <description><![CDATA[Image Matching is a core component of all best-performing algorithms and pipelines in 3D vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-image-matching-in-3d-with-mast3r</guid>
    </item>
    <item>
      <title>MambaVision: A Hybrid Mamba-Transformer Vision Backbone</title>
      <link>https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</link>
      <description><![CDATA[We propose a novel hybrid Mamba-Transformer backbone, denoted as MambaVision, which is specifically tailored for vision applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mambavision-a-hybrid-mamba-transformer-vision</guid>
    </item>
    <item>
      <title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title>
      <link>https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</link>
      <description><![CDATA[We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assisting-in-writing-wikipedia-like-articles</guid>
    </item>
    <item>
      <title>Cradle: Empowering Foundation Agents Towards General Computer Control</title>
      <link>https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</link>
      <description><![CDATA[To handle this issue, we propose the General Computer Control (GCC) setting to restrict foundation agents to interact with software through the most unified and standardized interface, i. e., using screenshots as input and keyboard and mouse actions as output.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-general-computer-control-a-multimodal</guid>
    </item>
    <item>
      <title>FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs</title>
      <link>https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</link>
      <description><![CDATA[This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/funaudiollm-voice-understanding-and</guid>
    </item>
    <item>
      <title>Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</title>
      <link>https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</link>
      <description><![CDATA[The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/internet-of-agents-weaving-a-web-of</guid>
    </item>
    <item>
      <title>RouteLLM: Learning to Route LLMs with Preference Data</title>
      <link>https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</link>
      <description><![CDATA[Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/routellm-learning-to-route-llms-with</guid>
    </item>
    <item>
      <title>OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training</title>
      <link>https://paperswithcode.com/paper/opendiloco-an-open-source-framework-for</link>
      <description><![CDATA[OpenDiLoCo is an open-source implementation and replication of the Distributed Low-Communication (DiLoCo) training method for large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opendiloco-an-open-source-framework-for</guid>
    </item>
    <item>
      <title>LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control</title>
      <link>https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</link>
      <description><![CDATA[Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liveportrait-efficient-portrait-animation</guid>
    </item>
    <item>
      <title>Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients</title>
      <link>https://paperswithcode.com/paper/q-galore-quantized-galore-with-int4</link>
      <description><![CDATA[To address these limitations, we introduce Q-Galore, a novel approach that substantially reduces memory usage by combining quantization and low-rank projection, surpassing the benefits of GaLore.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/q-galore-quantized-galore-with-int4</guid>
    </item>
    <item>
      <title>Video Diffusion Alignment via Reward Gradients</title>
      <link>https://paperswithcode.com/paper/video-diffusion-alignment-via-reward</link>
      <description><![CDATA[We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment of the video diffusion model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/video-diffusion-alignment-via-reward</guid>
    </item>
    <item>
      <title>On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards</title>
      <link>https://paperswithcode.com/paper/on-the-workflows-and-smells-of-leaderboard</link>
      <description><![CDATA[We then identify 8 unique types of leaderboard smells in LBOps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-workflows-and-smells-of-leaderboard</guid>
    </item>
    <item>
      <title>GIM: A Million-scale Benchmark for Generative Image Manipulation Detection and Localization</title>
      <link>https://paperswithcode.com/paper/gim-a-million-scale-benchmark-for-generative</link>
      <description><![CDATA[The extraordinary ability of generative models emerges as a new trend in image editing and generating realistic images, posing a serious threat to the trustworthiness of multimedia data and driving the research of image manipulation detection and location(IMDL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gim-a-million-scale-benchmark-for-generative</guid>
    </item>
    <item>
      <title>Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews</title>
      <link>https://paperswithcode.com/paper/take-the-atrain-introducing-an-interface-for</link>
      <description><![CDATA[If an entry-level graphics card is available, the transcription speed increases to 20% of the audio duration.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/take-the-atrain-introducing-an-interface-for</guid>
    </item>
    <item>
      <title>MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</title>
      <link>https://paperswithcode.com/paper/mobilellm-optimizing-sub-billion-parameter</link>
      <description><![CDATA[The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0. 7%/0. 8% than MobileLLM 125M/350M.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilellm-optimizing-sub-billion-parameter</guid>
    </item>
    <item>
      <title>A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights</title>
      <link>https://paperswithcode.com/paper/a-comprehensive-survey-on-human-video</link>
      <description><![CDATA[The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-comprehensive-survey-on-human-video</guid>
    </item>
    <item>
      <title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title>
      <link>https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</link>
      <description><![CDATA[With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minference-1-0-accelerating-pre-filling-for</guid>
    </item>
    <item>
      <title>CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds</title>
      <link>https://paperswithcode.com/paper/corrnet3d-unsupervised-end-to-end-learning-of</link>
      <description><![CDATA[The symmetric deformer, with an additional regularized loss, transforms the two permuted point clouds to each other to drive the unsupervised learning of the correspondence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/corrnet3d-unsupervised-end-to-end-learning-of</guid>
    </item>
    <item>
      <title>MAVIS: Mathematical Visual Instruction Tuning</title>
      <link>https://paperswithcode.com/paper/mavis-mathematical-visual-instruction-tuning</link>
      <description><![CDATA[We identify three key areas within MLLMs that need to be improved: visual encoding of math diagrams, diagram-language alignment, and mathematical reasoning skills.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mavis-mathematical-visual-instruction-tuning</guid>
    </item>
  </channel>
</rss>
