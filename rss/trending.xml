<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 17 Mar 2025 21:09:22 +0000</lastBuildDate>
    <item>
      <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
      <link>https://paperswithcode.com/paper/2503-01710</link>
      <description><![CDATA[Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2503-01710</guid>
    </item>
    <item>
      <title>YOLOE: Real-Time Seeing Anything</title>
      <link>https://paperswithcode.com/paper/yoloe-real-time-seeing-anything</link>
      <description><![CDATA[Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yoloe-real-time-seeing-anything</guid>
    </item>
    <item>
      <title>Neural Fields with Thermal Activations for Arbitrary-Scale Super-Resolution</title>
      <link>https://paperswithcode.com/paper/neural-fields-with-thermal-activations-for</link>
      <description><![CDATA[We present a novel way to design neural fields such that points can be queried with an adaptive Gaussian PSF, so as to guarantee correct anti-aliasing at any desired output resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-fields-with-thermal-activations-for</guid>
    </item>
    <item>
      <title>Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</title>
      <link>https://paperswithcode.com/paper/block-diffusion-interpolating-between</link>
      <description><![CDATA[Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/block-diffusion-interpolating-between</guid>
    </item>
    <item>
      <title>Agent S: An Open Agentic Framework that Uses Computers Like a Human</title>
      <link>https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</link>
      <description><![CDATA[We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</guid>
    </item>
    <item>
      <title>LBM: Latent Bridge Matching for Fast Image-to-Image Translation</title>
      <link>https://paperswithcode.com/paper/lbm-latent-bridge-matching-for-fast-image-to</link>
      <description><![CDATA[In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lbm-latent-bridge-matching-for-fast-image-to</guid>
    </item>
    <item>
      <title>FoundationStereo: Zero-Shot Stereo Matching</title>
      <link>https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</link>
      <description><![CDATA[However, achieving strong zero-shot generalization - a hallmark of foundation models in other computer vision tasks - remains challenging for stereo matching.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foundationstereo-zero-shot-stereo-matching</guid>
    </item>
    <item>
      <title>Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k</title>
      <link>https://paperswithcode.com/paper/open-sora-2-0-training-a-commercial-level</link>
      <description><![CDATA[With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/open-sora-2-0-training-a-commercial-level</guid>
    </item>
    <item>
      <title>Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/vision-r1-incentivizing-reasoning-capability</link>
      <description><![CDATA[However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-r1-incentivizing-reasoning-capability</guid>
    </item>
    <item>
      <title>Slim attention: cut your context memory in half without loss of accuracy -- K-cache is all you need for MHA</title>
      <link>https://paperswithcode.com/paper/slim-attention-cut-your-context-memory-in</link>
      <description><![CDATA[For encoder-decoder transformers, the context memory size can be reduced even further: For the Whisper models for example, slim attention reduces the context memory by 8x, which can speed up token generation by 5x for batch size 64 for example.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/slim-attention-cut-your-context-memory-in</guid>
    </item>
    <item>
      <title>Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</title>
      <link>https://paperswithcode.com/paper/comet-fine-grained-computation-communication</link>
      <description><![CDATA[The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/comet-fine-grained-computation-communication</guid>
    </item>
    <item>
      <title>Self-rewarding correction for mathematical reasoning</title>
      <link>https://paperswithcode.com/paper/self-rewarding-correction-for-mathematical</link>
      <description><![CDATA[We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-rewarding-correction-for-mathematical</guid>
    </item>
    <item>
      <title>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/search-r1-training-llms-to-reason-and</link>
      <description><![CDATA[Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/search-r1-training-llms-to-reason-and</guid>
    </item>
    <item>
      <title>Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models</title>
      <link>https://paperswithcode.com/paper/attentive-reasoning-queries-a-systematic</link>
      <description><![CDATA[We present Attentive Reasoning Queries (ARQs), a novel structured reasoning approach that significantly improves instruction-following in Large Language Models through domain-specialized reasoning blueprints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/attentive-reasoning-queries-a-systematic</guid>
    </item>
    <item>
      <title>LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL</title>
      <link>https://paperswithcode.com/paper/lmm-r1-empowering-3b-lmms-with-strong</link>
      <description><![CDATA[Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lmm-r1-empowering-3b-lmms-with-strong</guid>
    </item>
    <item>
      <title>ELIZA Reanimated: The world's first chatbot restored on the world's first time sharing system</title>
      <link>https://paperswithcode.com/paper/eliza-reanimated-the-world-s-first-chatbot</link>
      <description><![CDATA[The entire stack is open source, so that any user of a unix-like OS can run the world's first chatbot on the world's first time-sharing system.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eliza-reanimated-the-world-s-first-chatbot</guid>
    </item>
    <item>
      <title>HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</title>
      <link>https://paperswithcode.com/paper/healthgpt-a-medical-large-vision-language</link>
      <description><![CDATA[To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/healthgpt-a-medical-large-vision-language</guid>
    </item>
    <item>
      <title>2 OLMo 2 Furious</title>
      <link>https://paperswithcode.com/paper/2-olmo-2-furious</link>
      <description><![CDATA[Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2-olmo-2-furious</guid>
    </item>
    <item>
      <title>GENERator: A Long-Context Generative Genomic Foundation Model</title>
      <link>https://paperswithcode.com/paper/generator-a-long-context-generative-genomic</link>
      <description><![CDATA[Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generator-a-long-context-generative-genomic</guid>
    </item>
    <item>
      <title>VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control</title>
      <link>https://paperswithcode.com/paper/videopainter-any-length-video-inpainting-and</link>
      <description><![CDATA[Video inpainting, which aims to restore corrupted video content, has experienced substantial progress.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/videopainter-any-length-video-inpainting-and</guid>
    </item>
  </channel>
</rss>
