<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 24 Mar 2025 09:18:48 +0000</lastBuildDate>
    <item>
      <title>ImageNet Classification with Deep Convolutional Neural Networks</title>
      <link>https://paperswithcode.com/paper/imagenet-classification-with-deep</link>
      <description><![CDATA[We trained a large, deep convolutional neural network to classify the 1. 3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagenet-classification-with-deep</guid>
    </item>
    <item>
      <title>VGGT: Visual Geometry Grounded Transformer</title>
      <link>https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</link>
      <description><![CDATA[We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vggt-visual-geometry-grounded-transformer</guid>
    </item>
    <item>
      <title>KBLaM: Knowledge Base augmented Language Model</title>
      <link>https://paperswithcode.com/paper/kblam-knowledge-base-augmented-language-model</link>
      <description><![CDATA[In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a new method for augmenting Large Language Models (LLMs) with external knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kblam-knowledge-base-augmented-language-model</guid>
    </item>
    <item>
      <title>TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools</title>
      <link>https://paperswithcode.com/paper/txagent-an-ai-agent-for-therapeutic-reasoning</link>
      <description><![CDATA[It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/txagent-an-ai-agent-for-therapeutic-reasoning</guid>
    </item>
    <item>
      <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
      <link>https://paperswithcode.com/paper/2503-01710</link>
      <description><![CDATA[Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2503-01710</guid>
    </item>
    <item>
      <title>Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-outperforms-supervised</link>
      <description><![CDATA[Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-outperforms-supervised</guid>
    </item>
    <item>
      <title>Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model</title>
      <link>https://paperswithcode.com/paper/step-video-ti2v-technical-report-a-state-of</link>
      <description><![CDATA[We present Step-Video-TI2V, a state-of-the-art text-driven image-to-video generation model with 30B parameters, capable of generating videos up to 102 frames based on both text and image inputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/step-video-ti2v-technical-report-a-state-of</guid>
    </item>
    <item>
      <title>ReasonGraph: Visualisation of Reasoning Paths</title>
      <link>https://paperswithcode.com/paper/reasongraph-visualisation-of-reasoning-paths</link>
      <description><![CDATA[Large Language Models (LLMs) reasoning processes are challenging to analyze due to their complexity and the lack of organized visualization tools.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reasongraph-visualisation-of-reasoning-paths</guid>
    </item>
    <item>
      <title>LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds</title>
      <link>https://paperswithcode.com/paper/lhm-large-animatable-human-reconstruction</link>
      <description><![CDATA[Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lhm-large-animatable-human-reconstruction</guid>
    </item>
    <item>
      <title>Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way</title>
      <link>https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</link>
      <description><![CDATA[Data analysts often need to iterate between data transformations and chart designs to create rich visualizations for exploratory data analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-formulator-2-iteratively-creating-rich</guid>
    </item>
    <item>
      <title>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/search-r1-training-llms-to-reason-and</link>
      <description><![CDATA[Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/search-r1-training-llms-to-reason-and</guid>
    </item>
    <item>
      <title>Neural Fields with Thermal Activations for Arbitrary-Scale Super-Resolution</title>
      <link>https://paperswithcode.com/paper/neural-fields-with-thermal-activations-for</link>
      <description><![CDATA[We present a novel way to design neural fields such that points can be queried with an adaptive Gaussian PSF, so as to guarantee correct anti-aliasing at any desired output resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-fields-with-thermal-activations-for</guid>
    </item>
    <item>
      <title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title>
      <link>https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</link>
      <description><![CDATA[This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuan3d-2-0-scaling-diffusion-models-for</guid>
    </item>
    <item>
      <title>Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey</title>
      <link>https://paperswithcode.com/paper/multimodal-chain-of-thought-reasoning-a</link>
      <description><![CDATA[By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-chain-of-thought-reasoning-a</guid>
    </item>
    <item>
      <title>Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View</title>
      <link>https://paperswithcode.com/paper/niagara-normal-integrated-geometric-affine</link>
      <description><![CDATA[Recent advances in single-view 3D scene reconstruction have highlighted the challenges in capturing fine geometric details and ensuring structural consistency, particularly in high-fidelity outdoor scene modeling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/niagara-normal-integrated-geometric-affine</guid>
    </item>
    <item>
      <title>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</title>
      <link>https://paperswithcode.com/paper/light-r1-curriculum-sft-dpo-and-rl-for-long</link>
      <description><![CDATA[The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/light-r1-curriculum-sft-dpo-and-rl-for-long</guid>
    </item>
    <item>
      <title>HybridFlow: A Flexible and Efficient RLHF Framework</title>
      <link>https://paperswithcode.com/paper/hybridflow-a-flexible-and-efficient-rlhf</link>
      <description><![CDATA[Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hybridflow-a-flexible-and-efficient-rlhf</guid>
    </item>
    <item>
      <title>YOLOE: Real-Time Seeing Anything</title>
      <link>https://paperswithcode.com/paper/yoloe-real-time-seeing-anything</link>
      <description><![CDATA[Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yoloe-real-time-seeing-anything</guid>
    </item>
    <item>
      <title>Zero-shot Voice Conversion with Diffusion Transformers</title>
      <link>https://paperswithcode.com/paper/zero-shot-voice-conversion-with-diffusion</link>
      <description><![CDATA[Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-voice-conversion-with-diffusion</guid>
    </item>
    <item>
      <title>Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception</title>
      <link>https://paperswithcode.com/paper/seeing-the-future-perceiving-the-future-a</link>
      <description><![CDATA[Extensive experiments on the nuScenes dataset demonstrate that UniFuture outperforms specialized models on future generation and perception tasks, highlighting the advantages of a unified, structurally-aware world model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seeing-the-future-perceiving-the-future-a</guid>
    </item>
  </channel>
</rss>
