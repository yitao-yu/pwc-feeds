<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 13 Dec 2022 09:14:00 +0000</lastBuildDate>
    <item>
      <title>4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions</title>
      <link>https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields</link>
      <description><![CDATA[In this paper, we present a novel and effective framework, named 4K-NeRF, to pursue high fidelity view synthesis on the challenging scenarios of ultra high resolutions, building on the methodology of neural radiance fields (NeRF).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/4k-nerf-high-fidelity-neural-radiance-fields</guid>
    </item>
    <item>
      <title>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</title>
      <link>https://paperswithcode.com/paper/training-free-structured-diffusion-guidance</link>
      <description><![CDATA[In this work, we improve the compositional skills of T2I models, specifically more accurate attribute binding and better image compositions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-free-structured-diffusion-guidance</guid>
    </item>
    <item>
      <title>Programming Is Hard -- Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation</title>
      <link>https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</link>
      <description><![CDATA[The introductory programming sequence has been the focus of much research in computing education.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/programming-is-hard-or-at-least-it-used-to-be</guid>
    </item>
    <item>
      <title>Learning Video Representations from Large Language Models</title>
      <link>https://paperswithcode.com/paper/learning-video-representations-from-large</link>
      <description><![CDATA[We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-video-representations-from-large</guid>
    </item>
    <item>
      <title>ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency</title>
      <link>https://paperswithcode.com/paper/ace-cooperative-multi-agent-q-learning-with</link>
      <description><![CDATA[In the learning phase, each agent minimizes the TD error that is dependent on how the subsequent agents have reacted to their chosen action.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ace-cooperative-multi-agent-q-learning-with</guid>
    </item>
    <item>
      <title>DiffusionInst: Diffusion Model for Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/diffusioninst-diffusion-model-for-instance</link>
      <description><![CDATA[This paper proposes DiffusionInst, a novel framework that represents instances as instance-aware filters and formulates instance segmentation as a noise-to-filter denoising process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusioninst-diffusion-model-for-instance</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
    <item>
      <title>Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning</title>
      <link>https://paperswithcode.com/paper/yuan-1-0-large-scale-pre-trained-language</link>
      <description><![CDATA[With this method, Yuan 1. 0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yuan-1-0-large-scale-pre-trained-language</guid>
    </item>
    <item>
      <title>DAMO-YOLO : A Report on Real-Time Object Detection Design</title>
      <link>https://paperswithcode.com/paper/damo-yolo-a-report-on-real-time-object</link>
      <description><![CDATA[In this report, we present a fast and accurate object detection method dubbed DAMO-YOLO, which achieves higher performance than the state-of-the-art YOLO series.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/damo-yolo-a-report-on-real-time-object</guid>
    </item>
    <item>
      <title>Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</title>
      <link>https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</link>
      <description><![CDATA[To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</guid>
    </item>
    <item>
      <title>EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</title>
      <link>https://paperswithcode.com/paper/eva-exploring-the-limits-of-masked-visual</link>
      <description><![CDATA[We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/eva-exploring-the-limits-of-masked-visual</guid>
    </item>
    <item>
      <title>Paint by Example: Exemplar-based Image Editing with Diffusion Models</title>
      <link>https://paperswithcode.com/paper/paint-by-example-exemplar-based-image-editing</link>
      <description><![CDATA[Language-guided image editing has achieved great success recently.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paint-by-example-exemplar-based-image-editing</guid>
    </item>
    <item>
      <title>OpenFE: Automated Feature Generation beyond Expert-level Performance</title>
      <link>https://paperswithcode.com/paper/openfe-automated-feature-generation-beyond</link>
      <description><![CDATA[The major challenge in automated feature generation is to efficiently and accurately identify useful features from a vast pool of candidate features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openfe-automated-feature-generation-beyond</guid>
    </item>
    <item>
      <title>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</title>
      <link>https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</link>
      <description><![CDATA[Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dreambooth-fine-tuning-text-to-image</guid>
    </item>
    <item>
      <title>Discovering Latent Knowledge in Language Models Without Supervision</title>
      <link>https://paperswithcode.com/paper/discovering-latent-knowledge-in-language</link>
      <description><![CDATA[Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/discovering-latent-knowledge-in-language</guid>
    </item>
    <item>
      <title>Avocodo: Generative Adversarial Network for Artifact-free Vocoder</title>
      <link>https://paperswithcode.com/paper/avocodo-generative-adversarial-network-for</link>
      <description><![CDATA[Therefore, in this paper, we investigate the relationship between these artifacts and GAN-based neural vocoders and propose a GAN-based neural vocoder, called Avocodo, that allows the synthesis of high-fidelity speech with reduced artifacts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/avocodo-generative-adversarial-network-for</guid>
    </item>
    <item>
      <title>UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/unetr-delving-into-efficient-and-accurate-3d</link>
      <description><![CDATA[Owing to the success of transformer models, recent works study their applicability in 3D medical segmentation tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unetr-delving-into-efficient-and-accurate-3d</guid>
    </item>
    <item>
      <title>Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</link>
      <description><![CDATA[We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-speech-recognition-via-large-scale-1</guid>
    </item>
    <item>
      <title>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</title>
      <link>https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</link>
      <description><![CDATA[The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chinese-clip-contrastive-vision-language</guid>
    </item>
    <item>
      <title>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</title>
      <link>https://paperswithcode.com/paper/simvtp-simple-video-text-pre-training-with</link>
      <description><![CDATA[This paper presents SimVTP: a Simple Video-Text Pretraining framework via masked autoencoders.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simvtp-simple-video-text-pre-training-with</guid>
    </item>
  </channel>
</rss>
