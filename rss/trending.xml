<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 26 Dec 2022 09:12:43 +0000</lastBuildDate>
    <item>
      <title>Point-E: A System for Generating 3D Point Clouds from Complex Prompts</title>
      <link>https://paperswithcode.com/paper/point-e-a-system-for-generating-3d-point</link>
      <description><![CDATA[This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/point-e-a-system-for-generating-3d-point</guid>
    </item>
    <item>
      <title>Scalable Diffusion Models with Transformers</title>
      <link>https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</link>
      <description><![CDATA[We explore a new class of diffusion models based on the transformer architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers</guid>
    </item>
    <item>
      <title>Generalized Decoding for Pixel, Image, and Language</title>
      <link>https://paperswithcode.com/paper/generalized-decoding-for-pixel-image-and</link>
      <description><![CDATA[We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generalized-decoding-for-pixel-image-and</guid>
    </item>
    <item>
      <title>OPT: Open Pre-trained Transformer Language Models</title>
      <link>https://paperswithcode.com/paper/opt-open-pre-trained-transformer-language</link>
      <description><![CDATA[Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opt-open-pre-trained-transformer-language</guid>
    </item>
    <item>
      <title>Towards Reasoning in Large Language Models: A Survey</title>
      <link>https://paperswithcode.com/paper/towards-reasoning-in-large-language-models-a</link>
      <description><![CDATA[Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-reasoning-in-large-language-models-a</guid>
    </item>
    <item>
      <title>Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</title>
      <link>https://paperswithcode.com/paper/alpa-automating-inter-and-intra-operator</link>
      <description><![CDATA[Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alpa-automating-inter-and-intra-operator</guid>
    </item>
    <item>
      <title>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers</title>
      <link>https://paperswithcode.com/paper/why-can-gpt-learn-in-context-language-models</link>
      <description><![CDATA[In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit finetuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/why-can-gpt-learn-in-context-language-models</guid>
    </item>
    <item>
      <title>DifFace: Blind Face Restoration with Diffused Error Contraction</title>
      <link>https://paperswithcode.com/paper/difface-blind-face-restoration-with-diffused</link>
      <description><![CDATA[Moreover, the transition distribution can contract the error of the restoration backbone and thus makes our method more robust to unknown degradations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/difface-blind-face-restoration-with-diffused</guid>
    </item>
    <item>
      <title>Nonparametric Masked Language Modeling</title>
      <link>https://paperswithcode.com/paper/nonparametric-masked-language-modeling</link>
      <description><![CDATA[Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nonparametric-masked-language-modeling</guid>
    </item>
    <item>
      <title>FILM: Frame Interpolation for Large Motion</title>
      <link>https://paperswithcode.com/paper/film-frame-interpolation-for-large-motion</link>
      <description><![CDATA[Recent methods use multiple networks to estimate optical flow or depth and a separate network dedicated to frame synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/film-frame-interpolation-for-large-motion</guid>
    </item>
    <item>
      <title>Fast DistilBERT on CPUs</title>
      <link>https://paperswithcode.com/paper/fast-distilbert-on-cpus</link>
      <description><![CDATA[In this work, we propose a new pipeline for creating and running Fast Transformer models on CPUs, utilizing hardware-aware pruning, knowledge distillation, quantization, and our own Transformer inference runtime engine with optimized kernels for sparse and quantized operators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-distilbert-on-cpus</guid>
    </item>
    <item>
      <title>Towards Robust Blind Face Restoration with Codebook Lookup Transformer</title>
      <link>https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</link>
      <description><![CDATA[In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-robust-blind-face-restoration-with</guid>
    </item>
    <item>
      <title>DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients</title>
      <link>https://paperswithcode.com/paper/deeplsd-line-segment-detection-and-refinement</link>
      <description><![CDATA[Their learned counterparts are more repeatable and can handle challenging images, but at the cost of a lower accuracy and a bias towards wireframe lines.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deeplsd-line-segment-detection-and-refinement</guid>
    </item>
    <item>
      <title>Benchmarking and Analyzing Point Cloud Classification under Corruptions</title>
      <link>https://paperswithcode.com/paper/benchmarking-and-analyzing-point-cloud</link>
      <description><![CDATA[3D perception, especially point cloud classification, has achieved substantial progress.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/benchmarking-and-analyzing-point-cloud</guid>
    </item>
    <item>
      <title>Self-Instruct: Aligning Language Model with Self Generated Instructions</title>
      <link>https://paperswithcode.com/paper/self-instruct-aligning-language-model-with</link>
      <description><![CDATA[Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT_001, which is trained with private user data and human annotations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-instruct-aligning-language-model-with</guid>
    </item>
    <item>
      <title>3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions</title>
      <link>https://paperswithcode.com/paper/3d-highlighter-localizing-regions-on-3d</link>
      <description><![CDATA[A key feature of our system is the ability to interpret "out-of-domain" localizations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-highlighter-localizing-regions-on-3d</guid>
    </item>
    <item>
      <title>DI-engine</title>
      <link>https://github.com/opendilab/DI-engine</link>
      <description><![CDATA[OpenDILab Decision AI Engine]]></description>
      <guid isPermaLink="true">https://github.com/opendilab/DI-engine</guid>
    </item>
    <item>
      <title>Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</title>
      <link>https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</link>
      <description><![CDATA[To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/is-reinforcement-learning-not-for-natural</guid>
    </item>
    <item>
      <title>Goal-oriented Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/goal-oriented-autonomous-driving</link>
      <description><![CDATA[Modern autonomous driving system is characterized as modular tasks in sequential order, i. e., perception, prediction and planning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/goal-oriented-autonomous-driving</guid>
    </item>
    <item>
      <title>Active-Learning-as-a-Service: An Automatic and Efficient MLOps System for Data-Centric AI</title>
      <link>https://paperswithcode.com/paper/active-learning-as-a-service-an-efficient</link>
      <description><![CDATA[In data-centric AI, active learning (AL) plays a vital role, but current AL tools 1) require users to manually select AL strategies, and 2) can not perform AL tasks efficiently.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/active-learning-as-a-service-an-efficient</guid>
    </item>
  </channel>
</rss>
