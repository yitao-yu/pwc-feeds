<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 03 Jun 2024 09:14:33 +0000</lastBuildDate>
    <item>
      <title>AutoCoder: Enhancing Code Large Language Model with \textsc{AIEV-Instruct}</title>
      <link>https://paperswithcode.com/paper/autocoder-enhancing-code-large-language-model</link>
      <description><![CDATA[We introduce AutoCoder, the first Large Language Model to surpass GPT-4 Turbo (April 2024) and GPT-4o in pass@1 on the Human Eval benchmark test ($\mathbf{90. 9\%}$ vs. $\mathbf{90. 2\%}$).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autocoder-enhancing-code-large-language-model</guid>
    </item>
    <item>
      <title>FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models</title>
      <link>https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</link>
      <description><![CDATA[As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</guid>
    </item>
    <item>
      <title>EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture</title>
      <link>https://paperswithcode.com/paper/easyanimate-a-high-performance-long-video</link>
      <description><![CDATA[The motion module can be adapted to various DiT baseline methods to generate video with different styles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/easyanimate-a-high-performance-long-video</guid>
    </item>
    <item>
      <title>YOLOv10: Real-Time End-to-End Object Detection</title>
      <link>https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</link>
      <description><![CDATA[In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</guid>
    </item>
    <item>
      <title>InstaDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos</title>
      <link>https://paperswithcode.com/paper/instadrag-lightning-fast-and-accurate-drag</link>
      <description><![CDATA[Accuracy and speed are critical in image editing tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instadrag-lightning-fast-and-accurate-drag</guid>
    </item>
    <item>
      <title>$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/textit-s-3-gaussian-self-supervised-street</link>
      <description><![CDATA[Photorealistic 3D reconstruction of street scenes is a critical technique for developing real-world simulators for autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textit-s-3-gaussian-self-supervised-street</guid>
    </item>
    <item>
      <title>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
      <link>https://paperswithcode.com/paper/switch-transformers-scaling-to-trillion</link>
      <description><![CDATA[We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/switch-transformers-scaling-to-trillion</guid>
    </item>
    <item>
      <title>Looking Backward: Streaming Video-to-Video Translation with Feature Banks</title>
      <link>https://paperswithcode.com/paper/looking-backward-streaming-video-to-video</link>
      <description><![CDATA[This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/looking-backward-streaming-video-to-video</guid>
    </item>
    <item>
      <title>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</title>
      <link>https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</link>
      <description><![CDATA[To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llava-uhd-an-lmm-perceiving-any-aspect-ratio</guid>
    </item>
    <item>
      <title>FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research</title>
      <link>https://paperswithcode.com/paper/flashrag-a-modular-toolkit-for-efficient</link>
      <description><![CDATA[With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashrag-a-modular-toolkit-for-efficient</guid>
    </item>
    <item>
      <title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title>
      <link>https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</link>
      <description><![CDATA[In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</guid>
    </item>
    <item>
      <title>APISR: Anime Production Inspired Real-World Anime Super-Resolution</title>
      <link>https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</link>
      <description><![CDATA[In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/apisr-anime-production-inspired-real-world</guid>
    </item>
    <item>
      <title>NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer</title>
      <link>https://paperswithcode.com/paper/nvs-solver-video-diffusion-model-as-zero-shot</link>
      <description><![CDATA[By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose NVS-Solver, a new novel view synthesis (NVS) paradigm that operates \textit{without} the need for training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nvs-solver-video-diffusion-model-as-zero-shot</guid>
    </item>
    <item>
      <title>How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</title>
      <link>https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</link>
      <description><![CDATA[Compared to both open-source and proprietary models, InternVL 1. 5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-far-are-we-to-gpt-4v-closing-the-gap-to</guid>
    </item>
    <item>
      <title>Yuan 2.0-M32: Mixture of Experts with Attention Router</title>
      <link>https://paperswithcode.com/paper/yuan-2-0-m32-mixture-of-experts-with</link>
      <description><![CDATA[Yuan 2. 0-M32, with a similar base architecture as Yuan-2. 0 2B, uses a mixture-of-experts architecture with 32 experts of which 2 experts are active.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yuan-2-0-m32-mixture-of-experts-with</guid>
    </item>
    <item>
      <title>Look Once to Hear: Target Speech Hearing with Noisy Examples</title>
      <link>https://paperswithcode.com/paper/look-once-to-hear-target-speech-hearing-with</link>
      <description><![CDATA[We present the first enrollment interface where the wearer looks at the target speaker for a few seconds to capture a single, short, highly noisy, binaural example of the target speaker.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/look-once-to-hear-target-speech-hearing-with</guid>
    </item>
    <item>
      <title>Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head</title>
      <link>https://paperswithcode.com/paper/real-time-transformer-based-open-vocabulary</link>
      <description><![CDATA[End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-time-transformer-based-open-vocabulary</guid>
    </item>
    <item>
      <title>OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/occsora-4d-occupancy-generation-models-as</link>
      <description><![CDATA[To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/occsora-4d-occupancy-generation-models-as</guid>
    </item>
    <item>
      <title>Matryoshka Query Transformer for Large Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/matryoshka-query-transformer-for-large-vision</link>
      <description><![CDATA[This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matryoshka-query-transformer-for-large-vision</guid>
    </item>
    <item>
      <title>State Space Models for Event Cameras</title>
      <link>https://paperswithcode.com/paper/state-space-models-for-event-cameras</link>
      <description><![CDATA[We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/state-space-models-for-event-cameras</guid>
    </item>
  </channel>
</rss>
