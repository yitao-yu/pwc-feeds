<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 22 Jul 2023 09:11:02 +0000</lastBuildDate>
    <item>
      <title>Llama 2: Open Foundation and Fine-Tuned Chat Models</title>
      <link>https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat</link>
      <description><![CDATA[In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat</guid>
    </item>
    <item>
      <title>FABRIC: Personalizing Diffusion Models with Iterative Feedback</title>
      <link>https://paperswithcode.com/paper/fabric-personalizing-diffusion-models-with</link>
      <description><![CDATA[In an era where visual content generation is increasingly driven by machine learning, the integration of human feedback into generative models presents significant opportunities for enhancing user experience and output quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fabric-personalizing-diffusion-models-with</guid>
    </item>
    <item>
      <title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title>
      <link>https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</link>
      <description><![CDATA[With the advance of text-to-image models (e. g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/animatediff-animate-your-personalized-text-to</guid>
    </item>
    <item>
      <title>How is ChatGPT's behavior changing over time?</title>
      <link>https://paperswithcode.com/paper/how-is-chatgpt-s-behavior-changing-over-time</link>
      <description><![CDATA[We find that the performance and behavior of both GPT-3. 5 and GPT-4 can vary greatly over time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-is-chatgpt-s-behavior-changing-over-time</guid>
    </item>
    <item>
      <title>Semantic-SAM: Segment and Recognize Anything at Any Granularity</title>
      <link>https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</link>
      <description><![CDATA[In this paper, we introduce Semantic-SAM, a universal image segmentation model to enable segment and recognize anything at any desired granularity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-sam-segment-and-recognize-anything</guid>
    </item>
    <item>
      <title>CoTracker: It is Better to Track Together</title>
      <link>https://paperswithcode.com/paper/cotracker-it-is-better-to-track-together</link>
      <description><![CDATA[In this paper, we thus propose CoTracker, an architecture that jointly tracks multiple points throughout an entire video.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cotracker-it-is-better-to-track-together</guid>
    </item>
    <item>
      <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</link>
      <description><![CDATA[We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient</guid>
    </item>
    <item>
      <title>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title>
      <link>https://paperswithcode.com/paper/flashattention-2-faster-attention-with-better</link>
      <description><![CDATA[We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashattention-2-faster-attention-with-better</guid>
    </item>
    <item>
      <title>Petals: Collaborative Inference and Fine-tuning of Large Models</title>
      <link>https://paperswithcode.com/paper/petals-collaborative-inference-and-fine</link>
      <description><![CDATA[However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/petals-collaborative-inference-and-fine</guid>
    </item>
    <item>
      <title>Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image</title>
      <link>https://paperswithcode.com/paper/metric3d-towards-zero-shot-metric-3d</link>
      <description><![CDATA[State-of-the-art (SOTA) monocular metric depth estimation methods can only handle a single camera model and are unable to perform mixed-data training due to the metric ambiguity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metric3d-towards-zero-shot-metric-3d</guid>
    </item>
    <item>
      <title>Exploiting Diffusion Prior for Real-World Image Super-Resolution</title>
      <link>https://paperswithcode.com/paper/exploiting-diffusion-prior-for-real-world</link>
      <description><![CDATA[We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploiting-diffusion-prior-for-real-world</guid>
    </item>
    <item>
      <title>Planting a SEED of Vision in Large Language Model</title>
      <link>https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language</link>
      <description><![CDATA[Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.)]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/planting-a-seed-of-vision-in-large-language</guid>
    </item>
    <item>
      <title>L-Eval: Instituting Standardized Evaluation for Long Context Language Models</title>
      <link>https://paperswithcode.com/paper/l-eval-instituting-standardized-evaluation</link>
      <description><![CDATA[Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e. g. summarizing a paper) and conversations with more extensive histories.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/l-eval-instituting-standardized-evaluation</guid>
    </item>
    <item>
      <title>h2oGPT: Democratizing Large Language Models</title>
      <link>https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</link>
      <description><![CDATA[Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/h2ogpt-democratizing-large-language-models</guid>
    </item>
    <item>
      <title>Neural Video Depth Stabilizer</title>
      <link>https://paperswithcode.com/paper/neural-video-depth-stabilizer</link>
      <description><![CDATA[Video depth estimation aims to infer temporally consistent depth.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-video-depth-stabilizer</guid>
    </item>
    <item>
      <title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</title>
      <link>https://paperswithcode.com/paper/prior-free-category-level-pose-estimation</link>
      <description><![CDATA[Category-level 6D pose estimation aims to predict the poses and sizes of unseen objects from a specific category.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prior-free-category-level-pose-estimation</guid>
    </item>
    <item>
      <title>DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</title>
      <link>https://paperswithcode.com/paper/ds-fusion-artistic-typography-via</link>
      <description><![CDATA[We introduce a novel method to automatically generate an artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ds-fusion-artistic-typography-via</guid>
    </item>
    <item>
      <title>RepViT: Revisiting Mobile CNN From ViT Perspective</title>
      <link>https://paperswithcode.com/paper/repvit-revisiting-mobile-cnn-from-vit</link>
      <description><![CDATA[On ImageNet, RepViT achieves over 80\% top-1 accuracy with nearly 1ms latency on an iPhone 12, which is the first time for a lightweight model, to the best of our knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repvit-revisiting-mobile-cnn-from-vit</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation</title>
      <link>https://paperswithcode.com/paper/cnos-a-strong-baseline-for-cad-based-novel</link>
      <description><![CDATA[We propose a simple three-stage approach to segment unseen objects in RGB images using their CAD models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cnos-a-strong-baseline-for-cad-based-novel</guid>
    </item>
  </channel>
</rss>
