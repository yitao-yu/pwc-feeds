<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 30 Jun 2022 21:07:09 +0000</lastBuildDate>
    <item>
      <title>Pen and Paper Exercises in Machine Learning</title>
      <link>https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</link>
      <description><![CDATA[This is a collection of (mostly) pen-and-paper exercises in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pen-and-paper-exercises-in-machine-learning</guid>
    </item>
    <item>
      <title>Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation</title>
      <link>https://paperswithcode.com/paper/lifting-transformer-for-3d-human-pose</link>
      <description><![CDATA[The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lifting-transformer-for-3d-human-pose</guid>
    </item>
    <item>
      <title>Scaling Autoregressive Models for Content-Rich Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/scaling-autoregressive-models-for-content</link>
      <description><![CDATA[We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-autoregressive-models-for-content</guid>
    </item>
    <item>
      <title>Multi-Graph Fusion Networks for Urban Region Embedding</title>
      <link>https://paperswithcode.com/paper/multi-graph-fusion-networks-for-urban-region</link>
      <description><![CDATA[Human mobility data contains rich but abundant information, which yields to the comprehensive region embeddings for cross domain tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-graph-fusion-networks-for-urban-region</guid>
    </item>
    <item>
      <title>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation</title>
      <link>https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</link>
      <description><![CDATA[The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/epro-pnp-generalized-end-to-end-probabilistic</guid>
    </item>
    <item>
      <title>Ivy: Templated Deep Learning for Inter-Framework Portability</title>
      <link>https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</link>
      <description><![CDATA[We introduce Ivy, a templated Deep Learning (DL) framework which abstracts existing DL frameworks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ivy-templated-deep-learning-for-inter</guid>
    </item>
    <item>
      <title>HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D Reconstruction</title>
      <link>https://paperswithcode.com/paper/hm3d-abo-a-photo-realistic-dataset-for-object</link>
      <description><![CDATA[Reconstructing 3D objects is an important computer vision task that has wide application in AR/VR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hm3d-abo-a-photo-realistic-dataset-for-object</guid>
    </item>
    <item>
      <title>OmniXAI: A Library for Explainable AI</title>
      <link>https://paperswithcode.com/paper/omnixai-a-library-for-explainable-ai</link>
      <description><![CDATA[We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omnixai-a-library-for-explainable-ai</guid>
    </item>
    <item>
      <title>Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning</title>
      <link>https://paperswithcode.com/paper/weakly-supervised-video-anomaly-detection</link>
      <description><![CDATA[To address this issue, we introduce a novel and theoretically sound method, named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature magnitude learning function to effectively recognise the positive instances, substantially improving the robustness of the MIL approach to the negative instances from abnormal videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/weakly-supervised-video-anomaly-detection</guid>
    </item>
    <item>
      <title>Matryoshka Representations for Adaptive Deployment</title>
      <link>https://paperswithcode.com/paper/matryoshka-representations-for-adaptive</link>
      <description><![CDATA[The flexibility within the learned Matryoshka Representations offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matryoshka-representations-for-adaptive</guid>
    </item>
    <item>
      <title>Free-Form Image Inpainting with Gated Convolution</title>
      <link>https://paperswithcode.com/paper/free-form-image-inpainting-with-gated</link>
      <description><![CDATA[We present a generative image inpainting system to complete images with free-form mask and guidance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/free-form-image-inpainting-with-gated</guid>
    </item>
    <item>
      <title>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
      <link>https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</link>
      <description><![CDATA[We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/photorealistic-text-to-image-diffusion-models</guid>
    </item>
    <item>
      <title>Zero-Shot Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/zero-shot-text-to-image-generation</link>
      <description><![CDATA[Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-text-to-image-generation</guid>
    </item>
    <item>
      <title>OPT: Open Pre-trained Transformer Language Models</title>
      <link>https://paperswithcode.com/paper/opt-open-pre-trained-transformer-language</link>
      <description><![CDATA[Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opt-open-pre-trained-transformer-language</guid>
    </item>
    <item>
      <title>The ArtBench Dataset: Benchmarking Generative Models with Artworks</title>
      <link>https://paperswithcode.com/paper/the-artbench-dataset-benchmarking-generative</link>
      <description><![CDATA[We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-artbench-dataset-benchmarking-generative</guid>
    </item>
    <item>
      <title>Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case</title>
      <link>https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</link>
      <description><![CDATA[In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pythae-unifying-generative-autoencoders-in</guid>
    </item>
    <item>
      <title>BokehMe: When Neural Rendering Meets Classical Rendering</title>
      <link>https://paperswithcode.com/paper/bokehme-when-neural-rendering-meets-classical-1</link>
      <description><![CDATA[Based on this formulation, we implement the classical renderer by a scattering-based method and propose a two-stage neural renderer to fix the erroneous areas from the classical renderer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bokehme-when-neural-rendering-meets-classical-1</guid>
    </item>
    <item>
      <title>Vision GNN: An Image is Worth Graph of Nodes</title>
      <link>https://paperswithcode.com/paper/vision-gnn-an-image-is-worth-graph-of-nodes</link>
      <description><![CDATA[In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vision-gnn-an-image-is-worth-graph-of-nodes</guid>
    </item>
    <item>
      <title>GhostNet: More Features from Cheap Operations</title>
      <link>https://paperswithcode.com/paper/ghostnet-more-features-from-cheap-operations</link>
      <description><![CDATA[Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ghostnet-more-features-from-cheap-operations</guid>
    </item>
    <item>
      <title>GhostNets on Heterogeneous Devices via Cheap Operations</title>
      <link>https://paperswithcode.com/paper/ghostnets-on-heterogeneous-devices-via-cheap</link>
      <description><![CDATA[The proposed C-Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ghostnets-on-heterogeneous-devices-via-cheap</guid>
    </item>
  </channel>
</rss>
