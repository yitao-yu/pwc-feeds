<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 13 Oct 2024 21:09:06 +0000</lastBuildDate>
    <item>
      <title>Pyramidal Flow Matching for Efficient Video Generative Modeling</title>
      <link>https://paperswithcode.com/paper/pyramidal-flow-matching-for-efficient-video</link>
      <description><![CDATA[Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pyramidal-flow-matching-for-efficient-video</guid>
    </item>
    <item>
      <title>Aria: An Open Multimodal Native Mixture-of-Experts Model</title>
      <link>https://paperswithcode.com/paper/aria-an-open-multimodal-native-mixture-of</link>
      <description><![CDATA[Information comes in diverse modalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aria-an-open-multimodal-native-mixture-of</guid>
    </item>
    <item>
      <title>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering</title>
      <link>https://paperswithcode.com/paper/mle-bench-evaluating-machine-learning-agents</link>
      <description><![CDATA[We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mle-bench-evaluating-machine-learning-agents</guid>
    </item>
    <item>
      <title>Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</title>
      <link>https://paperswithcode.com/paper/depth-pro-sharp-monocular-metric-depth-in</link>
      <description><![CDATA[We present a foundation model for zero-shot metric monocular depth estimation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/depth-pro-sharp-monocular-metric-depth-in</guid>
    </item>
    <item>
      <title>Diffusion for World Modeling: Visual Details Matter in Atari</title>
      <link>https://paperswithcode.com/paper/diffusion-for-world-modeling-visual-details</link>
      <description><![CDATA[Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-for-world-modeling-visual-details</guid>
    </item>
    <item>
      <title>LightRAG: Simple and Fast Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lightrag-simple-and-fast-retrieval-augmented</guid>
    </item>
    <item>
      <title>Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration</title>
      <link>https://paperswithcode.com/paper/posterior-mean-rectified-flow-towards-minimum</link>
      <description><![CDATA[Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e. g., PSNR, SSIM) and by perceptual quality measures (e. g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/posterior-mean-rectified-flow-towards-minimum</guid>
    </item>
    <item>
      <title>Agent S: An Open Agentic Framework that Uses Computers Like a Human</title>
      <link>https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</link>
      <description><![CDATA[We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-s-an-open-agentic-framework-that-uses</guid>
    </item>
    <item>
      <title>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</title>
      <link>https://paperswithcode.com/paper/scaling-proprioceptive-visual-learning-with</link>
      <description><![CDATA[Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-proprioceptive-visual-learning-with</guid>
    </item>
    <item>
      <title>Making Images Real Again: A Comprehensive Survey on Deep Image Composition</title>
      <link>https://paperswithcode.com/paper/making-images-real-again-a-comprehensive</link>
      <description><![CDATA[We have also contributed the first image composition toolbox: libcom https://github. com/bcmi/libcom, which assembles 10+ image composition related functions (e. g., image blending, image harmonization, object placement, shadow generation, generative composition).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/making-images-real-again-a-comprehensive</guid>
    </item>
    <item>
      <title>Generalizable and Animatable Gaussian Head Avatar</title>
      <link>https://paperswithcode.com/paper/generalizable-and-animatable-gaussian-head</link>
      <description><![CDATA[In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generalizable-and-animatable-gaussian-head</guid>
    </item>
    <item>
      <title>Fast Feedforward 3D Gaussian Splatting Compression</title>
      <link>https://paperswithcode.com/paper/fast-feedforward-3d-gaussian-splatting</link>
      <description><![CDATA[With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity rendering for novel view synthesis, storage requirements pose challenges for their widespread adoption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-feedforward-3d-gaussian-splatting</guid>
    </item>
    <item>
      <title>Libra: Building Decoupled Vision System on Large Language Models</title>
      <link>https://paperswithcode.com/paper/libra-building-decoupled-vision-system-on</link>
      <description><![CDATA[Specifically, we incorporate a routed visual expert with a cross-modal bridge module into a pretrained LLM to route the vision and language flows during attention computing to enable different attention patterns in inner-modal modeling and cross-modal interaction scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/libra-building-decoupled-vision-system-on</guid>
    </item>
    <item>
      <title>AgentKit: Structured LLM Reasoning with Dynamic Graphs</title>
      <link>https://paperswithcode.com/paper/agentkit-flow-engineering-with-graphs-not</link>
      <description><![CDATA[The chains of nodes can be designed to explicitly enforce a naturally structured "thought process".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agentkit-flow-engineering-with-graphs-not</guid>
    </item>
    <item>
      <title>PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling</title>
      <link>https://paperswithcode.com/paper/pdf-wukong-a-large-multimodal-model-for</link>
      <description><![CDATA[In this paper, we introduce PDF-WuKong, a multimodal large language model (MLLM) which is designed to enhance multimodal question-answering (QA) for long PDF documents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pdf-wukong-a-large-multimodal-model-for</guid>
    </item>
    <item>
      <title>Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow</title>
      <link>https://paperswithcode.com/paper/rectified-diffusion-straightness-is-not-your</link>
      <description><![CDATA[Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rectified-diffusion-straightness-is-not-your</guid>
    </item>
    <item>
      <title>VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models</title>
      <link>https://paperswithcode.com/paper/vptq-extreme-low-bit-vector-post-training</link>
      <description><![CDATA[Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vptq-extreme-low-bit-vector-post-training</guid>
    </item>
    <item>
      <title>Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate</title>
      <link>https://paperswithcode.com/paper/deciphering-cross-modal-alignment-in-large</link>
      <description><![CDATA[We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deciphering-cross-modal-alignment-in-large</guid>
    </item>
    <item>
      <title>IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/itercomp-iterative-composition-aware-feedback</link>
      <description><![CDATA[IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/itercomp-iterative-composition-aware-feedback</guid>
    </item>
    <item>
      <title>Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale</title>
      <link>https://paperswithcode.com/paper/windows-agent-arena-evaluating-multi-modal-os</link>
      <description><![CDATA[To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/windows-agent-arena-evaluating-multi-modal-os</guid>
    </item>
  </channel>
</rss>
