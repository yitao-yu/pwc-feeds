<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 07 Jun 2025 21:09:10 +0000</lastBuildDate>
    <item>
      <title>HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters</title>
      <link>https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</link>
      <description><![CDATA[This ensures the dynamic motion and strong character consistency; (ii) An Audio Emotion Module (AEM) is introduced to extract and transfer the emotional cues from an emotion reference image to the target generated video, enabling fine-grained and accurate emotion style control; (iii) A Face-Aware Audio Adapter (FAA) is proposed to isolate the audio-driven character with latent-level face mask, enabling independent audio injection via cross-attention for multi-character scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hunyuanvideo-avatar-high-fidelity-audio</guid>
    </item>
    <item>
      <title>Emerging Properties in Unified Multimodal Pretraining</title>
      <link>https://paperswithcode.com/paper/emerging-properties-in-unified-multimodal</link>
      <description><![CDATA[Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/emerging-properties-in-unified-multimodal</guid>
    </item>
    <item>
      <title>AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment</title>
      <link>https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</link>
      <description><![CDATA[In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alphaevolve-a-learning-framework-to-discover</guid>
    </item>
    <item>
      <title>RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination</title>
      <link>https://paperswithcode.com/paper/renderformer-transformer-based-neural</link>
      <description><![CDATA[We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/renderformer-transformer-based-neural</guid>
    </item>
    <item>
      <title>WebDancer: Towards Autonomous Information Seeking Agency</title>
      <link>https://paperswithcode.com/paper/webdancer-towards-autonomous-information</link>
      <description><![CDATA[We instantiate this framework in a web agent based on the ReAct, WebDancer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/webdancer-towards-autonomous-information</guid>
    </item>
    <item>
      <title>syftr: Pareto-Optimal Generative AI</title>
      <link>https://paperswithcode.com/paper/syftr-pareto-optimal-generative-ai</link>
      <description><![CDATA[Retrieval-Augmented Generation (RAG) pipelines are central to applying large language models (LLMs) to proprietary or dynamic data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/syftr-pareto-optimal-generative-ai</guid>
    </item>
    <item>
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <link>https://paperswithcode.com/paper/dolphin-document-image-parsing-via</link>
      <description><![CDATA[Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dolphin-document-image-parsing-via</guid>
    </item>
    <item>
      <title>Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution</title>
      <link>https://paperswithcode.com/paper/alita-generalist-agent-enabling-scalable</link>
      <description><![CDATA[For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alita-generalist-agent-enabling-scalable</guid>
    </item>
    <item>
      <title>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System</title>
      <link>https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</link>
      <description><![CDATA[Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities. Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/indextts-an-industrial-level-controllable-and</guid>
    </item>
    <item>
      <title>ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/chartgalaxy-a-dataset-for-infographic-chart</link>
      <description><![CDATA[We showcase the utility of this dataset through: 1) improving infographic chart understanding via fine-tuning, 2) benchmarking code generation for infographic charts, and 3) enabling example-based infographic chart generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chartgalaxy-a-dataset-for-infographic-chart</guid>
    </item>
    <item>
      <title>Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers</title>
      <link>https://paperswithcode.com/paper/paper2poster-towards-multimodal-poster</link>
      <description><![CDATA[To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/paper2poster-towards-multimodal-poster</guid>
    </item>
    <item>
      <title>OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation</title>
      <link>https://paperswithcode.com/paper/opens2v-nexus-a-detailed-benchmark-and</link>
      <description><![CDATA[In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opens2v-nexus-a-detailed-benchmark-and</guid>
    </item>
    <item>
      <title>s3: You Don't Need That Much Data to Train a Search Agent via RL</title>
      <link>https://paperswithcode.com/paper/s3-you-don-t-need-that-much-data-to-train-a</link>
      <description><![CDATA[Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/s3-you-don-t-need-that-much-data-to-train-a</guid>
    </item>
    <item>
      <title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
      <link>https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</link>
      <description><![CDATA[Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</guid>
    </item>
    <item>
      <title>One-shot Entropy Minimization</title>
      <link>https://paperswithcode.com/paper/one-shot-entropy-minimization</link>
      <description><![CDATA[We trained 13, 440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-shot-entropy-minimization</guid>
    </item>
    <item>
      <title>Aligning Anime Video Generation with Human Feedback</title>
      <link>https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</link>
      <description><![CDATA[Existing reward models, designed primarily for real-world videos, fail to capture the unique appearance and consistency requirements of anime.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aligning-anime-video-generation-with-human</guid>
    </item>
    <item>
      <title>Learning to Reason without External Rewards</title>
      <link>https://paperswithcode.com/paper/learning-to-reason-without-external-rewards</link>
      <description><![CDATA[Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-to-reason-without-external-rewards</guid>
    </item>
    <item>
      <title>Multi-head Temporal Latent Attention</title>
      <link>https://paperswithcode.com/paper/multi-head-temporal-latent-attention</link>
      <description><![CDATA[While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-head-temporal-latent-attention</guid>
    </item>
    <item>
      <title>BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs</title>
      <link>https://paperswithcode.com/paper/bizfinbench-a-business-driven-real-world</link>
      <description><![CDATA[Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3. 5-Sonnet (63. 18) and DeepSeek-R1 (64. 04) lead, while smaller models like Qwen2. 5-VL-3B (15. 92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83. 58, Gemini-2. 0-Flash: 81. 15), with open-source models trailing by up to 19. 49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71. 46, while Qwen3-1. 7B scores 11. 23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39. 16 and 50. 00.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bizfinbench-a-business-driven-real-world</guid>
    </item>
    <item>
      <title>Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis</title>
      <link>https://paperswithcode.com/paper/reservoir-enhanced-segment-anything-model-for</link>
      <description><![CDATA[Urban roads and infrastructure, vital to city operations, face growing threats from subsurface anomalies like cracks and cavities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reservoir-enhanced-segment-anything-model-for</guid>
    </item>
  </channel>
</rss>
