<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Trending (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 23 Mar 2023 09:12:39 +0000</lastBuildDate>
    <item>
      <title>Zero-1-to-3: Zero-shot One Image to 3D Object</title>
      <link>https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</link>
      <description><![CDATA[We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-1-to-3-zero-shot-one-image-to-3d-object</guid>
    </item>
    <item>
      <title>VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</title>
      <link>https://paperswithcode.com/paper/voxelnext-fully-sparse-voxelnet-for-3d-object-1</link>
      <description><![CDATA[Our core insight is to predict objects directly based on sparse voxel features, without relying on hand-crafted proxies.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/voxelnext-fully-sparse-voxelnet-for-3d-object-1</guid>
    </item>
    <item>
      <title>A Dynamic Multi-Scale Voxel Flow Network for Video Prediction</title>
      <link>https://paperswithcode.com/paper/a-dynamic-multi-scale-voxel-flow-network-for</link>
      <description><![CDATA[The performance of video prediction has been greatly boosted by advanced deep neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-dynamic-multi-scale-voxel-flow-network-for</guid>
    </item>
    <item>
      <title>Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</title>
      <link>https://paperswithcode.com/paper/text2room-extracting-textured-3d-meshes-from</link>
      <description><![CDATA[We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/text2room-extracting-textured-3d-meshes-from</guid>
    </item>
    <item>
      <title>NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping</title>
      <link>https://paperswithcode.com/paper/nerf-loam-neural-implicit-representation-for</link>
      <description><![CDATA[To bridge this gap, in this paper, we propose a novel NeRF-based LiDAR odometry and mapping approach, NeRF-LOAM, consisting of three modules neural odometry, neural mapping, and mesh reconstruction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nerf-loam-neural-implicit-representation-for</guid>
    </item>
    <item>
      <title>LLaMA: Open and Efficient Foundation Language Models</title>
      <link>https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</link>
      <description><![CDATA[We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1</guid>
    </item>
    <item>
      <title>VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation</title>
      <link>https://paperswithcode.com/paper/decomposed-diffusion-models-for-high-quality</link>
      <description><![CDATA[A diffusion probabilistic model (DPM), which constructs a forward diffusion process by gradually adding noise to data points and learns the reverse denoising process to generate new samples, has been shown to handle complex data distribution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decomposed-diffusion-models-for-high-quality</guid>
    </item>
    <item>
      <title>CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition</title>
      <link>https://paperswithcode.com/paper/clip-goes-3d-leveraging-prompt-tuning-for</link>
      <description><![CDATA[Attempting to train the visual and text encoder to account for this shift results in catastrophic forgetting and a notable decrease in performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/clip-goes-3d-leveraging-prompt-tuning-for</guid>
    </item>
    <item>
      <title>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</title>
      <link>https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</link>
      <description><![CDATA[To this end, We build a system called \textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-chatgpt-talking-drawing-and-editing</guid>
    </item>
    <item>
      <title>FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</title>
      <link>https://paperswithcode.com/paper/fatezero-fusing-attentions-for-zero-shot-text</link>
      <description><![CDATA[We also have a better zero-shot shape-aware editing ability based on the text-to-video model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fatezero-fusing-attentions-for-zero-shot-text</guid>
    </item>
    <item>
      <title>Generative Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/generative-semantic-segmentation</link>
      <description><![CDATA[To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-semantic-segmentation</guid>
    </item>
    <item>
      <title>SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</title>
      <link>https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</link>
      <description><![CDATA[We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sadtalker-learning-realistic-3d-motion</guid>
    </item>
    <item>
      <title>Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws</title>
      <link>https://paperswithcode.com/paper/deep-symbolic-regression-for-physics-guided</link>
      <description><![CDATA[Here we present $\Phi$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-symbolic-regression-for-physics-guided</guid>
    </item>
    <item>
      <title>GPT-4 Technical Report</title>
      <link>https://paperswithcode.com/paper/gpt-4-technical-report-1</link>
      <description><![CDATA[We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpt-4-technical-report-1</guid>
    </item>
    <item>
      <title>Deep Learning for Camera Calibration and Beyond: A Survey</title>
      <link>https://paperswithcode.com/paper/deep-learning-for-camera-calibration-and</link>
      <description><![CDATA[In this paper, we provide a comprehensive survey of learning-based camera calibration techniques, by analyzing their strengths and limitations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-learning-for-camera-calibration-and</guid>
    </item>
    <item>
      <title>CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/cat-seg-cost-aggregation-for-open-vocabulary</link>
      <description><![CDATA[However, the problem of transferring these capabilities learned from image-level supervision to the pixel-level task of segmentation and addressing arbitrary unseen categories at inference makes this task challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cat-seg-cost-aggregation-for-open-vocabulary</guid>
    </item>
    <item>
      <title>GLM-130B: An Open Bilingual Pre-trained Model</title>
      <link>https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</link>
      <description><![CDATA[We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/glm-130b-an-open-bilingual-pre-trained-model</guid>
    </item>
    <item>
      <title>Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</title>
      <link>https://paperswithcode.com/paper/point-transformer-v2-grouped-vector-attention</link>
      <description><![CDATA[In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/point-transformer-v2-grouped-vector-attention</guid>
    </item>
    <item>
      <title>DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</title>
      <link>https://paperswithcode.com/paper/debertav3-improving-deberta-using-electra</link>
      <description><![CDATA[We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/debertav3-improving-deberta-using-electra</guid>
    </item>
    <item>
      <title>SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</title>
      <link>https://paperswithcode.com/paper/surroundocc-multi-camera-3d-occupancy</link>
      <description><![CDATA[Towards a more comprehensive perception of a 3D scene, in this paper, we propose a SurroundOcc method to predict the 3D occupancy with multi-camera images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/surroundocc-multi-camera-3d-occupancy</guid>
    </item>
  </channel>
</rss>
