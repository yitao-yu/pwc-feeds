<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 24 Feb 2025 09:17:59 +0000</lastBuildDate>
    <item>
      <title>SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features</title>
      <link>https://paperswithcode.com/paper/siglip-2-multilingual-vision-language</link>
      <description><![CDATA[We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/siglip-2-multilingual-vision-language</guid>
    </item>
    <item>
      <title>ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</title>
      <link>https://paperswithcode.com/paper/reqflow-rectified-quaternion-flow-for</link>
      <description><![CDATA[Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reqflow-rectified-quaternion-flow-for</guid>
    </item>
    <item>
      <title>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</title>
      <link>https://paperswithcode.com/paper/steca-step-level-trajectory-calibration-for</link>
      <description><![CDATA[To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/steca-step-level-trajectory-calibration-for</guid>
    </item>
    <item>
      <title>Distribution Matching for Self-Supervised Transfer Learning</title>
      <link>https://paperswithcode.com/paper/distribution-matching-for-self-supervised</link>
      <description><![CDATA[In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/distribution-matching-for-self-supervised</guid>
    </item>
    <item>
      <title>Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition</title>
      <link>https://paperswithcode.com/paper/bridging-text-and-vision-a-multi-view-text</link>
      <description><![CDATA[To overcome this challenge, we bridge text and vision by proposing a multiview (360{\deg} views of the surroundings) text-vision registration approach called Text4VPR for place recognition task, which is the first method that exclusively utilizes textual descriptions to match a database of images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bridging-text-and-vision-a-multi-view-text</guid>
    </item>
    <item>
      <title>From RAG to Memory: Non-Parametric Continual Learning for Large Language Models</title>
      <link>https://paperswithcode.com/paper/from-rag-to-memory-non-parametric-continual</link>
      <description><![CDATA[We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-rag-to-memory-non-parametric-continual</guid>
    </item>
    <item>
      <title>PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC</title>
      <link>https://paperswithcode.com/paper/pc-agent-a-hierarchical-multi-agent</link>
      <description><![CDATA[From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pc-agent-a-hierarchical-multi-agent</guid>
    </item>
    <item>
      <title>LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</title>
      <link>https://paperswithcode.com/paper/lserve-efficient-long-sequence-llm-serving</link>
      <description><![CDATA[On average, LServe accelerates LLM prefilling by up to 2. 9x and decoding by 1. 3-2. 1x over vLLM, maintaining long-context accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lserve-efficient-long-sequence-llm-serving</guid>
    </item>
    <item>
      <title>S*: Test Time Scaling for Code Generation</title>
      <link>https://paperswithcode.com/paper/s-test-time-scaling-for-code-generation</link>
      <description><![CDATA[Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/s-test-time-scaling-for-code-generation</guid>
    </item>
    <item>
      <title>Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity</title>
      <link>https://paperswithcode.com/paper/exploring-rwkv-for-sentence-embeddings-layer</link>
      <description><![CDATA[This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-rwkv-for-sentence-embeddings-layer</guid>
    </item>
    <item>
      <title>seqKAN: Sequence processing with Kolmogorov-Arnold Networks</title>
      <link>https://paperswithcode.com/paper/seqkan-sequence-processing-with-kolmogorov</link>
      <description><![CDATA[Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seqkan-sequence-processing-with-kolmogorov</guid>
    </item>
    <item>
      <title>PredictaBoard: Benchmarking LLM Score Predictability</title>
      <link>https://paperswithcode.com/paper/predictaboard-benchmarking-llm-score</link>
      <description><![CDATA[Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/predictaboard-benchmarking-llm-score</guid>
    </item>
    <item>
      <title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title>
      <link>https://paperswithcode.com/paper/how-much-knowledge-can-you-pack-into-a-lora-1</link>
      <description><![CDATA[The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-much-knowledge-can-you-pack-into-a-lora-1</guid>
    </item>
    <item>
      <title>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields</title>
      <link>https://paperswithcode.com/paper/medfuncta-modality-agnostic-representations</link>
      <description><![CDATA[Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medfuncta-modality-agnostic-representations</guid>
    </item>
    <item>
      <title>English Please: Evaluating Machine Translation for Multilingual Bug Reports</title>
      <link>https://paperswithcode.com/paper/english-please-evaluating-machine-translation</link>
      <description><![CDATA[In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/english-please-evaluating-machine-translation</guid>
    </item>
    <item>
      <title>Pre-training Graph Neural Networks on Molecules by Using Subgraph-Conditioned Graph Information Bottleneck</title>
      <link>https://paperswithcode.com/paper/pre-training-graph-neural-networks-on</link>
      <description><![CDATA[The key challenge to build a pre-trained GNN on molecules is how to (1) generate well-distinguished graph-level representations and (2) automatically discover the functional groups without prior knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pre-training-graph-neural-networks-on</guid>
    </item>
    <item>
      <title>Judging the Judges: A Collection of LLM-Generated Relevance Judgements</title>
      <link>https://paperswithcode.com/paper/judging-the-judges-a-collection-of-llm</link>
      <description><![CDATA[Using Large Language Models (LLMs) for relevance assessments offers promising opportunities to improve Information Retrieval (IR), Natural Language Processing (NLP), and related fields.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/judging-the-judges-a-collection-of-llm</guid>
    </item>
    <item>
      <title>Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?</title>
      <link>https://paperswithcode.com/paper/lost-in-sequence-do-large-language-models</link>
      <description><![CDATA[Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users' item interaction sequences has been largely overlooked.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lost-in-sequence-do-large-language-models</guid>
    </item>
    <item>
      <title>PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference</title>
      <link>https://paperswithcode.com/paper/pldr-llms-learn-a-generalizable-tensor</link>
      <description><![CDATA[We show that Large Language Model from Power Law Decoder Representations (PLDR-LLM) is a foundational model whose deductive outputs are invariant tensors up to a small perturbation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pldr-llms-learn-a-generalizable-tensor</guid>
    </item>
    <item>
      <title>Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models</title>
      <link>https://paperswithcode.com/paper/train-small-infer-large-memory-efficient-lora</link>
      <description><![CDATA[For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/train-small-infer-large-memory-efficient-lora</guid>
    </item>
    <item>
      <title>Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks</title>
      <link>https://paperswithcode.com/paper/measuring-the-effect-of-transcription-noise</link>
      <description><![CDATA[For instance, we find that task models can tolerate a certain level of noise, and are affected differently by the types of errors in the transcript.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/measuring-the-effect-of-transcription-noise</guid>
    </item>
    <item>
      <title>Toeplitz-Hermitian ADMM-Net for DoA Estimation</title>
      <link>https://paperswithcode.com/paper/toeplitz-hermitian-admm-net-for-doa</link>
      <description><![CDATA[This paper presents Toeplitz-Hermitian ADMM-Net (THADMM-Net), a deep neural network obtained by deep unfolding the alternating direction method of multipliers (ADMM) algorithm for solving the least absolute shrinkage thresholding operator problem in the context of direction of arrival estimation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/toeplitz-hermitian-admm-net-for-doa</guid>
    </item>
    <item>
      <title>Calibration and Option Pricing with Stochastic Volatility and Double Exponential Jumps</title>
      <link>https://paperswithcode.com/paper/calibration-and-option-pricing-with</link>
      <description><![CDATA[We provide evidence that this model outperforms challenger models possessing similar features (stochastic volatility and jumps), especially in the fit of the short term implied volatility smile, and that it is particularly tractable for the pricing of exotic options from different generations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/calibration-and-option-pricing-with</guid>
    </item>
    <item>
      <title>Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective</title>
      <link>https://paperswithcode.com/paper/noise-may-contain-transferable-knowledge</link>
      <description><![CDATA[Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/noise-may-contain-transferable-knowledge</guid>
    </item>
    <item>
      <title>Deep Learning for VWAP Execution in Crypto Markets: Beyond the Volume Curve</title>
      <link>https://paperswithcode.com/paper/deep-learning-for-vwap-execution-in-crypto</link>
      <description><![CDATA[This research contributes a more efficient and robust framework for VWAP execution in volatile markets, illustrating the potential of deep learning in complex financial systems where direct objective optimization is crucial.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-learning-for-vwap-execution-in-crypto</guid>
    </item>
    <item>
      <title>Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models</title>
      <link>https://paperswithcode.com/paper/refining-sentence-embedding-model-through</link>
      <description><![CDATA[Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/refining-sentence-embedding-model-through</guid>
    </item>
    <item>
      <title>Unsupervised Graph Embeddings for Session-based Recommendation with Item Features</title>
      <link>https://paperswithcode.com/paper/unsupervised-graph-embeddings-for-session</link>
      <description><![CDATA[State-of-the-art sequential recommendation algorithms either use graph neural networks to model sessions in a graph or leverage the similarity of sessions by exploiting item features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-graph-embeddings-for-session</guid>
    </item>
    <item>
      <title>Structural determinants of soft memory in recurrent biological networks</title>
      <link>https://paperswithcode.com/paper/structural-determinants-of-soft-memory-in</link>
      <description><![CDATA[Recurrent neural networks are frequently studied in terms of their information-processing capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/structural-determinants-of-soft-memory-in</guid>
    </item>
    <item>
      <title>Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI</title>
      <link>https://paperswithcode.com/paper/fighter-jet-navigation-and-combat-using-deep</link>
      <description><![CDATA[This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fighter-jet-navigation-and-combat-using-deep</guid>
    </item>
    <item>
      <title>Emergence of the Primacy Effect in Structured State-Space Models</title>
      <link>https://paperswithcode.com/paper/emergence-of-the-primacy-effect-in-structured</link>
      <description><![CDATA[Human and animal memory for sequentially presented items is well-documented to be more accurate for those at the beginning and end of the sequence, phenomena known as the primacy and recency effects, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/emergence-of-the-primacy-effect-in-structured</guid>
    </item>
    <item>
      <title>Stability of difference equations with interspecific density dependence, competition, and maturation delays</title>
      <link>https://paperswithcode.com/paper/stability-of-difference-equations-with</link>
      <description><![CDATA[The stability of coexistence depends on the relative abundances of the species at the unique interior equilibrium.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stability-of-difference-equations-with</guid>
    </item>
    <item>
      <title>Empirical Study of Dynamic Regret in Online Model Predictive Control for Linear Time-Varying Systems</title>
      <link>https://paperswithcode.com/paper/empirical-study-of-dynamic-regret-in-online</link>
      <description><![CDATA[Model Predictive Control (MPC) is a widely used technique for managing timevarying systems, supported by extensive theoretical analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/empirical-study-of-dynamic-regret-in-online</guid>
    </item>
    <item>
      <title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title>
      <link>https://paperswithcode.com/paper/adaptivestep-automatically-dividing-reasoning</link>
      <description><![CDATA[Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptivestep-automatically-dividing-reasoning</guid>
    </item>
    <item>
      <title>LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization</title>
      <link>https://paperswithcode.com/paper/longpo-long-context-self-evolution-of-large</link>
      <description><![CDATA[Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longpo-long-context-self-evolution-of-large</guid>
    </item>
    <item>
      <title>Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs</title>
      <link>https://paperswithcode.com/paper/democratizing-large-language-model-based</link>
      <description><![CDATA[Most of the existing augmentation methods overlook the context information inherited from the dataset as they rely solely on the graph structure for augmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/democratizing-large-language-model-based</guid>
    </item>
    <item>
      <title>Identifying metric structures of deep latent variable models</title>
      <link>https://paperswithcode.com/paper/identifying-metric-structures-of-deep-latent</link>
      <description><![CDATA[Current solutions limit the lack of identifiability through additional constraints on the latent variable model, e. g. by requiring labeled training data, or by restricting the expressivity of the model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/identifying-metric-structures-of-deep-latent</guid>
    </item>
    <item>
      <title>Qwen2.5-VL Technical Report</title>
      <link>https://paperswithcode.com/paper/qwen2-5-vl-technical-report</link>
      <description><![CDATA[We introduce Qwen2. 5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qwen2-5-vl-technical-report</guid>
    </item>
    <item>
      <title>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis</title>
      <link>https://paperswithcode.com/paper/mobilevim-a-light-weight-and-dimension</link>
      <description><![CDATA[Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilevim-a-light-weight-and-dimension</guid>
    </item>
    <item>
      <title>MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads</title>
      <link>https://paperswithcode.com/paper/mudaf-long-context-multi-document-attention</link>
      <description><![CDATA[Large Language Models (LLMs) frequently show distracted attention due to irrelevant information in the input, which severely impairs their long-context capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mudaf-long-context-multi-document-attention</guid>
    </item>
    <item>
      <title>2.5D U-Net with Depth Reduction for 3D CryoET Object Identification</title>
      <link>https://paperswithcode.com/paper/2-5d-u-net-with-depth-reduction-for-3d-cryoet</link>
      <description><![CDATA[Cryo-electron tomography (cryoET) is a crucial technique for unveiling the structure of protein complexes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/2-5d-u-net-with-depth-reduction-for-3d-cryoet</guid>
    </item>
    <item>
      <title>JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</title>
      <link>https://paperswithcode.com/paper/jl1-cd-a-new-benchmark-for-remote-sensing</link>
      <description><![CDATA[Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/jl1-cd-a-new-benchmark-for-remote-sensing</guid>
    </item>
    <item>
      <title>Pretrained Image-Text Models are Secretly Video Captioners</title>
      <link>https://paperswithcode.com/paper/pretrained-image-text-models-are-secretly</link>
      <description><![CDATA[Developing video captioning models is computationally expensive.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pretrained-image-text-models-are-secretly</guid>
    </item>
    <item>
      <title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
      <link>https://paperswithcode.com/paper/mom-linear-sequence-modeling-with-mixture-of</link>
      <description><![CDATA[Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mom-linear-sequence-modeling-with-mixture-of</guid>
    </item>
    <item>
      <title>Fine-grained Fallacy Detection with Human Label Variation</title>
      <link>https://paperswithcode.com/paper/fine-grained-fallacy-detection-with-human</link>
      <description><![CDATA[We introduce Faina, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-grained-fallacy-detection-with-human</guid>
    </item>
    <item>
      <title>DataSciBench: An LLM Agent Benchmark for Data Science</title>
      <link>https://paperswithcode.com/paper/datascibench-an-llm-agent-benchmark-for-data</link>
      <description><![CDATA[In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datascibench-an-llm-agent-benchmark-for-data</guid>
    </item>
    <item>
      <title>TESS 2: A Large-Scale Generalist Diffusion Language Model</title>
      <link>https://paperswithcode.com/paper/tess-2-a-large-scale-generalist-diffusion</link>
      <description><![CDATA[We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tess-2-a-large-scale-generalist-diffusion</guid>
    </item>
    <item>
      <title>Playing Hex and Counter Wargames using Reinforcement Learning and Recurrent Neural Networks</title>
      <link>https://paperswithcode.com/paper/playing-hex-and-counter-wargames-using</link>
      <description><![CDATA[Hex and Counter Wargames are adversarial two-player simulations of real military conflicts requiring complex strategic decision-making.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/playing-hex-and-counter-wargames-using</guid>
    </item>
    <item>
      <title>MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification</title>
      <link>https://paperswithcode.com/paper/mm-verify-enhancing-multimodal-reasoning-with</link>
      <description><![CDATA[Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65. 3 on MathVista, surpassing GPT-4o (63. 8) with 12 rollouts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mm-verify-enhancing-multimodal-reasoning-with</guid>
    </item>
    <item>
      <title>Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics</title>
      <link>https://paperswithcode.com/paper/helix-mrna-a-hybrid-foundation-model-for-full</link>
      <description><![CDATA[We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/helix-mrna-a-hybrid-foundation-model-for-full</guid>
    </item>
    <item>
      <title>LESA: Learnable LLM Layer Scaling-Up</title>
      <link>https://paperswithcode.com/paper/lesa-learnable-llm-layer-scaling-up</link>
      <description><![CDATA[LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lesa-learnable-llm-layer-scaling-up</guid>
    </item>
  </channel>
</rss>
