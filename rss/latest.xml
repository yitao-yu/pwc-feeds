<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 28 Feb 2024 21:07:06 +0000</lastBuildDate>
    <item>
      <title>OSCaR: Object State Captioning and State Change Representation</title>
      <link>https://paperswithcode.com/paper/oscar-object-state-captioning-and-state</link>
      <description><![CDATA[To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/oscar-object-state-captioning-and-state</guid>
    </item>
    <item>
      <title>Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing</title>
      <link>https://paperswithcode.com/paper/implicit-regularization-via-spectral-neural</link>
      <description><![CDATA[In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradient descent. In this vein, we contribute a network architecture called Spectral Neural Networks (abbrv.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/implicit-regularization-via-spectral-neural</guid>
    </item>
    <item>
      <title>Beacon, a lightweight deep reinforcement learning benchmark library for flow control</title>
      <link>https://paperswithcode.com/paper/beacon-a-lightweight-deep-reinforcement</link>
      <description><![CDATA[Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/beacon-a-lightweight-deep-reinforcement</guid>
    </item>
    <item>
      <title>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</title>
      <link>https://paperswithcode.com/paper/gradient-based-discrete-sampling-with</link>
      <description><![CDATA[Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gradient-based-discrete-sampling-with</guid>
    </item>
    <item>
      <title>Massive Activations in Large Language Models</title>
      <link>https://paperswithcode.com/paper/massive-activations-in-large-language-models</link>
      <description><![CDATA[We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e. g., 100, 000 times larger).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/massive-activations-in-large-language-models</guid>
    </item>
    <item>
      <title>Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective</title>
      <link>https://paperswithcode.com/paper/consistency-matters-explore-llms-consistency</link>
      <description><![CDATA[The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/consistency-matters-explore-llms-consistency</guid>
    </item>
    <item>
      <title>Reinforced In-Context Black-Box Optimization</title>
      <link>https://paperswithcode.com/paper/reinforced-in-context-black-box-optimization</link>
      <description><![CDATA[In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforced-in-context-black-box-optimization</guid>
    </item>
    <item>
      <title>KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark</title>
      <link>https://paperswithcode.com/paper/kodialogbench-evaluating-conversational</link>
      <description><![CDATA[As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kodialogbench-evaluating-conversational</guid>
    </item>
    <item>
      <title>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</title>
      <link>https://paperswithcode.com/paper/sora-a-review-on-background-technology</link>
      <description><![CDATA[Sora is a text-to-video generative AI model, released by OpenAI in February 2024.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sora-a-review-on-background-technology</guid>
    </item>
    <item>
      <title>Diffusion Model-Based Image Editing: A Survey</title>
      <link>https://paperswithcode.com/paper/diffusion-model-based-image-editing-a-survey</link>
      <description><![CDATA[In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-model-based-image-editing-a-survey</guid>
    </item>
    <item>
      <title>The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns</title>
      <link>https://paperswithcode.com/paper/the-kandy-benchmark-incremental-neuro</link>
      <description><![CDATA[Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-kandy-benchmark-incremental-neuro</guid>
    </item>
    <item>
      <title>Scaling Supervised Local Learning with Augmented Auxiliary Networks</title>
      <link>https://paperswithcode.com/paper/scaling-supervised-local-learning-with</link>
      <description><![CDATA[AugLocal constructs each hidden layer's auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-supervised-local-learning-with</guid>
    </item>
    <item>
      <title>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</title>
      <link>https://paperswithcode.com/paper/learning-dynamic-tetrahedra-for-high-quality</link>
      <description><![CDATA[Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-dynamic-tetrahedra-for-high-quality</guid>
    </item>
    <item>
      <title>Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation</title>
      <link>https://paperswithcode.com/paper/metasql-a-generate-then-rank-framework-for</link>
      <description><![CDATA[While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metasql-a-generate-then-rank-framework-for</guid>
    </item>
    <item>
      <title>Towards Fairness-Aware Adversarial Learning</title>
      <link>https://paperswithcode.com/paper/towards-fairness-aware-adversarial-learning</link>
      <description><![CDATA[As a generalization of conventional AT, we re-define the problem of adversarial training as a min-max-max framework, to ensure both robustness and fairness of the trained model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-fairness-aware-adversarial-learning</guid>
    </item>
    <item>
      <title>Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning</title>
      <link>https://paperswithcode.com/paper/predicting-instability-in-complex-oscillator</link>
      <description><![CDATA[A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/predicting-instability-in-complex-oscillator</guid>
    </item>
    <item>
      <title>RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations</title>
      <link>https://paperswithcode.com/paper/ravel-evaluating-interpretability-methods-on</link>
      <description><![CDATA[Individual neurons participate in the representation of multiple high-level concepts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ravel-evaluating-interpretability-methods-on</guid>
    </item>
    <item>
      <title>PANDAS: Prototype-based Novel Class Discovery and Detection</title>
      <link>https://paperswithcode.com/paper/pandas-prototype-based-novel-class-discovery</link>
      <description><![CDATA[In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pandas-prototype-based-novel-class-discovery</guid>
    </item>
    <item>
      <title>Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation</title>
      <link>https://paperswithcode.com/paper/scaling-on-chip-photonic-neural-processors</link>
      <description><![CDATA[On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-on-chip-photonic-neural-processors</guid>
    </item>
    <item>
      <title>Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?</title>
      <link>https://paperswithcode.com/paper/prescribing-large-language-models-for</link>
      <description><![CDATA[Adapting models further improved performance: (1) self-supervised finetuning by 3. 2% for AUROC and 1. 5% for AUPRC; (2) semi-supervised finetuning by 1. 8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3. 6% for AUROC and 2. 6% for AUPRC, compared to self-supervised finetuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prescribing-large-language-models-for</guid>
    </item>
    <item>
      <title>Purified and Unified Steganographic Network</title>
      <link>https://paperswithcode.com/paper/purified-and-unified-steganographic-network</link>
      <description><![CDATA[It is also shown to be capable of imperceptibly carrying the steganographic networks in a purified network.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/purified-and-unified-steganographic-network</guid>
    </item>
    <item>
      <title>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</title>
      <link>https://paperswithcode.com/paper/deepdrk-deep-dependency-regularized-knockoff</link>
      <description><![CDATA[In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the "swap property".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepdrk-deep-dependency-regularized-knockoff</guid>
    </item>
    <item>
      <title>Principled Architecture-aware Scaling of Hyperparameters</title>
      <link>https://paperswithcode.com/paper/principled-architecture-aware-scaling-of</link>
      <description><![CDATA[However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/principled-architecture-aware-scaling-of</guid>
    </item>
    <item>
      <title>Efficient Backpropagation with Variance-Controlled Adaptive Sampling</title>
      <link>https://paperswithcode.com/paper/efficient-backpropagation-with-variance</link>
      <description><![CDATA[On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73. 87% FLOPs reduction of BP and 49. 58% FLOPs reduction of the whole training process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-backpropagation-with-variance</guid>
    </item>
    <item>
      <title>Variational Learning is Effective for Large Deep Networks</title>
      <link>https://paperswithcode.com/paper/variational-learning-is-effective-for-large</link>
      <description><![CDATA[We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/variational-learning-is-effective-for-large</guid>
    </item>
    <item>
      <title>Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology</title>
      <link>https://paperswithcode.com/paper/feature-re-embedding-towards-foundation-model</link>
      <description><![CDATA[Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/feature-re-embedding-towards-foundation-model</guid>
    </item>
    <item>
      <title>Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data</title>
      <link>https://paperswithcode.com/paper/are-llms-capable-of-data-based-statistical</link>
      <description><![CDATA[To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/are-llms-capable-of-data-based-statistical</guid>
    </item>
    <item>
      <title>Explicit Interaction for Fusion-Based Place Recognition</title>
      <link>https://paperswithcode.com/paper/explicit-interaction-for-fusion-based-place</link>
      <description><![CDATA[Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/explicit-interaction-for-fusion-based-place</guid>
    </item>
    <item>
      <title>TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation</title>
      <link>https://paperswithcode.com/paper/taxdiff-taxonomic-guided-diffusion-model-for</link>
      <description><![CDATA[In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taxdiff-taxonomic-guided-diffusion-model-for</guid>
    </item>
    <item>
      <title>AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis</title>
      <link>https://paperswithcode.com/paper/alignmif-geometry-aligned-multimodal-implicit</link>
      <description><![CDATA[Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/alignmif-geometry-aligned-multimodal-implicit</guid>
    </item>
    <item>
      <title>Sinkhorn Distance Minimization for Knowledge Distillation</title>
      <link>https://paperswithcode.com/paper/sinkhorn-distance-minimization-for-knowledge</link>
      <description><![CDATA[We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sinkhorn-distance-minimization-for-knowledge</guid>
    </item>
    <item>
      <title>TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations</title>
      <link>https://paperswithcode.com/paper/torchmd-net-2-0-fast-neural-network</link>
      <description><![CDATA[Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/torchmd-net-2-0-fast-neural-network</guid>
    </item>
    <item>
      <title>Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</title>
      <link>https://paperswithcode.com/paper/scribble-hides-class-promoting-scribble-based</link>
      <description><![CDATA[In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scribble-hides-class-promoting-scribble-based</guid>
    </item>
    <item>
      <title>Case-Based or Rule-Based: How Do Transformers Do the Math?</title>
      <link>https://paperswithcode.com/paper/case-based-or-rule-based-how-do-transformers</link>
      <description><![CDATA[Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/case-based-or-rule-based-how-do-transformers</guid>
    </item>
    <item>
      <title>Benchmarking Data Science Agents</title>
      <link>https://paperswithcode.com/paper/benchmarking-data-science-agents</link>
      <description><![CDATA[In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/benchmarking-data-science-agents</guid>
    </item>
    <item>
      <title>Quantifying the Resolution of a Template after Image Registration</title>
      <link>https://paperswithcode.com/paper/quantifying-the-resolution-of-a-template</link>
      <description><![CDATA[In many image processing applications (e. g. computational anatomy) a groupwise registration is performed on a sample of images and a template image is simultaneously generated.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/quantifying-the-resolution-of-a-template</guid>
    </item>
    <item>
      <title>Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)</title>
      <link>https://paperswithcode.com/paper/linguistic-knowledge-can-enhance-encoder</link>
      <description><![CDATA[In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/linguistic-knowledge-can-enhance-encoder</guid>
    </item>
    <item>
      <title>SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution</title>
      <link>https://paperswithcode.com/paper/sam-diffsr-structure-modulated-diffusion</link>
      <description><![CDATA[In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sam-diffsr-structure-modulated-diffusion</guid>
    </item>
    <item>
      <title>Active propulsion noise shaping for multi-rotor aircraft localization</title>
      <link>https://paperswithcode.com/paper/active-propulsion-noise-shaping-for-multi</link>
      <description><![CDATA[Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/active-propulsion-noise-shaping-for-multi</guid>
    </item>
    <item>
      <title>Label-Noise Robust Diffusion Models</title>
      <link>https://paperswithcode.com/paper/label-noise-robust-diffusion-models</link>
      <description><![CDATA[This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/label-noise-robust-diffusion-models</guid>
    </item>
    <item>
      <title>MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation</title>
      <link>https://paperswithcode.com/paper/medcontext-learning-contextual-cues-for</link>
      <description><![CDATA[The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medcontext-learning-contextual-cues-for</guid>
    </item>
    <item>
      <title>Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control</title>
      <link>https://paperswithcode.com/paper/impact-of-computation-in-integral</link>
      <description><![CDATA[We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat\'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat\'ern kernel's smoothness parameter.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/impact-of-computation-in-integral</guid>
    </item>
    <item>
      <title>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</title>
      <link>https://paperswithcode.com/paper/coupled-laplacian-eigenmaps-for-locally-aware</link>
      <description><![CDATA[In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/coupled-laplacian-eigenmaps-for-locally-aware</guid>
    </item>
    <item>
      <title>Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation</title>
      <link>https://paperswithcode.com/paper/towards-robust-and-efficient-cloud-edge</link>
      <description><![CDATA[Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-robust-and-efficient-cloud-edge</guid>
    </item>
    <item>
      <title>Exploiting Emotion-Semantic Correlations for Empathetic Response Generation</title>
      <link>https://paperswithcode.com/paper/exploiting-emotion-semantic-correlations-for</link>
      <description><![CDATA[Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation graph convolutional network to guide the model in learning context meanings in dialogue and generating empathetic responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploiting-emotion-semantic-correlations-for</guid>
    </item>
    <item>
      <title>Supervised machine learning for microbiomics: bridging the gap between current and best practices</title>
      <link>https://paperswithcode.com/paper/supervised-machine-learning-for-microbiomics</link>
      <description><![CDATA[Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/supervised-machine-learning-for-microbiomics</guid>
    </item>
    <item>
      <title>PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning</title>
      <link>https://paperswithcode.com/paper/promptmm-multi-modal-knowledge-distillation</link>
      <description><![CDATA[Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled multi-modal list-wise distillation is developed with modality-aware re-weighting mechanism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/promptmm-multi-modal-knowledge-distillation</guid>
    </item>
    <item>
      <title>SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification</title>
      <link>https://paperswithcode.com/paper/sdf2net-shallow-to-deep-feature-fusion</link>
      <description><![CDATA[Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sdf2net-shallow-to-deep-feature-fusion</guid>
    </item>
    <item>
      <title>QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations</title>
      <link>https://paperswithcode.com/paper/quce-the-minimisation-and-quantification-of</link>
      <description><![CDATA[In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/quce-the-minimisation-and-quantification-of</guid>
    </item>
    <item>
      <title>Preserving Fairness Generalization in Deepfake Detection</title>
      <link>https://paperswithcode.com/paper/preserving-fairness-generalization-in</link>
      <description><![CDATA[The existing method for addressing this problem is providing a fair loss function.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/preserving-fairness-generalization-in</guid>
    </item>
  </channel>
</rss>
