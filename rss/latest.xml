<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 24 May 2024 21:07:08 +0000</lastBuildDate>
    <item>
      <title>TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/timemixer-decomposable-multiscale-mixing-for-1</link>
      <description><![CDATA[Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, which is based on an intuitive but important observation that time series present distinct patterns in different sampling scales.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/timemixer-decomposable-multiscale-mixing-for-1</guid>
    </item>
    <item>
      <title>Which Information Matters? Dissecting Human-written Multi-document Summaries with Partial Information Decomposition</title>
      <link>https://paperswithcode.com/paper/which-information-matters-dissecting-human</link>
      <description><![CDATA[Understanding the nature of high-quality summaries is crucial to further improve the performance of multi-document summarization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/which-information-matters-dissecting-human</guid>
    </item>
    <item>
      <title>Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title>
      <link>https://paperswithcode.com/paper/markovian-flow-matching-accelerating-mcmc</link>
      <description><![CDATA[Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/markovian-flow-matching-accelerating-mcmc</guid>
    </item>
    <item>
      <title>Prediction of cancer dynamics under treatment using Bayesian neural networks: A simulated study</title>
      <link>https://paperswithcode.com/paper/prediction-of-cancer-dynamics-under-treatment</link>
      <description><![CDATA[In this work, we develop a hierarchical Bayesian model of subpopulation dynamics that uses baseline covariate information to predict cancer dynamics under treatment, inspired by cancer dynamics in multiple myeloma (MM), where serum M protein is a well-known proxy of tumor burden.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prediction-of-cancer-dynamics-under-treatment</guid>
    </item>
    <item>
      <title>Newton Informed Neural Operator for Computing Multiple Solutions of Nonlinear Partials Differential Equations</title>
      <link>https://paperswithcode.com/paper/newton-informed-neural-operator-for-computing</link>
      <description><![CDATA[Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/newton-informed-neural-operator-for-computing</guid>
    </item>
    <item>
      <title>Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs</title>
      <link>https://paperswithcode.com/paper/mitigating-quantization-errors-due-to</link>
      <description><![CDATA[In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mitigating-quantization-errors-due-to</guid>
    </item>
    <item>
      <title>A Watermark for Low-entropy and Unbiased Generation in Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-watermark-for-low-entropy-and-unbiased</link>
      <description><![CDATA[This study proposes the Sampling One Then Accepting (STA-1) method, an unbiased watermark that does not require access to LLMs nor prompts during detection and has statistical guarantees for the type II error.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-watermark-for-low-entropy-and-unbiased</guid>
    </item>
    <item>
      <title>Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models</title>
      <link>https://paperswithcode.com/paper/impact-of-non-standard-unicode-characters-on</link>
      <description><![CDATA[Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/impact-of-non-standard-unicode-characters-on</guid>
    </item>
    <item>
      <title>U-TELL: Unsupervised Task Expert Lifelong Learning</title>
      <link>https://paperswithcode.com/paper/u-tell-unsupervised-task-expert-lifelong</link>
      <description><![CDATA[To address these issues, we propose an unsupervised CL model with task experts called Unsupervised Task Expert Lifelong Learning (U-TELL) to continually learn the data arriving in a sequence addressing catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/u-tell-unsupervised-task-expert-lifelong</guid>
    </item>
    <item>
      <title>Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models</title>
      <link>https://paperswithcode.com/paper/combining-denoising-autoencoders-with</link>
      <description><![CDATA[Recently, using large pretrained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters or combinations with unsupervised approaches, among many others.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/combining-denoising-autoencoders-with</guid>
    </item>
    <item>
      <title>EHRMamba: Towards Generalizable and Scalable Foundation Models for Electronic Health Records</title>
      <link>https://paperswithcode.com/paper/ehrmamba-towards-generalizable-and-scalable</link>
      <description><![CDATA[We also introduce a novel approach to Multitask Prompted Finetuning (MTF) for EHR data, which enables EHRMamba to simultaneously learn multiple clinical tasks in a single finetuning phase, significantly enhancing deployment and cross-task generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ehrmamba-towards-generalizable-and-scalable</guid>
    </item>
    <item>
      <title>RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models</title>
      <link>https://paperswithcode.com/paper/refchecker-reference-based-fine-grained</link>
      <description><![CDATA[In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/refchecker-reference-based-fine-grained</guid>
    </item>
    <item>
      <title>Controllable Continual Test-Time Adaptation</title>
      <link>https://paperswithcode.com/paper/controllable-continual-test-time-adaptation</link>
      <description><![CDATA[Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controllable-continual-test-time-adaptation</guid>
    </item>
    <item>
      <title>TerDiT: Ternary Diffusion Models with Transformers</title>
      <link>https://paperswithcode.com/paper/terdit-ternary-diffusion-models-with</link>
      <description><![CDATA[Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/terdit-ternary-diffusion-models-with</guid>
    </item>
    <item>
      <title>ReactXT: Understanding Molecular "Reaction-ship" via Reaction-Contextualized Molecule-Text Pretraining</title>
      <link>https://paperswithcode.com/paper/reactxt-understanding-molecular-reaction-ship</link>
      <description><![CDATA[To resolve the challenges above, we propose a new pretraining method, ReactXT, for reaction-text modeling, and a new dataset, OpenExp, for experimental procedure prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reactxt-understanding-molecular-reaction-ship</guid>
    </item>
    <item>
      <title>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</title>
      <link>https://paperswithcode.com/paper/rectifid-personalizing-rectified-flow-with</link>
      <description><![CDATA[Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rectifid-personalizing-rectified-flow-with</guid>
    </item>
    <item>
      <title>SimPO: Simple Preference Optimization with a Reference-Free Reward</title>
      <link>https://paperswithcode.com/paper/simpo-simple-preference-optimization-with-a</link>
      <description><![CDATA[Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44. 7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33. 8 win rate on Arena-Hard -- making it the strongest 8B open-source model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simpo-simple-preference-optimization-with-a</guid>
    </item>
    <item>
      <title>FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models</title>
      <link>https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</link>
      <description><![CDATA[As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</guid>
    </item>
    <item>
      <title>Scalable Optimization in the Modular Norm</title>
      <link>https://paperswithcode.com/paper/scalable-optimization-in-the-modular-norm</link>
      <description><![CDATA[When ramping up the width of a single layer, graceful scaling of training has been linked to the need to normalize the weights and their updates in the "natural norm" particular to that layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-optimization-in-the-modular-norm</guid>
    </item>
    <item>
      <title>Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution</title>
      <link>https://paperswithcode.com/paper/differentiable-annealed-importance-sampling-1</link>
      <description><![CDATA[Differentiable annealed importance sampling (DAIS), proposed by Geffner & Domke (2021) and Zhang et al. (2021), allows optimizing, among others, over the initial distribution of AIS.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/differentiable-annealed-importance-sampling-1</guid>
    </item>
    <item>
      <title>An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models</title>
      <link>https://paperswithcode.com/paper/an-empirical-study-of-training-state-of-the</link>
      <description><![CDATA[In the rapidly evolving field of autonomous driving, precise segmentation of LiDAR data is crucial for understanding complex 3D environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-empirical-study-of-training-state-of-the</guid>
    </item>
    <item>
      <title>CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments</title>
      <link>https://paperswithcode.com/paper/coped-advancing-multi-robot-collaborative</link>
      <description><![CDATA[We believe this work will unlock the potential research of high-level scene understanding through multi-modal collaborative perception in multi-robot settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/coped-advancing-multi-robot-collaborative</guid>
    </item>
    <item>
      <title>ViHateT5: Enhancing Hate Speech Detection in Vietnamese With A Unified Text-to-Text Transformer Model</title>
      <link>https://paperswithcode.com/paper/vihatet5-enhancing-hate-speech-detection-in</link>
      <description><![CDATA[Recent advancements in hate speech detection (HSD) in Vietnamese have made significant progress, primarily attributed to the emergence of transformer-based pre-trained language models, particularly those built on the BERT architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vihatet5-enhancing-hate-speech-detection-in</guid>
    </item>
    <item>
      <title>MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability</title>
      <link>https://paperswithcode.com/paper/mogu-a-framework-for-enhancing-safety-of-open</link>
      <description><![CDATA[When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mogu-a-framework-for-enhancing-safety-of-open</guid>
    </item>
    <item>
      <title>YOLOv10: Real-Time End-to-End Object Detection</title>
      <link>https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</link>
      <description><![CDATA[In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</guid>
    </item>
    <item>
      <title>GCondenser: Benchmarking Graph Condensation</title>
      <link>https://paperswithcode.com/paper/gcondenser-benchmarking-graph-condensation</link>
      <description><![CDATA[Large-scale graphs are valuable for graph representation learning, yet the abundant data in these graphs hinders the efficiency of the training process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gcondenser-benchmarking-graph-condensation</guid>
    </item>
    <item>
      <title>LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention Networks</title>
      <link>https://paperswithcode.com/paper/lora-ensemble-efficient-uncertainty-modelling</link>
      <description><![CDATA[We introduce LoRA-Ensemble, a parameter-efficient deep ensemble method for self-attention networks, which is based on Low-Rank Adaptation (LoRA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-ensemble-efficient-uncertainty-modelling</guid>
    </item>
    <item>
      <title>Joining up the scattered anticancer knowledge on auraptene and umbelliprenin: a meta-analysis</title>
      <link>https://paperswithcode.com/paper/joining-up-the-scattered-anticancer-knowledge</link>
      <description><![CDATA[Mixed-effects models revealed significant negative associations between coumarin dose and viability for AUR (est.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/joining-up-the-scattered-anticancer-knowledge</guid>
    </item>
    <item>
      <title>Not All Language Model Features Are Linear</title>
      <link>https://paperswithcode.com/paper/not-all-language-model-features-are-linear</link>
      <description><![CDATA[Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/not-all-language-model-features-are-linear</guid>
    </item>
    <item>
      <title>D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup</title>
      <link>https://paperswithcode.com/paper/d-miso-editing-dynamic-3d-scenes-using-multi</link>
      <description><![CDATA[Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-miso-editing-dynamic-3d-scenes-using-multi</guid>
    </item>
    <item>
      <title>Adaptive Teaching in Heterogeneous Agents: Balancing Surprise in Sparse Reward Scenarios</title>
      <link>https://paperswithcode.com/paper/adaptive-teaching-in-heterogeneous-agents</link>
      <description><![CDATA[Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptive-teaching-in-heterogeneous-agents</guid>
    </item>
    <item>
      <title>WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models</title>
      <link>https://paperswithcode.com/paper/wise-rethinking-the-knowledge-memory-for</link>
      <description><![CDATA[In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wise-rethinking-the-knowledge-memory-for</guid>
    </item>
    <item>
      <title>CAPE: Context-Adaptive Positional Encoding for Length Extrapolation</title>
      <link>https://paperswithcode.com/paper/cape-context-adaptive-positional-encoding-for</link>
      <description><![CDATA[Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cape-context-adaptive-positional-encoding-for</guid>
    </item>
    <item>
      <title>A Neighbor-Searching Discrepancy-based Drift Detection Scheme for Learning Evolving Data</title>
      <link>https://paperswithcode.com/paper/a-neighbor-searching-discrepancy-based-drift</link>
      <description><![CDATA[Uncertain changes in data streams present challenges for machine learning models to dynamically adapt and uphold performance in real-time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-neighbor-searching-discrepancy-based-drift</guid>
    </item>
    <item>
      <title>Metadata Integration for Spam Reviews Detection on Vietnamese E-commerce Websites</title>
      <link>https://paperswithcode.com/paper/metadata-integration-for-spam-reviews</link>
      <description><![CDATA[The problem of detecting spam reviews (opinions) has received significant attention in recent years, especially with the rapid development of e-commerce.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metadata-integration-for-spam-reviews</guid>
    </item>
    <item>
      <title>Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</title>
      <link>https://paperswithcode.com/paper/do-language-models-enjoy-their-own-stories</link>
      <description><![CDATA[Storytelling is an integral part of human experience and plays a crucial role in social interactions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-language-models-enjoy-their-own-stories</guid>
    </item>
    <item>
      <title>Mosaic IT: Enhancing Instruction Tuning with Data Mosaics</title>
      <link>https://paperswithcode.com/paper/mosaic-it-enhancing-instruction-tuning-with</link>
      <description><![CDATA[Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mosaic-it-enhancing-instruction-tuning-with</guid>
    </item>
    <item>
      <title>DirectMultiStep: Direct Route Generation for Multi-Step Retrosynthesis</title>
      <link>https://paperswithcode.com/paper/directmultistep-direct-route-generation-for</link>
      <description><![CDATA[Traditional computer-aided synthesis planning (CASP) methods rely on iterative single-step predictions, leading to exponential search space growth that limits efficiency and scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/directmultistep-direct-route-generation-for</guid>
    </item>
    <item>
      <title>Annotation-Efficient Preference Optimization for Language Model Alignment</title>
      <link>https://paperswithcode.com/paper/annotation-efficient-preference-optimization</link>
      <description><![CDATA[Instead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes quality and diversity from the available responses, and then annotates preference over the selected ones.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/annotation-efficient-preference-optimization</guid>
    </item>
    <item>
      <title>Why Not Transform Chat Large Language Models to Non-English?</title>
      <link>https://paperswithcode.com/paper/why-not-transform-chat-large-language-models</link>
      <description><![CDATA[For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/why-not-transform-chat-large-language-models</guid>
    </item>
    <item>
      <title>Grounding Toxicity in Real-World Events across Languages</title>
      <link>https://paperswithcode.com/paper/grounding-toxicity-in-real-world-events</link>
      <description><![CDATA[Social media conversations frequently suffer from toxicity, creating significant issues for users, moderators, and entire communities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-toxicity-in-real-world-events</guid>
    </item>
    <item>
      <title>FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research</title>
      <link>https://paperswithcode.com/paper/flashrag-a-modular-toolkit-for-efficient</link>
      <description><![CDATA[With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flashrag-a-modular-toolkit-for-efficient</guid>
    </item>
    <item>
      <title>Class-Conditional self-reward mechanism for improved Text-to-Image models</title>
      <link>https://paperswithcode.com/paper/class-conditional-self-reward-mechanism-for</link>
      <description><![CDATA[This approach works by fine-tuning diffusion model on a self-generated self-judged dataset, making the fine-tuning more automated and with better data quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/class-conditional-self-reward-mechanism-for</guid>
    </item>
    <item>
      <title>Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective</title>
      <link>https://paperswithcode.com/paper/bridging-operator-learning-and-conditioned</link>
      <description><![CDATA[Our contributions can be viewed as a first step towards adapting advanced computer vision architectures for building more flexible and accurate machine learning models in physical sciences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bridging-operator-learning-and-conditioned</guid>
    </item>
    <item>
      <title>Knowledge-Driven Cross-Document Relation Extraction</title>
      <link>https://paperswithcode.com/paper/knowledge-driven-cross-document-relation</link>
      <description><![CDATA[Relation extraction (RE) is a well-known NLP application often treated as a sentence- or document-level task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/knowledge-driven-cross-document-relation</guid>
    </item>
    <item>
      <title>Advancing Graph Convolutional Networks via General Spectral Wavelets</title>
      <link>https://paperswithcode.com/paper/advancing-graph-convolutional-networks-via</link>
      <description><![CDATA[Spectral graph convolution, an important tool of data filtering on graphs, relies on two essential decisions; selecting spectral bases for signal transformation and parameterizing the kernel for frequency analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advancing-graph-convolutional-networks-via</guid>
    </item>
    <item>
      <title>DeTox: Toxic Subspace Projection for Model Editing</title>
      <link>https://paperswithcode.com/paper/detox-toxic-subspace-projection-for-model</link>
      <description><![CDATA[Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/detox-toxic-subspace-projection-for-model</guid>
    </item>
    <item>
      <title>A Multilingual Similarity Dataset for News Article Frame</title>
      <link>https://paperswithcode.com/paper/a-multilingual-similarity-dataset-for-news</link>
      <description><![CDATA[Overall we introduce the most extensive cross-lingual news article similarity dataset available to date with 26, 555 labeled news article pairs across 10 languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-multilingual-similarity-dataset-for-news</guid>
    </item>
    <item>
      <title>Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization</title>
      <link>https://paperswithcode.com/paper/adversarial-training-of-two-layer-polynomial</link>
      <description><![CDATA[Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of polynomial activation networks via the S-procedure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adversarial-training-of-two-layer-polynomial</guid>
    </item>
    <item>
      <title>What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions</title>
      <link>https://paperswithcode.com/paper/what-is-your-data-worth-to-gpt-llm-scale-data</link>
      <description><![CDATA[Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-is-your-data-worth-to-gpt-llm-scale-data</guid>
    </item>
  </channel>
</rss>
