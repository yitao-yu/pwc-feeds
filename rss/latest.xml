<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 13 Oct 2022 21:11:18 +0000</lastBuildDate>
    <item>
      <title>Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics</title>
      <link>https://paperswithcode.com/paper/lbl2vec-an-embedding-based-approach-for</link>
      <description><![CDATA[When successively retrieving documents on different predefined topics from publicly available and commonly used datasets, we achieved an average area under the receiver operating characteristic curve value of 0. 95 on one dataset and 0. 92 on another.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lbl2vec-an-embedding-based-approach-for</guid>
    </item>
    <item>
      <title>Diffusion Models for Causal Discovery via Topological Ordering</title>
      <link>https://paperswithcode.com/paper/diffusion-models-for-causal-discovery-via</link>
      <description><![CDATA[Topological ordering approaches for causal discovery exploit this by performing graph discovery in two steps, first sequentially identifying nodes in reverse order of depth (topological ordering), and secondly pruning the potential relations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-models-for-causal-discovery-via</guid>
    </item>
    <item>
      <title>A Unified Framework for Alternating Offline Model Training and Policy Learning</title>
      <link>https://paperswithcode.com/paper/a-unified-framework-for-alternating-offline</link>
      <description><![CDATA[In offline model-based reinforcement learning (offline MBRL), we learn a dynamic model from historically collected data, and subsequently utilize the learned model and fixed datasets for policy learning, without further interacting with the environment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-unified-framework-for-alternating-offline</guid>
    </item>
    <item>
      <title>ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding</title>
      <link>https://paperswithcode.com/paper/ernie-layout-layout-knowledge-enhanced-pre</link>
      <description><![CDATA[Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ernie-layout-layout-knowledge-enhanced-pre</guid>
    </item>
    <item>
      <title>Task Compass: Scaling Multi-task Pre-training with Task Prefix</title>
      <link>https://paperswithcode.com/paper/task-compass-scaling-multi-task-pre-training</link>
      <description><![CDATA[Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/task-compass-scaling-multi-task-pre-training</guid>
    </item>
    <item>
      <title>Entity Aware Negative Sampling with Auxiliary Loss of False Negative Prediction for Knowledge Graph Embedding</title>
      <link>https://paperswithcode.com/paper/entity-aware-negative-sampling-with-auxiliary</link>
      <description><![CDATA[The proposed method can generate high-quality negative samples regardless of negative sample size and effectively mitigate the influence of false negative samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/entity-aware-negative-sampling-with-auxiliary</guid>
    </item>
    <item>
      <title>CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations</title>
      <link>https://paperswithcode.com/paper/ctl-evaluating-generalization-on-never-seen</link>
      <description><![CDATA[While the original CTL is used to test length generalization or productivity, CTL++ is designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ctl-evaluating-generalization-on-never-seen</guid>
    </item>
    <item>
      <title>Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary</title>
      <link>https://paperswithcode.com/paper/probing-commonsense-knowledge-in-pre-trained</link>
      <description><![CDATA[However, this approach is restricted by the LM's vocabulary available for masked predictions, and its precision is subject to the context provided by the assertion.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probing-commonsense-knowledge-in-pre-trained</guid>
    </item>
    <item>
      <title>Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks</title>
      <link>https://paperswithcode.com/paper/multi-level-firing-with-spiking-ds-resnet</link>
      <description><![CDATA[Spiking neural networks (SNNs) are bio-inspired neural networks with asynchronous discrete and sparse characteristics, which have increasingly manifested their superiority in low energy consumption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-level-firing-with-spiking-ds-resnet</guid>
    </item>
    <item>
      <title>Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/efficient-adversarial-training-without</link>
      <description><![CDATA[Recent studies reveal that a well-trained deep reinforcement learning (RL) policy can be particularly vulnerable to adversarial perturbations on input observations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-adversarial-training-without</guid>
    </item>
    <item>
      <title>Travel the Same Path: A Novel TSP Solving Strategy</title>
      <link>https://paperswithcode.com/paper/travel-the-same-path-a-novel-tsp-solving</link>
      <description><![CDATA[In this paper, we provide a novel strategy for solving Traveling Salesman Problem, which is a famous combinatorial optimization problem studied intensely in the TCS community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/travel-the-same-path-a-novel-tsp-solving</guid>
    </item>
    <item>
      <title>Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity</title>
      <link>https://paperswithcode.com/paper/double-bubble-toil-and-trouble-enhancing</link>
      <description><![CDATA[In response to subtle adversarial examples flipping classifications of neural network models, recent research has promoted certified robustness as a solution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/double-bubble-toil-and-trouble-enhancing</guid>
    </item>
    <item>
      <title>RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media</title>
      <link>https://paperswithcode.com/paper/redhot-a-corpus-of-annotated-medical</link>
      <description><![CDATA[Using this corpus, we introduce the task of retrieving trustworthy evidence relevant to a given claim made on social media.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/redhot-a-corpus-of-annotated-medical</guid>
    </item>
    <item>
      <title>On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning</title>
      <link>https://paperswithcode.com/paper/on-the-effectiveness-of-lipschitz-driven</link>
      <description><![CDATA[Rehearsal approaches enjoy immense popularity with Continual Learning (CL) practitioners.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-effectiveness-of-lipschitz-driven</guid>
    </item>
    <item>
      <title>RING++: Roto-translation Invariant Gram for Global Localization on a Sparse Scan Map</title>
      <link>https://paperswithcode.com/paper/ring-roto-translation-invariant-gram-for</link>
      <description><![CDATA[In addition, we derive sufficient conditions of feature extractors for the representation preserving the roto-translation invariance, making RING++ a framework applicable to generic multi-channel features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ring-roto-translation-invariant-gram-for</guid>
    </item>
    <item>
      <title>Transformer-based Text Classification on Unified Bangla Multi-class Emotion Corpus</title>
      <link>https://paperswithcode.com/paper/transformer-based-text-classification-on</link>
      <description><![CDATA[We propose a complete set of approaches for identifying and extracting emotions from Bangla texts in this research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transformer-based-text-classification-on</guid>
    </item>
    <item>
      <title>Modular Flows: Differential Molecular Generation</title>
      <link>https://paperswithcode.com/paper/modular-flows-differential-molecular</link>
      <description><![CDATA[Generating new molecules is fundamental to advancing critical applications such as drug discovery and material synthesis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/modular-flows-differential-molecular</guid>
    </item>
    <item>
      <title>Generating approximate state preparation circuits for NISQ computers with a genetic algorithm</title>
      <link>https://paperswithcode.com/paper/generating-approximate-state-preparation</link>
      <description><![CDATA[We study the approximate state preparation problem on noisy intermediate-scale quantum (NISQ) computers by applying a genetic algorithm to generate quantum circuits for state preparation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generating-approximate-state-preparation</guid>
    </item>
    <item>
      <title>Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting</title>
      <link>https://paperswithcode.com/paper/regularized-graph-structure-learning-with</link>
      <description><![CDATA[In this paper, we propose Regularized Graph Structure Learning (RGSL) model to incorporate both explicit prior structure and implicit structure together, and learn the forecasting deep networks along with the graph structure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/regularized-graph-structure-learning-with</guid>
    </item>
    <item>
      <title>Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers</title>
      <link>https://paperswithcode.com/paper/prompt-generation-networks-for-efficient</link>
      <description><![CDATA[Large-scale pretrained models, especially those trained from vision-language data have demonstrated the tremendous value that can be gained from both larger training datasets and models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompt-generation-networks-for-efficient</guid>
    </item>
    <item>
      <title>Efficient Image Super-Resolution using Vast-Receptive-Field Attention</title>
      <link>https://paperswithcode.com/paper/efficient-image-super-resolution-using-vast</link>
      <description><![CDATA[In this work, we design an efficient SR network by improving the attention mechanism.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-image-super-resolution-using-vast</guid>
    </item>
    <item>
      <title>How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization</title>
      <link>https://paperswithcode.com/paper/how-much-data-are-augmentations-worth-an</link>
      <description><![CDATA[Despite the clear performance benefits of data augmentations, little is known about why they are so effective.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-much-data-are-augmentations-worth-an</guid>
    </item>
    <item>
      <title>Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features</title>
      <link>https://paperswithcode.com/paper/hate-clipper-multimodal-hateful-meme</link>
      <description><![CDATA[A simple classifier based on the FIM representation is able to achieve state-of-the-art performance on the Hateful Memes Challenge (HMC) dataset with an AUROC of 85. 8, which even surpasses the human performance of 82. 65.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hate-clipper-multimodal-hateful-meme</guid>
    </item>
    <item>
      <title>Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation</title>
      <link>https://paperswithcode.com/paper/boosting-the-transferability-of-adversarial-2</link>
      <description><![CDATA[Furthermore, RAP can be naturally combined with many existing black-box attack techniques, to further boost the transferability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/boosting-the-transferability-of-adversarial-2</guid>
    </item>
    <item>
      <title>Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets</title>
      <link>https://paperswithcode.com/paper/bridging-the-gap-between-vision-transformers</link>
      <description><![CDATA[On channel aspect, we introduce a dynamic feature aggregation module in MLP and a brand new "head token" design in multi-head self-attention module to help re-calibrate channel representation and make different channel group representation interacts with each other.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bridging-the-gap-between-vision-transformers</guid>
    </item>
    <item>
      <title>Robust Models are less Over-Confident</title>
      <link>https://paperswithcode.com/paper/robust-models-are-less-over-confident</link>
      <description><![CDATA[Further, our analysis of robust models shows that not only AT but also the model's building blocks (like activation functions and pooling) have a strong influence on the models' prediction confidences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-models-are-less-over-confident</guid>
    </item>
    <item>
      <title>On Divergence Measures for Bayesian Pseudocoresets</title>
      <link>https://paperswithcode.com/paper/on-divergence-measures-for-bayesian</link>
      <description><![CDATA[Finally, we propose a novel Bayesian pseudocoreset algorithm based on minimizing forward KL divergence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-divergence-measures-for-bayesian</guid>
    </item>
    <item>
      <title>Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting Transformers</title>
      <link>https://paperswithcode.com/paper/uplift-and-upsample-efficient-3d-human-pose</link>
      <description><![CDATA[The state-of-the-art for monocular 3D human pose estimation in videos is dominated by the paradigm of 2D-to-3D pose uplifting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uplift-and-upsample-efficient-3d-human-pose</guid>
    </item>
    <item>
      <title>Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/centralized-training-with-hybrid-execution-in</link>
      <description><![CDATA[We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully perform cooperative tasks with any communication level at execution time by taking advantage of information-sharing among the agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/centralized-training-with-hybrid-execution-in</guid>
    </item>
    <item>
      <title>3D Brain and Heart Volume Generative Models: A Survey</title>
      <link>https://paperswithcode.com/paper/3d-brain-and-heart-volume-generative-models-a</link>
      <description><![CDATA[Generative models such as generative adversarial networks and autoencoders have gained a great deal of attention in the medical field due to their excellent data generation capability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-brain-and-heart-volume-generative-models-a</guid>
    </item>
    <item>
      <title>Latency-aware Spatial-wise Dynamic Networks</title>
      <link>https://paperswithcode.com/paper/latency-aware-spatial-wise-dynamic-networks</link>
      <description><![CDATA[The latency prediction model can efficiently estimate the inference latency of dynamic networks by simultaneously considering algorithms, scheduling strategies, and hardware properties.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/latency-aware-spatial-wise-dynamic-networks</guid>
    </item>
    <item>
      <title>Self-Attention Message Passing for Contrastive Few-Shot Learning</title>
      <link>https://paperswithcode.com/paper/self-attention-message-passing-for</link>
      <description><![CDATA[Humans have a unique ability to learn new representations from just a handful of examples with little to no supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-attention-message-passing-for</guid>
    </item>
    <item>
      <title>AdaNorm: Adaptive Gradient Norm Correction based Optimizer for CNNs</title>
      <link>https://paperswithcode.com/paper/adanorm-adaptive-gradient-norm-correction</link>
      <description><![CDATA[In this paper, we propose a novel AdaNorm based SGD optimizers by correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adanorm-adaptive-gradient-norm-correction</guid>
    </item>
    <item>
      <title>Efficient Offline Policy Optimization with a Learned Model</title>
      <link>https://paperswithcode.com/paper/efficient-offline-policy-optimization-with-a</link>
      <description><![CDATA[Instead of planning with the expensive MCTS, we use the learned model to construct an advantage estimation based on a one-step rollout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-offline-policy-optimization-with-a</guid>
    </item>
    <item>
      <title>MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers</title>
      <link>https://paperswithcode.com/paper/minialbert-model-distillation-via-parameter</link>
      <description><![CDATA[Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/minialbert-model-distillation-via-parameter</guid>
    </item>
    <item>
      <title>What Makes Graph Neural Networks Miscalibrated?</title>
      <link>https://paperswithcode.com/paper/what-makes-graph-neural-networks</link>
      <description><![CDATA[Furthermore, based on the insights from this study, we design a novel calibration method named Graph Attention Temperature Scaling (GATS), which is tailored for calibrating graph neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-makes-graph-neural-networks</guid>
    </item>
    <item>
      <title>DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action Recognition</title>
      <link>https://paperswithcode.com/paper/dg-stgcn-dynamic-spatial-temporal-modeling</link>
      <description><![CDATA[Graph convolution networks (GCN) have been widely used in skeleton-based action recognition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dg-stgcn-dynamic-spatial-temporal-modeling</guid>
    </item>
    <item>
      <title>Energy Consumption-Aware Tabular Benchmarks for Neural Architecture Search</title>
      <link>https://paperswithcode.com/paper/energy-consumption-aware-tabular-benchmarks</link>
      <description><![CDATA[To support the hypothesis, an existing tabular benchmark for NAS is augmented with the energy consumption of each architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/energy-consumption-aware-tabular-benchmarks</guid>
    </item>
    <item>
      <title>FasterRisk: Fast and Accurate Interpretable Risk Scores</title>
      <link>https://paperswithcode.com/paper/fasterrisk-fast-and-accurate-interpretable</link>
      <description><![CDATA[Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fasterrisk-fast-and-accurate-interpretable</guid>
    </item>
    <item>
      <title>Fast Bayesian Updates for Deep Learning with a Use Case in Active Learning</title>
      <link>https://paperswithcode.com/paper/fast-bayesian-updates-for-deep-learning-with</link>
      <description><![CDATA[Retraining deep neural networks when new data arrives is typically computationally expensive.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-bayesian-updates-for-deep-learning-with</guid>
    </item>
    <item>
      <title>Predictive Querying for Autoregressive Neural Sequence Models</title>
      <link>https://paperswithcode.com/paper/predictive-querying-for-autoregressive-neural</link>
      <description><![CDATA[In reasoning about sequential events it is natural to pose probabilistic queries such as "when will event A occur next" or "what is the probability of A occurring before B", with applications in areas such as user modeling, medicine, and finance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/predictive-querying-for-autoregressive-neural</guid>
    </item>
    <item>
      <title>Unsupervised Learning of Equivariant Structure from Sequences</title>
      <link>https://paperswithcode.com/paper/unsupervised-learning-of-equivariant</link>
      <description><![CDATA[In this study, we present meta-sequential prediction (MSP), an unsupervised framework to learn the symmetry from the time sequence of length at least three.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-learning-of-equivariant</guid>
    </item>
    <item>
      <title>Concentration of the exponential mechanism and differentially private multivariate medians</title>
      <link>https://paperswithcode.com/paper/concentration-of-the-exponential-mechanism</link>
      <description><![CDATA[We prove concentration inequalities for the output of the exponential mechanism about the maximizer of the population objective function.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/concentration-of-the-exponential-mechanism</guid>
    </item>
    <item>
      <title>AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning</title>
      <link>https://paperswithcode.com/paper/ad-drop-attribution-driven-dropout-for-robust</link>
      <description><![CDATA[Fine-tuning large pre-trained language models on downstream tasks is apt to suffer from overfitting when limited training data is available.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ad-drop-attribution-driven-dropout-for-robust</guid>
    </item>
    <item>
      <title>Equal Experience in Recommender Systems</title>
      <link>https://paperswithcode.com/paper/equal-experience-in-recommender-systems-1</link>
      <description><![CDATA[We explore the fairness issue that arises in recommender systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/equal-experience-in-recommender-systems-1</guid>
    </item>
    <item>
      <title>SpecRNet: Towards Faster and More Accessible Audio DeepFake Detection</title>
      <link>https://paperswithcode.com/paper/specrnet-towards-faster-and-more-accessible</link>
      <description><![CDATA[In this work, we focus on increasing accessibility to the audio DeepFake detection methods by providing SpecRNet, a neural network architecture characterized by a quick inference time and low computational requirements.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/specrnet-towards-faster-and-more-accessible</guid>
    </item>
    <item>
      <title>A Lower Bound of Hash Codes' Performance</title>
      <link>https://paperswithcode.com/paper/a-lower-bound-of-hash-codes-performance</link>
      <description><![CDATA[By testing on a series of hash-models, we obtain performance improvements among all of them, with an up to $26. 5\%$ increase in mean Average Precision and an up to $20. 5\%$ increase in accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-lower-bound-of-hash-codes-performance</guid>
    </item>
    <item>
      <title>Token-Label Alignment for Vision Transformers</title>
      <link>https://paperswithcode.com/paper/token-label-alignment-for-vision-transformers</link>
      <description><![CDATA[Data mixing strategies (e. g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/token-label-alignment-for-vision-transformers</guid>
    </item>
    <item>
      <title>Better Smatch = Better Parser? AMR evaluation is not so simple anymore</title>
      <link>https://paperswithcode.com/paper/better-smatch-better-parser-amr-evaluation-is</link>
      <description><![CDATA[Recently, astonishing advances have been observed in AMR parsing, as measured by the structural Smatch metric.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/better-smatch-better-parser-amr-evaluation-is</guid>
    </item>
    <item>
      <title>Towards Theoretically Inspired Neural Initialization Optimization</title>
      <link>https://paperswithcode.com/paper/towards-theoretically-inspired-neural</link>
      <description><![CDATA[With NIO, we improve the classification performance of a variety of neural architectures on CIFAR-10, CIFAR-100, and ImageNet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-theoretically-inspired-neural</guid>
    </item>
  </channel>
</rss>
