<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 22 Feb 2025 09:15:10 +0000</lastBuildDate>
    <item>
      <title>Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition</title>
      <link>https://paperswithcode.com/paper/bridging-text-and-vision-a-multi-view-text</link>
      <description><![CDATA[To overcome this challenge, we bridge text and vision by proposing a multiview (360{\deg} views of the surroundings) text-vision registration approach called Text4VPR for place recognition task, which is the first method that exclusively utilizes textual descriptions to match a database of images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bridging-text-and-vision-a-multi-view-text</guid>
    </item>
    <item>
      <title>STeCa: Step-level Trajectory Calibration for LLM Agent Learning</title>
      <link>https://paperswithcode.com/paper/steca-step-level-trajectory-calibration-for</link>
      <description><![CDATA[To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/steca-step-level-trajectory-calibration-for</guid>
    </item>
    <item>
      <title>Distribution Matching for Self-Supervised Transfer Learning</title>
      <link>https://paperswithcode.com/paper/distribution-matching-for-self-supervised</link>
      <description><![CDATA[In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/distribution-matching-for-self-supervised</guid>
    </item>
    <item>
      <title>ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</title>
      <link>https://paperswithcode.com/paper/reqflow-rectified-quaternion-flow-for</link>
      <description><![CDATA[Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reqflow-rectified-quaternion-flow-for</guid>
    </item>
    <item>
      <title>Pre-training Graph Neural Networks on Molecules by Using Subgraph-Conditioned Graph Information Bottleneck</title>
      <link>https://paperswithcode.com/paper/pre-training-graph-neural-networks-on</link>
      <description><![CDATA[The key challenge to build a pre-trained GNN on molecules is how to (1) generate well-distinguished graph-level representations and (2) automatically discover the functional groups without prior knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pre-training-graph-neural-networks-on</guid>
    </item>
    <item>
      <title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title>
      <link>https://paperswithcode.com/paper/how-much-knowledge-can-you-pack-into-a-lora-1</link>
      <description><![CDATA[The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-much-knowledge-can-you-pack-into-a-lora-1</guid>
    </item>
    <item>
      <title>seqKAN: Sequence processing with Kolmogorov-Arnold Networks</title>
      <link>https://paperswithcode.com/paper/seqkan-sequence-processing-with-kolmogorov</link>
      <description><![CDATA[Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seqkan-sequence-processing-with-kolmogorov</guid>
    </item>
    <item>
      <title>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields</title>
      <link>https://paperswithcode.com/paper/medfuncta-modality-agnostic-representations</link>
      <description><![CDATA[Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medfuncta-modality-agnostic-representations</guid>
    </item>
    <item>
      <title>Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity</title>
      <link>https://paperswithcode.com/paper/exploring-rwkv-for-sentence-embeddings-layer</link>
      <description><![CDATA[This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-rwkv-for-sentence-embeddings-layer</guid>
    </item>
    <item>
      <title>Pretrained Image-Text Models are Secretly Video Captioners</title>
      <link>https://paperswithcode.com/paper/pretrained-image-text-models-are-secretly</link>
      <description><![CDATA[Developing video captioning models is computationally expensive.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pretrained-image-text-models-are-secretly</guid>
    </item>
    <item>
      <title>Craw4LLM: Efficient Web Crawling for LLM Pretraining</title>
      <link>https://paperswithcode.com/paper/craw4llm-efficient-web-crawling-for-llm</link>
      <description><![CDATA[Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/craw4llm-efficient-web-crawling-for-llm</guid>
    </item>
    <item>
      <title>LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization</title>
      <link>https://paperswithcode.com/paper/longpo-long-context-self-evolution-of-large</link>
      <description><![CDATA[Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/longpo-long-context-self-evolution-of-large</guid>
    </item>
    <item>
      <title>From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions</title>
      <link>https://paperswithcode.com/paper/from-tools-to-teammates-evaluating-llms-in</link>
      <description><![CDATA[Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-tools-to-teammates-evaluating-llms-in</guid>
    </item>
    <item>
      <title>Fine-grained Fallacy Detection with Human Label Variation</title>
      <link>https://paperswithcode.com/paper/fine-grained-fallacy-detection-with-human</link>
      <description><![CDATA[We introduce Faina, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fine-grained-fallacy-detection-with-human</guid>
    </item>
    <item>
      <title>TESS 2: A Large-Scale Generalist Diffusion Language Model</title>
      <link>https://paperswithcode.com/paper/tess-2-a-large-scale-generalist-diffusion</link>
      <description><![CDATA[We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tess-2-a-large-scale-generalist-diffusion</guid>
    </item>
    <item>
      <title>Cascading CMA-ES Instances for Generating Input-diverse Solution Batches</title>
      <link>https://paperswithcode.com/paper/cascading-cma-es-instances-for-generating</link>
      <description><![CDATA[At the same time, it is crucial that the quality of the best solution found is not compromised.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cascading-cma-es-instances-for-generating</guid>
    </item>
    <item>
      <title>Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation</title>
      <link>https://paperswithcode.com/paper/exploring-mutual-cross-modal-attention-for</link>
      <description><![CDATA[In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-mutual-cross-modal-attention-for</guid>
    </item>
    <item>
      <title>PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models</title>
      <link>https://paperswithcode.com/paper/priv-qa-privacy-preserving-question-answering</link>
      <description><![CDATA[In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/priv-qa-privacy-preserving-question-answering</guid>
    </item>
    <item>
      <title>JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</title>
      <link>https://paperswithcode.com/paper/jl1-cd-a-new-benchmark-for-remote-sensing</link>
      <description><![CDATA[Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/jl1-cd-a-new-benchmark-for-remote-sensing</guid>
    </item>
    <item>
      <title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
      <link>https://paperswithcode.com/paper/mom-linear-sequence-modeling-with-mixture-of</link>
      <description><![CDATA[Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mom-linear-sequence-modeling-with-mixture-of</guid>
    </item>
    <item>
      <title>LESA: Learnable LLM Layer Scaling-Up</title>
      <link>https://paperswithcode.com/paper/lesa-learnable-llm-layer-scaling-up</link>
      <description><![CDATA[LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lesa-learnable-llm-layer-scaling-up</guid>
    </item>
    <item>
      <title>DataSciBench: An LLM Agent Benchmark for Data Science</title>
      <link>https://paperswithcode.com/paper/datascibench-an-llm-agent-benchmark-for-data</link>
      <description><![CDATA[In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/datascibench-an-llm-agent-benchmark-for-data</guid>
    </item>
    <item>
      <title>SPEX: Scaling Feature Interaction Explanations for LLMs</title>
      <link>https://paperswithcode.com/paper/spex-scaling-feature-interaction-explanations</link>
      <description><![CDATA[We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spex-scaling-feature-interaction-explanations</guid>
    </item>
    <item>
      <title>PeerQA: A Scientific Question Answering Dataset from Peer Reviews</title>
      <link>https://paperswithcode.com/paper/peerqa-a-scientific-question-answering</link>
      <description><![CDATA[We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/peerqa-a-scientific-question-answering</guid>
    </item>
    <item>
      <title>Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition</title>
      <link>https://paperswithcode.com/paper/latent-distribution-decoupling-a</link>
      <description><![CDATA[Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/latent-distribution-decoupling-a</guid>
    </item>
    <item>
      <title>MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping</title>
      <link>https://paperswithcode.com/paper/maizeear-sam-zero-shot-maize-ear-phenotyping</link>
      <description><![CDATA[Our approach successfully identifies the number of kernels per row across a wide range of maize ears, showing the potential of zero-shot learning with foundation vision models combined with image processing techniques to improve automation and reduce subjectivity in agronomic data collection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/maizeear-sam-zero-shot-maize-ear-phenotyping</guid>
    </item>
    <item>
      <title>Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization</title>
      <link>https://paperswithcode.com/paper/poster-spidersim-multi-agent-driven</link>
      <description><![CDATA[Rapid industrial digitalization has created intricate cybersecurity demands that necessitate effective validation methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/poster-spidersim-multi-agent-driven</guid>
    </item>
    <item>
      <title>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis</title>
      <link>https://paperswithcode.com/paper/mobilevim-a-light-weight-and-dimension</link>
      <description><![CDATA[Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mobilevim-a-light-weight-and-dimension</guid>
    </item>
    <item>
      <title>ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails</title>
      <link>https://paperswithcode.com/paper/thinkguard-deliberative-slow-thinking-leads</link>
      <description><![CDATA[Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/thinkguard-deliberative-slow-thinking-leads</guid>
    </item>
    <item>
      <title>Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention</title>
      <link>https://paperswithcode.com/paper/medical-image-classification-with-kan</link>
      <description><![CDATA[Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/medical-image-classification-with-kan</guid>
    </item>
    <item>
      <title>Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields</title>
      <link>https://paperswithcode.com/paper/event-based-video-frame-interpolation-with-2</link>
      <description><![CDATA[However, existing methods estimate bidirectional inter-frame motion fields with only events or approximations, which can not consider the complex motion in real-world scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/event-based-video-frame-interpolation-with-2</guid>
    </item>
    <item>
      <title>Noise May Contain Transferable Knowledge: Understanding Semi-supervised Heterogeneous Domain Adaptation from an Empirical Perspective</title>
      <link>https://paperswithcode.com/paper/noise-may-contain-transferable-knowledge</link>
      <description><![CDATA[Based on this insight, we perform a series of experiments to uncover the underlying principles of transferable knowledge in SHDA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/noise-may-contain-transferable-knowledge</guid>
    </item>
    <item>
      <title>Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics</title>
      <link>https://paperswithcode.com/paper/helix-mrna-a-hybrid-foundation-model-for-full</link>
      <description><![CDATA[We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/helix-mrna-a-hybrid-foundation-model-for-full</guid>
    </item>
    <item>
      <title>Unsupervised Graph Embeddings for Session-based Recommendation with Item Features</title>
      <link>https://paperswithcode.com/paper/unsupervised-graph-embeddings-for-session</link>
      <description><![CDATA[State-of-the-art sequential recommendation algorithms either use graph neural networks to model sessions in a graph or leverage the similarity of sessions by exploiting item features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-graph-embeddings-for-session</guid>
    </item>
    <item>
      <title>Deep Learning for VWAP Execution in Crypto Markets: Beyond the Volume Curve</title>
      <link>https://paperswithcode.com/paper/deep-learning-for-vwap-execution-in-crypto</link>
      <description><![CDATA[This research contributes a more efficient and robust framework for VWAP execution in volatile markets, illustrating the potential of deep learning in complex financial systems where direct objective optimization is crucial.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-learning-for-vwap-execution-in-crypto</guid>
    </item>
    <item>
      <title>Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI</title>
      <link>https://paperswithcode.com/paper/fighter-jet-navigation-and-combat-using-deep</link>
      <description><![CDATA[This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fighter-jet-navigation-and-combat-using-deep</guid>
    </item>
    <item>
      <title>Reasoning with Reinforced Functional Token Tuning</title>
      <link>https://paperswithcode.com/paper/reasoning-with-reinforced-functional-token</link>
      <description><![CDATA[Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reasoning-with-reinforced-functional-token</guid>
    </item>
    <item>
      <title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title>
      <link>https://paperswithcode.com/paper/adaptivestep-automatically-dividing-reasoning</link>
      <description><![CDATA[Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptivestep-automatically-dividing-reasoning</guid>
    </item>
    <item>
      <title>SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation</title>
      <link>https://paperswithcode.com/paper/songgen-a-single-stage-auto-regressive</link>
      <description><![CDATA[To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/songgen-a-single-stage-auto-regressive</guid>
    </item>
    <item>
      <title>UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models</title>
      <link>https://paperswithcode.com/paper/uniguardian-a-unified-defense-for-detecting</link>
      <description><![CDATA[Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uniguardian-a-unified-defense-for-detecting</guid>
    </item>
    <item>
      <title>Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction</title>
      <link>https://paperswithcode.com/paper/label-drop-for-multi-aspect-relation-modeling</link>
      <description><![CDATA[Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/label-drop-for-multi-aspect-relation-modeling</guid>
    </item>
    <item>
      <title>Soundwave: Less is More for Speech-Text Alignment in LLMs</title>
      <link>https://paperswithcode.com/paper/soundwave-less-is-more-for-speech-text</link>
      <description><![CDATA[Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soundwave-less-is-more-for-speech-text</guid>
    </item>
    <item>
      <title>MotifBench: A standardized protein design benchmark for motif-scaffolding problems</title>
      <link>https://paperswithcode.com/paper/motifbench-a-standardized-protein-design</link>
      <description><![CDATA[The motif-scaffolding problem is a central task in computational protein design: Given the coordinates of atoms in a geometry chosen to confer a desired biochemical function (a motif), the task is to identify diverse protein structures (scaffolds) that include the motif and maintain its geometry.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/motifbench-a-standardized-protein-design</guid>
    </item>
    <item>
      <title>Keep what you need : extracting efficient subnetworks from large audio representation models</title>
      <link>https://paperswithcode.com/paper/keep-what-you-need-extracting-efficient</link>
      <description><![CDATA[In this paper, we address this issue with a simple, yet effective method to extract lightweight specialist subnetworks from large foundation models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/keep-what-you-need-extracting-efficient</guid>
    </item>
    <item>
      <title>Magma: A Foundation Model for Multimodal AI Agents</title>
      <link>https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</link>
      <description><![CDATA[We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magma-a-foundation-model-for-multimodal-ai</guid>
    </item>
    <item>
      <title>Probabilistic neural operators for functional uncertainty quantification</title>
      <link>https://paperswithcode.com/paper/probabilistic-neural-operators-for-functional</link>
      <description><![CDATA[Neural operators aim to approximate the solution operator of a system of differential equations purely from data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probabilistic-neural-operators-for-functional</guid>
    </item>
    <item>
      <title>NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation</title>
      <link>https://paperswithcode.com/paper/next-mol-3d-diffusion-meets-1d-language</link>
      <description><![CDATA[To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/next-mol-3d-diffusion-meets-1d-language</guid>
    </item>
    <item>
      <title>Dimension reduction methods, persistent homology and machine learning for EEG signal analysis of Interictal Epileptic Discharges</title>
      <link>https://paperswithcode.com/paper/dimension-reduction-methods-persistent</link>
      <description><![CDATA[The reduced data are examined using topological data analysis (TDA), specifically using a persistent homology algorithm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dimension-reduction-methods-persistent</guid>
    </item>
    <item>
      <title>SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings</title>
      <link>https://paperswithcode.com/paper/sea-low-resource-safety-alignment-for</link>
      <description><![CDATA[Multimodal Large Language Models (MLLMs) have serious security vulnerabilities. While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sea-low-resource-safety-alignment-for</guid>
    </item>
    <item>
      <title>DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent</title>
      <link>https://paperswithcode.com/paper/demonagent-dynamically-encrypted-multi</link>
      <description><![CDATA[As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demonagent-dynamically-encrypted-multi</guid>
    </item>
  </channel>
</rss>
