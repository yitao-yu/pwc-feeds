<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 16 Oct 2024 09:16:55 +0000</lastBuildDate>
    <item>
      <title>Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement</title>
      <link>https://paperswithcode.com/paper/meta-dt-offline-meta-rl-as-conditional</link>
      <description><![CDATA[We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-dt-offline-meta-rl-as-conditional</guid>
    </item>
    <item>
      <title>MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models</title>
      <link>https://paperswithcode.com/paper/mtu-bench-a-multi-granularity-tool-use</link>
      <description><![CDATA[Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mtu-bench-a-multi-granularity-tool-use</guid>
    </item>
    <item>
      <title>TraM : Enhancing User Sleep Prediction with Transformer-based Multivariate Time Series Modeling and Machine Learning Ensembles</title>
      <link>https://paperswithcode.com/paper/tram-enhancing-user-sleep-prediction-with</link>
      <description><![CDATA[Time Series Transformer was used for labels where time series characteristics are crucial, while Machine Learning Ensembles were employed for labels requiring comprehensive daily activity statistics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tram-enhancing-user-sleep-prediction-with</guid>
    </item>
    <item>
      <title>Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository</title>
      <link>https://paperswithcode.com/paper/enhancing-assamese-nlp-capabilities</link>
      <description><![CDATA[This paper introduces a centralized, open-source dataset repository designed to advance NLP and NMT for Assamese, a low-resource language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/enhancing-assamese-nlp-capabilities</guid>
    </item>
    <item>
      <title>Improving Long-Text Alignment for Text-to-Image Diffusion Models</title>
      <link>https://paperswithcode.com/paper/improving-long-text-alignment-for-text-to</link>
      <description><![CDATA[To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/improving-long-text-alignment-for-text-to</guid>
    </item>
    <item>
      <title>On the potential of Optimal Transport in Geospatial Data Science</title>
      <link>https://paperswithcode.com/paper/on-the-potential-of-optimal-transport-in</link>
      <description><![CDATA[We put forward Optimal Transport (OT) as a spatial evaluation metric and loss function.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-potential-of-optimal-transport-in</guid>
    </item>
    <item>
      <title>Overcoming Domain Limitations in Open-vocabulary Segmentation</title>
      <link>https://paperswithcode.com/paper/overcoming-domain-limitations-in-open</link>
      <description><![CDATA[Extensive experiments demonstrate that this approach allows OVS models to adapt to new domains while maintaining performance on the previous training dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/overcoming-domain-limitations-in-open</guid>
    </item>
    <item>
      <title>RATE: Score Reward Models with Imperfect Rewrites of Rewrites</title>
      <link>https://paperswithcode.com/paper/rate-score-reward-models-with-imperfect</link>
      <description><![CDATA[In this paper, we develop an evaluation method, RATE (Rewrite-based Attribute Treatment Estimators), that allows us to measure the causal effect of a given attribute of a response (e. g., length) on the reward assigned to that response.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rate-score-reward-models-with-imperfect</guid>
    </item>
    <item>
      <title>Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/automatically-generating-visual-hallucination</link>
      <description><![CDATA[Our theoretical analysis shows that symmetric accuracy is an unbiased evaluation metric that remains unaffected by the imbalance of VH testing cases with varying answers when an MLLM is randomly guessing the answers, whereas traditional accuracy is prone to such imbalance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/automatically-generating-visual-hallucination</guid>
    </item>
    <item>
      <title>MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark</title>
      <link>https://paperswithcode.com/paper/mctbench-multimodal-cognition-towards-text</link>
      <description><![CDATA[The comprehension of text-rich visual scenes has become a focal point for evaluating Multi-modal Large Language Models (MLLMs) due to their widespread applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mctbench-multimodal-cognition-towards-text</guid>
    </item>
    <item>
      <title>Zero-shot Model-based Reinforcement Learning using Large Language Models</title>
      <link>https://paperswithcode.com/paper/zero-shot-model-based-reinforcement-learning</link>
      <description><![CDATA[The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zero-shot-model-based-reinforcement-learning</guid>
    </item>
    <item>
      <title>MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/manet-fine-tuning-segment-anything-model-for</link>
      <description><![CDATA[Building upon recent advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study introduces a novel Multimodal Adapter-based Network (MANet) for multimodal remote sensing semantic segmentation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/manet-fine-tuning-segment-anything-model-for</guid>
    </item>
    <item>
      <title>Visual-Geometric Collaborative Guidance for Affordance Learning</title>
      <link>https://paperswithcode.com/paper/visual-geometric-collaborative-guidance-for</link>
      <description><![CDATA[Perceiving potential ``action possibilities'' (\ie, affordance) regions of images and learning interactive functionalities of objects from human demonstration is a challenging task due to the diversity of human-object interactions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-geometric-collaborative-guidance-for</guid>
    </item>
    <item>
      <title>Dual-Teacher Ensemble Models with Double-Copy-Paste for 3D Semi-Supervised Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/dual-teacher-ensemble-models-with-double-copy</link>
      <description><![CDATA[Dual-teacher models were introduced to address this problem but often neglected the importance of maintaining teacher model diversity, leading to coupling issues among teachers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dual-teacher-ensemble-models-with-double-copy</guid>
    </item>
    <item>
      <title>Breaking Modality Gap in RGBT Tracking: Coupled Knowledge Distillation</title>
      <link>https://paperswithcode.com/paper/breaking-modality-gap-in-rgbt-tracking</link>
      <description><![CDATA[To handle this issue, we take original RGB and TIR networks as the teachers, and distill their content knowledge into two student networks respectively by the style-content orthogonal feature decoupling scheme.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/breaking-modality-gap-in-rgbt-tracking</guid>
    </item>
    <item>
      <title>LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under Low-Resource Scenarios</title>
      <link>https://paperswithcode.com/paper/lr-sql-a-supervised-fine-tuning-method-for</link>
      <description><![CDATA[Large language models revolutionize Text2SQL through supervised fine-tuning, yet a crucial limitation is overlooked: the complexity of databases leads to an increased context length, consequently resulting in higher GPU memory demands for model fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lr-sql-a-supervised-fine-tuning-method-for</guid>
    </item>
    <item>
      <title>Adversarially Guided Stateful Defense Against Backdoor Attacks in Federated Deep Learning</title>
      <link>https://paperswithcode.com/paper/adversarially-guided-stateful-defense-against</link>
      <description><![CDATA[We show that in realistic FL settings, state-of-the-art (SOTA) defenses struggle to perform well against backdoor attacks in FL.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adversarially-guided-stateful-defense-against</guid>
    </item>
    <item>
      <title>A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies</title>
      <link>https://paperswithcode.com/paper/a-cross-lingual-statutory-article-retrieval</link>
      <description><![CDATA[This paper introduces a cross-lingual statutory article retrieval (SAR) dataset designed to enhance legal information retrieval in multilingual settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-cross-lingual-statutory-article-retrieval</guid>
    </item>
    <item>
      <title>MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation</title>
      <link>https://paperswithcode.com/paper/mllm-can-see-dynamic-correction-decoding-for</link>
      <description><![CDATA[Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mllm-can-see-dynamic-correction-decoding-for</guid>
    </item>
    <item>
      <title>Safety Filtering While Training: Improving the Performance and Sample Efficiency of Reinforcement Learning Agents</title>
      <link>https://paperswithcode.com/paper/safety-filtering-while-training-improving-the</link>
      <description><![CDATA[In experiments, we show that the proposed training approaches require significantly fewer environment interactions and improve performance by up to 20% compared to standard RL training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/safety-filtering-while-training-improving-the</guid>
    </item>
    <item>
      <title>Self-adaptive Multimodal Retrieval-Augmented Generation</title>
      <link>https://paperswithcode.com/paper/self-adaptive-multimodal-retrieval-augmented</link>
      <description><![CDATA[Traditional Retrieval-Augmented Generation (RAG) methods are limited by their reliance on a fixed number of retrieved documents, often resulting in incomplete or noisy information that undermines task performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-adaptive-multimodal-retrieval-augmented</guid>
    </item>
    <item>
      <title>Deep unrolled primal dual network for TOF-PET list-mode image reconstruction</title>
      <link>https://paperswithcode.com/paper/deep-unrolled-primal-dual-network-for-tof-pet</link>
      <description><![CDATA[In this study, we propose a deep unrolled primal dual network for TOF-PET list-mode reconstruction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deep-unrolled-primal-dual-network-for-tof-pet</guid>
    </item>
    <item>
      <title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title>
      <link>https://paperswithcode.com/paper/moh-multi-head-attention-as-mixture-of-head</link>
      <description><![CDATA[We show that multi-head attention can be expressed in the summation form.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/moh-multi-head-attention-as-mixture-of-head</guid>
    </item>
    <item>
      <title>Subspace Optimization for Large Language Models with Convergence Guarantees</title>
      <link>https://paperswithcode.com/paper/subspace-optimization-for-large-language</link>
      <description><![CDATA[Subspace optimization algorithms, with GaLore (Zhao et al., 2024) as a representative method, have gained popularity for pre-training or fine-tuning large language models (LLMs) due to their memory efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/subspace-optimization-for-large-language</guid>
    </item>
    <item>
      <title>DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection</title>
      <link>https://paperswithcode.com/paper/darnet-dual-attention-refinement-network-with</link>
      <description><![CDATA[To address these issues, this paper proposes a dual attention refinement network with spatiotemporal construction for AAD, named DARNet, which consists of the spatiotemporal construction module, dual attention refinement module, and feature fusion \& classifier module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/darnet-dual-attention-refinement-network-with</guid>
    </item>
    <item>
      <title>AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a simple RAG task</title>
      <link>https://paperswithcode.com/paper/aic-ctu-system-at-averitec-re-framing</link>
      <description><![CDATA[This paper describes our $3^{rd}$ place submission in the AVeriTeC shared task in which we attempted to address the challenge of fact-checking with evidence retrieved in the wild using a simple scheme of Retrieval-Augmented Generation (RAG) designed for the task, leveraging the predictive power of Large Language Models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aic-ctu-system-at-averitec-re-framing</guid>
    </item>
    <item>
      <title>CVCP-Fusion: On Implicit Depth Estimation for 3D Bounding Box Prediction</title>
      <link>https://paperswithcode.com/paper/cvcp-fusion-on-implicit-depth-estimation-for</link>
      <description><![CDATA[In this paper we propose Cross-View Center Point-Fusion, a state-of-the-art model to perform 3D object detection by combining camera and LiDAR-derived features in the BEV space to preserve semantic density from the camera stream while incorporating spacial data from the LiDAR stream.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cvcp-fusion-on-implicit-depth-estimation-for</guid>
    </item>
    <item>
      <title>Do LLMs Have the Generalization Ability in Conducting Causal Inference?</title>
      <link>https://paperswithcode.com/paper/do-llms-have-the-generalization-ability-in</link>
      <description><![CDATA[In this paper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment (BA), Factual Inference (FI), and Counterfactual Inference (CI) as representatives of causal inference tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-llms-have-the-generalization-ability-in</guid>
    </item>
    <item>
      <title>Contrastive learning of cell state dynamics in response to perturbations</title>
      <link>https://paperswithcode.com/paper/contrastive-learning-of-cell-state-dynamics</link>
      <description><![CDATA[We illustrate the features and applications of DynaCLR with the following experiments: analyzing the kinetics of viral infection in human cells, detecting transient changes in cell morphology due to cell division, and mapping the dynamics of organelles due to viral infection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/contrastive-learning-of-cell-state-dynamics</guid>
    </item>
    <item>
      <title>Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models</title>
      <link>https://paperswithcode.com/paper/layer-wise-importance-matters-less-memory-for</link>
      <description><![CDATA[Extensive experiments on a range of LLMs, PEFTs, and downstream tasks substantiate the effectiveness of our proposed method, showcasing IST's capacity to enhance existing layer-based PEFT methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/layer-wise-importance-matters-less-memory-for</guid>
    </item>
    <item>
      <title>Personas with Attitudes: Controlling LLMs for Diverse Data Annotation</title>
      <link>https://paperswithcode.com/paper/personas-with-attitudes-controlling-llms-for</link>
      <description><![CDATA[We present a novel approach for enhancing diversity and control in data annotation tasks by personalizing large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/personas-with-attitudes-controlling-llms-for</guid>
    </item>
    <item>
      <title>On-the-fly Modulation for Balanced Multimodal Learning</title>
      <link>https://paperswithcode.com/paper/on-the-fly-modulation-for-balanced-multimodal</link>
      <description><![CDATA[Then, On-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM) strategies are proposed to modulate the optimization of each modality, by monitoring the discriminative discrepancy between modalities during training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-fly-modulation-for-balanced-multimodal</guid>
    </item>
    <item>
      <title>Efficiera Residual Networks: Hardware-Friendly Fully Binary Weight with 2-bit Activation Model Achieves Practical ImageNet Accuracy</title>
      <link>https://paperswithcode.com/paper/efficiera-residual-networks-hardware-friendly</link>
      <description><![CDATA[The edge-device environment imposes severe resource limitations, encompassing computation costs, hardware resource usage, and energy consumption for deploying deep neural network models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficiera-residual-networks-hardware-friendly</guid>
    </item>
    <item>
      <title>FOOGD: Federated Collaboration for Both Out-of-distribution Generalization and Detection</title>
      <link>https://paperswithcode.com/paper/foogd-federated-collaboration-for-both-out-of</link>
      <description><![CDATA[Federated learning (FL) is a promising machine learning paradigm that collaborates with client models to capture global knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/foogd-federated-collaboration-for-both-out-of</guid>
    </item>
    <item>
      <title>Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation</title>
      <link>https://paperswithcode.com/paper/advancing-training-efficiency-of-deep-spiking</link>
      <description><![CDATA[Recent insights have revealed that rate-coding is a primary form of information representation captured by surrogate-gradient-based Backpropagation Through Time (BPTT) in training deep Spiking Neural Networks (SNNs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/advancing-training-efficiency-of-deep-spiking</guid>
    </item>
    <item>
      <title>Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero Shot Music Emotion Prediction</title>
      <link>https://paperswithcode.com/paper/leveraging-llm-embeddings-for-cross-dataset</link>
      <description><![CDATA[In this work, we present a novel method for music emotion recognition that leverages Large Language Model (LLM) embeddings for label alignment across multiple datasets and zero-shot prediction on novel categories.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leveraging-llm-embeddings-for-cross-dataset</guid>
    </item>
    <item>
      <title>MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding</title>
      <link>https://paperswithcode.com/paper/mmfuser-multimodal-multi-layer-feature-fuser</link>
      <description><![CDATA[To address this issue, we propose \modelname, a simple yet effective multi-layer feature fuser that efficiently integrates deep and shallow features from Vision Transformers (ViTs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmfuser-multimodal-multi-layer-feature-fuser</guid>
    </item>
    <item>
      <title>Reproducible Machine Learning-based Voice Pathology Detection: Introducing the Pitch Difference Feature</title>
      <link>https://paperswithcode.com/paper/reproducible-machine-learning-based-voice</link>
      <description><![CDATA[We combine this feature set, containing data from the publicly available Saarbr\"ucken Voice Database (SVD), with preprocessing using the K-Means Synthetic Minority Over-Sampling Technique algorithm to address class imbalance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reproducible-machine-learning-based-voice</guid>
    </item>
    <item>
      <title>Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations</title>
      <link>https://paperswithcode.com/paper/balanced-neural-odes-nonlinear-model-order</link>
      <description><![CDATA[Variational Autoencoders (VAEs) are a powerful framework for learning compact latent representations, while NeuralODEs excel in learning transient system dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/balanced-neural-odes-nonlinear-model-order</guid>
    </item>
    <item>
      <title>TRESTLE: A Model of Concept Formation in Structured Domains</title>
      <link>https://paperswithcode.com/paper/trestle-a-model-of-concept-formation-in</link>
      <description><![CDATA[The literature on concept formation has demonstrated that humans are capable of learning concepts incrementally, with a variety of attribute types, and in both supervised and unsupervised settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/trestle-a-model-of-concept-formation-in</guid>
    </item>
    <item>
      <title>Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance</title>
      <link>https://paperswithcode.com/paper/context-parametric-inversion-why-instruction</link>
      <description><![CDATA[However, even state-of-the-art models often struggle to follow the instruction, especially when the input context is not aligned with the model's parametric knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/context-parametric-inversion-why-instruction</guid>
    </item>
    <item>
      <title>Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification with View-Based Aggregation</title>
      <link>https://paperswithcode.com/paper/ensemble-of-convnext-v2-and-maxvit-for-long</link>
      <description><![CDATA[Through experiments, we demonstrate the advantages of our approach in improving both detection accuracy and the handling of the long-tailed distribution in CXR findings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ensemble-of-convnext-v2-and-maxvit-for-long</guid>
    </item>
    <item>
      <title>RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates</title>
      <link>https://paperswithcode.com/paper/rocoft-efficient-finetuning-of-large-language</link>
      <description><![CDATA[We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale language models (LMs) based on updating only a few rows and columns of the weight matrices in transformers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rocoft-efficient-finetuning-of-large-language</guid>
    </item>
    <item>
      <title>Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts</title>
      <link>https://paperswithcode.com/paper/efficiently-democratizing-medical-llms-for-50</link>
      <description><![CDATA[In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficiently-democratizing-medical-llms-for-50</guid>
    </item>
    <item>
      <title>StegaINR4MIH: steganography by implicit neural representation for multi-image hiding</title>
      <link>https://paperswithcode.com/paper/stegainr4mih-steganography-by-implicit-neural</link>
      <description><![CDATA[Multi-image hiding, which embeds multiple secret images into a cover image and is able to recover these images with high quality, has gradually become a research hotspot in the field of image steganography.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stegainr4mih-steganography-by-implicit-neural</guid>
    </item>
    <item>
      <title>AFlow: Automating Agentic Workflow Generation</title>
      <link>https://paperswithcode.com/paper/aflow-automating-agentic-workflow-generation</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aflow-automating-agentic-workflow-generation</guid>
    </item>
    <item>
      <title>Enhancing Attributed Graph Networks with Alignment and Uniformity Constraints for Session-based Recommendation</title>
      <link>https://paperswithcode.com/paper/enhancing-attributed-graph-networks-with</link>
      <description><![CDATA[In this paper, we propose a model-agnostic framework, named AttrGAU (Attributed Graph Networks with Alignment and Uniformity Constraints), to bring the MIA's superiority into existing attribute-agnostic models, to improve their accuracy and robustness for recommendation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/enhancing-attributed-graph-networks-with</guid>
    </item>
    <item>
      <title>Liger Kernel: Efficient Triton Kernels for LLM Training</title>
      <link>https://paperswithcode.com/paper/liger-kernel-efficient-triton-kernels-for-llm</link>
      <description><![CDATA[Training Large Language Models (LLMs) efficiently at scale presents a formidable challenge, driven by their ever-increasing computational demands and the need for enhanced performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/liger-kernel-efficient-triton-kernels-for-llm</guid>
    </item>
    <item>
      <title>LVD-2M: A Long-take Video Dataset with Temporally Dense Captions</title>
      <link>https://paperswithcode.com/paper/lvd-2m-a-long-take-video-dataset-with</link>
      <description><![CDATA[The efficacy of video generation models heavily depends on the quality of their training datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lvd-2m-a-long-take-video-dataset-with</guid>
    </item>
    <item>
      <title>Get Rid of Task Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework</title>
      <link>https://paperswithcode.com/paper/get-rid-of-task-isolation-a-continuous-multi</link>
      <description><![CDATA[To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower collective urban intelligence, which reforms the urban spatiotemporal learning from single-domain to cooperatively multi-dimensional and multi-task learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/get-rid-of-task-isolation-a-continuous-multi</guid>
    </item>
  </channel>
</rss>
