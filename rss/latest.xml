<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Wed, 18 Oct 2023 21:06:25 +0000</lastBuildDate>
    <item>
      <title>BitNet: Scaling 1-bit Transformers for Large Language Models</title>
      <link>https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</link>
      <description><![CDATA[The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bitnet-scaling-1-bit-transformers-for-large</guid>
    </item>
    <item>
      <title>Efficiently Visualizing Large Graphs</title>
      <link>https://paperswithcode.com/paper/efficiently-visualizing-large-graphs</link>
      <description><![CDATA[Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up to 300K nodes and 1M edges within 5 minutes and achieve approximately 10% improvement in visualization quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficiently-visualizing-large-graphs</guid>
    </item>
    <item>
      <title>Probing the Creativity of Large Language Models: Can models produce divergent semantic association?</title>
      <link>https://paperswithcode.com/paper/probing-the-creativity-of-large-language</link>
      <description><![CDATA[Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probing-the-creativity-of-large-language</guid>
    </item>
    <item>
      <title>Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads</title>
      <link>https://paperswithcode.com/paper/sparse-dysta-sparsity-aware-dynamic-and</link>
      <description><![CDATA[Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sparse-dysta-sparsity-aware-dynamic-and</guid>
    </item>
    <item>
      <title>Heterogenous Memory Augmented Neural Networks</title>
      <link>https://paperswithcode.com/paper/heterogenous-memory-augmented-neural-networks</link>
      <description><![CDATA[It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/heterogenous-memory-augmented-neural-networks</guid>
    </item>
    <item>
      <title>xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization</title>
      <link>https://paperswithcode.com/paper/xmen-a-modular-toolkit-for-cross-lingual</link>
      <description><![CDATA[Weakly supervised cross-encoders are effective when no training data is available for the target task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/xmen-a-modular-toolkit-for-cross-lingual</guid>
    </item>
    <item>
      <title>Instilling Inductive Biases with Subnetworks</title>
      <link>https://paperswithcode.com/paper/instilling-inductive-biases-with-subnetworks</link>
      <description><![CDATA[Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instilling-inductive-biases-with-subnetworks</guid>
    </item>
    <item>
      <title>An Empirical Study of Translation Hypothesis Ensembling with Large Language Models</title>
      <link>https://paperswithcode.com/paper/an-empirical-study-of-translation-hypothesis</link>
      <description><![CDATA[Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-empirical-study-of-translation-hypothesis</guid>
    </item>
    <item>
      <title>KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models</title>
      <link>https://paperswithcode.com/paper/kg-gpt-a-general-framework-for-reasoning-on</link>
      <description><![CDATA[While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kg-gpt-a-general-framework-for-reasoning-on</guid>
    </item>
    <item>
      <title>Fast and Simple Spectral Clustering in Theory and Practice</title>
      <link>https://paperswithcode.com/paper/fast-and-simple-spectral-clustering-in-theory</link>
      <description><![CDATA[Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-and-simple-spectral-clustering-in-theory</guid>
    </item>
    <item>
      <title>Adaptive Pairwise Encodings for Link Prediction</title>
      <link>https://paperswithcode.com/paper/adaptive-pairwise-encodings-for-link</link>
      <description><![CDATA[In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptive-pairwise-encodings-for-link</guid>
    </item>
    <item>
      <title>TEQ: Trainable Equivalent Transformation for Quantization of LLMs</title>
      <link>https://paperswithcode.com/paper/teq-trainable-equivalent-transformation-for</link>
      <description><![CDATA[As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/teq-trainable-equivalent-transformation-for</guid>
    </item>
    <item>
      <title>VcT: Visual change Transformer for Remote Sensing Image Change Detection</title>
      <link>https://paperswithcode.com/paper/vct-visual-change-transformer-for-remote</link>
      <description><![CDATA[Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vct-visual-change-transformer-for-remote</guid>
    </item>
    <item>
      <title>Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning</title>
      <link>https://paperswithcode.com/paper/dual-cognitive-architecture-incorporating</link>
      <description><![CDATA[Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dual-cognitive-architecture-incorporating</guid>
    </item>
    <item>
      <title>An active learning convolutional neural network for predicting river flow in a human impacted system</title>
      <link>https://paperswithcode.com/paper/an-active-learning-convolutional-neural</link>
      <description><![CDATA[Here we find that a convolutional Long Short-Term Memory (LSTM) network is well suited to modeling flow in parts of this basin that are strongly impacted by water projects as well as ones that are relatively free from direct human modifications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-active-learning-convolutional-neural</guid>
    </item>
    <item>
      <title>Agent-Specific Effects</title>
      <link>https://paperswithcode.com/paper/agent-specific-effects</link>
      <description><![CDATA[These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/agent-specific-effects</guid>
    </item>
    <item>
      <title>Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs</title>
      <link>https://paperswithcode.com/paper/query2triple-unified-query-encoding-for</link>
      <description><![CDATA[However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/query2triple-unified-query-encoding-for</guid>
    </item>
    <item>
      <title>Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks</title>
      <link>https://paperswithcode.com/paper/neural-attention-enhancing-qkv-calculation-in</link>
      <description><![CDATA[In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-attention-enhancing-qkv-calculation-in</guid>
    </item>
    <item>
      <title>MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient</title>
      <link>https://paperswithcode.com/paper/monoskd-general-distillation-framework-for</link>
      <description><![CDATA[Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monoskd-general-distillation-framework-for</guid>
    </item>
    <item>
      <title>Understanding Contrastive Learning via Distributionally Robust Optimization</title>
      <link>https://paperswithcode.com/paper/understanding-contrastive-learning-via</link>
      <description><![CDATA[This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\eg labels).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/understanding-contrastive-learning-via</guid>
    </item>
    <item>
      <title>IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems</title>
      <link>https://paperswithcode.com/paper/imtlab-an-open-source-platform-for-building</link>
      <description><![CDATA[We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imtlab-an-open-source-platform-for-building</guid>
    </item>
    <item>
      <title>Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?</title>
      <link>https://paperswithcode.com/paper/emergent-mixture-of-experts-can-dense-pre</link>
      <description><![CDATA[Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/emergent-mixture-of-experts-can-dense-pre</guid>
    </item>
    <item>
      <title>QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering</title>
      <link>https://paperswithcode.com/paper/qadynamics-training-dynamics-driven-synthetic</link>
      <description><![CDATA[Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/qadynamics-training-dynamics-driven-synthetic</guid>
    </item>
    <item>
      <title>Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles</title>
      <link>https://paperswithcode.com/paper/robust-wake-up-word-detection-by-two-stage</link>
      <description><![CDATA[It employs two models: a lightweight on-device model for real-time processing of the audio stream and a verification model on the server-side, which is an ensemble of heterogeneous architectures that refine detection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-wake-up-word-detection-by-two-stage</guid>
    </item>
    <item>
      <title>Watermarking LLMs with Weight Quantization</title>
      <link>https://paperswithcode.com/paper/watermarking-llms-with-weight-quantization</link>
      <description><![CDATA[Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/watermarking-llms-with-weight-quantization</guid>
    </item>
    <item>
      <title>Zipformer: A faster and better encoder for automatic speech recognition</title>
      <link>https://paperswithcode.com/paper/zipformer-a-faster-and-better-encoder-for</link>
      <description><![CDATA[The Conformer has become the most popular encoder model for automatic speech recognition (ASR).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zipformer-a-faster-and-better-encoder-for</guid>
    </item>
    <item>
      <title>NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain</title>
      <link>https://paperswithcode.com/paper/nuclearqa-a-human-made-benchmark-for-language</link>
      <description><![CDATA[As LLMs have become increasingly popular, they have been used in almost every field.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nuclearqa-a-human-made-benchmark-for-language</guid>
    </item>
    <item>
      <title>FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation</title>
      <link>https://paperswithcode.com/paper/fusionu-net-u-net-with-enhanced-skip</link>
      <description><![CDATA[One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fusionu-net-u-net-with-enhanced-skip</guid>
    </item>
    <item>
      <title>NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning</title>
      <link>https://paperswithcode.com/paper/nice-improving-panoptic-narrative-detection</link>
      <description><![CDATA[To address this, we introduce two cascading modules based on the barycenter of the mask, which are Coordinate Guided Aggregation (CGA) and Barycenter Driven Localization (BDL), responsible for segmentation and detection, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nice-improving-panoptic-narrative-detection</guid>
    </item>
    <item>
      <title>Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation</title>
      <link>https://paperswithcode.com/paper/nonet-at-semeval-2023-task-6-methodologies</link>
      <description><![CDATA[This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nonet-at-semeval-2023-task-6-methodologies</guid>
    </item>
    <item>
      <title>EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset</title>
      <link>https://paperswithcode.com/paper/exmodd-an-explanatory-multimodal-open-domain</link>
      <description><![CDATA[The need for high-quality data has been a key issue hindering the research of dialogue tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exmodd-an-explanatory-multimodal-open-domain</guid>
    </item>
    <item>
      <title>A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback</title>
      <link>https://paperswithcode.com/paper/a-modified-exp3-and-its-adaptive-variant-in</link>
      <description><![CDATA[Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-modified-exp3-and-its-adaptive-variant-in</guid>
    </item>
    <item>
      <title>In-Context Few-Shot Relation Extraction via Pre-Trained Language Models</title>
      <link>https://paperswithcode.com/paper/in-context-few-shot-relation-extraction-via</link>
      <description><![CDATA[To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-few-shot-relation-extraction-via</guid>
    </item>
    <item>
      <title>A voxel-level approach to brain age prediction: A method to assess regional brain aging</title>
      <link>https://paperswithcode.com/paper/a-voxel-level-approach-to-brain-age</link>
      <description><![CDATA[Brain aging is a regional phenomenon, a facet that remains relatively under-explored within the realm of brain age prediction research using machine learning methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-voxel-level-approach-to-brain-age</guid>
    </item>
    <item>
      <title>SODA: Robust Training of Test-Time Data Adaptors</title>
      <link>https://paperswithcode.com/paper/soda-robust-training-of-test-time-data</link>
      <description><![CDATA[Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soda-robust-training-of-test-time-data</guid>
    </item>
    <item>
      <title>Unanswerable Visual Question Answering</title>
      <link>https://paperswithcode.com/paper/unanswerable-visual-question-answering</link>
      <description><![CDATA[Teaching Visual Question Answering (VQA) models to abstain from unanswerable questions is indispensable for building a trustworthy AI system.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unanswerable-visual-question-answering</guid>
    </item>
    <item>
      <title>Compatible Transformer for Irregularly Sampled Multivariate Time Series</title>
      <link>https://paperswithcode.com/paper/compatible-transformer-for-irregularly</link>
      <description><![CDATA[To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/compatible-transformer-for-irregularly</guid>
    </item>
    <item>
      <title>Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</title>
      <link>https://paperswithcode.com/paper/towards-generic-semi-supervised-framework-for</link>
      <description><![CDATA[As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-generic-semi-supervised-framework-for</guid>
    </item>
    <item>
      <title>Evaluating LLMs for Privilege-Escalation Scenarios</title>
      <link>https://paperswithcode.com/paper/evaluating-llms-for-privilege-escalation</link>
      <description><![CDATA[We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-llms-for-privilege-escalation</guid>
    </item>
    <item>
      <title>A State-Vector Framework for Dataset Effects</title>
      <link>https://paperswithcode.com/paper/a-state-vector-framework-for-dataset-effects</link>
      <description><![CDATA[The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-state-vector-framework-for-dataset-effects</guid>
    </item>
    <item>
      <title>PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising</title>
      <link>https://paperswithcode.com/paper/puca-patch-unshuffle-and-channel-attention</link>
      <description><![CDATA[Blind-spot networks (BSNs) have been a prevalent choice to ensure J-invariance in self-supervised image denoising.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/puca-patch-unshuffle-and-channel-attention</guid>
    </item>
    <item>
      <title>Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow</title>
      <link>https://paperswithcode.com/paper/probabilistic-classification-by-density</link>
      <description><![CDATA[A family of density estimators is mixture models, such as Gaussian Mixture Model (GMM) by expectation maximization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/probabilistic-classification-by-density</guid>
    </item>
    <item>
      <title>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</title>
      <link>https://paperswithcode.com/paper/infogcn-learning-representation-by-predicting</link>
      <description><![CDATA[To overcome this barrier, we introduce InfoGCN++, an innovative extension of InfoGCN, explicitly developed for online skeleton-based action recognition.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/infogcn-learning-representation-by-predicting</guid>
    </item>
    <item>
      <title>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation</title>
      <link>https://paperswithcode.com/paper/pela-learning-parameter-efficient-models-with</link>
      <description><![CDATA[This allows for direct and efficient utilization of the low-rank model for downstream tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pela-learning-parameter-efficient-models-with</guid>
    </item>
    <item>
      <title>Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification</title>
      <link>https://paperswithcode.com/paper/taming-the-sigmoid-bottleneck-provably</link>
      <description><![CDATA[We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taming-the-sigmoid-bottleneck-provably</guid>
    </item>
    <item>
      <title>TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/tram-nerf-tracing-mirror-and-near-perfect</link>
      <description><![CDATA[Implicit representations like Neural Radiance Fields (NeRF) showed impressive results for photorealistic rendering of complex scenes with fine details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tram-nerf-tracing-mirror-and-near-perfect</guid>
    </item>
    <item>
      <title>Data Contamination Through the Lens of Time</title>
      <link>https://paperswithcode.com/paper/data-contamination-through-the-lens-of-time</link>
      <description><![CDATA[Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-contamination-through-the-lens-of-time</guid>
    </item>
    <item>
      <title>Implicit regularization via soft ascent-descent</title>
      <link>https://paperswithcode.com/paper/implicit-regularization-via-soft-ascent</link>
      <description><![CDATA[As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/implicit-regularization-via-soft-ascent</guid>
    </item>
    <item>
      <title>SoTTA: Robust Test-Time Adaptation on Noisy Data Streams</title>
      <link>https://paperswithcode.com/paper/sotta-robust-test-time-adaptation-on-noisy</link>
      <description><![CDATA[To address this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel TTA algorithm that is robust to noisy samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sotta-robust-test-time-adaptation-on-noisy</guid>
    </item>
    <item>
      <title>Towards Unified and Effective Domain Generalization</title>
      <link>https://paperswithcode.com/paper/towards-unified-and-effective-domain</link>
      <description><![CDATA[We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for $\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-unified-and-effective-domain</guid>
    </item>
  </channel>
</rss>
