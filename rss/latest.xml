<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sat, 24 Feb 2024 09:10:26 +0000</lastBuildDate>
    <item>
      <title>Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization</title>
      <link>https://paperswithcode.com/paper/take-the-bull-by-the-horns-hard-sample</link>
      <description><![CDATA[Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/take-the-bull-by-the-horns-hard-sample</guid>
    </item>
    <item>
      <title>Data Science with LLMs and Interpretable Models</title>
      <link>https://paperswithcode.com/paper/data-science-with-llms-and-interpretable</link>
      <description><![CDATA[Recent years have seen important advances in the building of interpretable models, machine learning models that are designed to be easily understood by humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-science-with-llms-and-interpretable</guid>
    </item>
    <item>
      <title>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</title>
      <link>https://paperswithcode.com/paper/conceptmath-a-bilingual-concept-wise</link>
      <description><![CDATA[This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/conceptmath-a-bilingual-concept-wise</guid>
    </item>
    <item>
      <title>Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs</title>
      <link>https://paperswithcode.com/paper/federated-learning-on-transcriptomic-data</link>
      <description><![CDATA[However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/federated-learning-on-transcriptomic-data</guid>
    </item>
    <item>
      <title>Subobject-level Image Tokenization</title>
      <link>https://paperswithcode.com/paper/subobject-level-image-tokenization</link>
      <description><![CDATA[Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/subobject-level-image-tokenization</guid>
    </item>
    <item>
      <title>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</title>
      <link>https://paperswithcode.com/paper/geneoh-diffusion-towards-generalizable-hand</link>
      <description><![CDATA[We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/geneoh-diffusion-towards-generalizable-hand</guid>
    </item>
    <item>
      <title>HyperFast: Instant Classification for Tabular Data</title>
      <link>https://paperswithcode.com/paper/hyperfast-instant-classification-for-tabular</link>
      <description><![CDATA[Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hyperfast-instant-classification-for-tabular</guid>
    </item>
    <item>
      <title>The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations</title>
      <link>https://paperswithcode.com/paper/the-impact-of-word-splitting-on-the-semantic</link>
      <description><![CDATA[When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-impact-of-word-splitting-on-the-semantic</guid>
    </item>
    <item>
      <title>DynGMA: a robust approach for learning stochastic differential equations from data</title>
      <link>https://paperswithcode.com/paper/dyngma-a-robust-approach-for-learning</link>
      <description><![CDATA[Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invariant distribution from trajectory data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dyngma-a-robust-approach-for-learning</guid>
    </item>
    <item>
      <title>IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus</title>
      <link>https://paperswithcode.com/paper/iepile-unearthing-large-scale-schema-based</link>
      <description><![CDATA[Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/iepile-unearthing-large-scale-schema-based</guid>
    </item>
    <item>
      <title>Can Language Models Act as Knowledge Bases at Scale?</title>
      <link>https://paperswithcode.com/paper/can-language-models-act-as-knowledge-bases-at</link>
      <description><![CDATA[Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-language-models-act-as-knowledge-bases-at</guid>
    </item>
    <item>
      <title>WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition</title>
      <link>https://paperswithcode.com/paper/weaksam-segment-anything-meets-weakly</link>
      <description><![CDATA[Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/weaksam-segment-anything-meets-weakly</guid>
    </item>
    <item>
      <title>HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention</title>
      <link>https://paperswithcode.com/paper/hint-high-quality-inpainting-transformer-with</link>
      <description><![CDATA[In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hint-high-quality-inpainting-transformer-with</guid>
    </item>
    <item>
      <title>Graph Parsing Networks</title>
      <link>https://paperswithcode.com/paper/graph-parsing-networks</link>
      <description><![CDATA[GPN benefits from the discrete assignments generated by the graph parsing algorithm, allowing good memory efficiency while preserving node information intact.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/graph-parsing-networks</guid>
    </item>
    <item>
      <title>Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion</title>
      <link>https://paperswithcode.com/paper/symbolic-music-generation-with-non</link>
      <description><![CDATA[We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/symbolic-music-generation-with-non</guid>
    </item>
    <item>
      <title>TinyLLaVA: A Framework of Small-scale Large Multimodal Models</title>
      <link>https://paperswithcode.com/paper/tinyllava-a-framework-of-small-scale-large</link>
      <description><![CDATA[We present the TinyLLaVA framework that provides a unified perspective in designing and analyzing the small-scale Large Multimodal Models (LMMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinyllava-a-framework-of-small-scale-large</guid>
    </item>
    <item>
      <title>INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning</title>
      <link>https://paperswithcode.com/paper/instraug-automatic-instruction-augmentation</link>
      <description><![CDATA[Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instraug-automatic-instruction-augmentation</guid>
    </item>
    <item>
      <title>PALO: A Polyglot Large Multimodal Model for 5B People</title>
      <link>https://paperswithcode.com/paper/palo-a-polyglot-large-multimodal-model-for-5b</link>
      <description><![CDATA[\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/palo-a-polyglot-large-multimodal-model-for-5b</guid>
    </item>
    <item>
      <title>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection</title>
      <link>https://paperswithcode.com/paper/nerf-det-incorporating-semantic-cues-and</link>
      <description><![CDATA[We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nerf-det-incorporating-semantic-cues-and</guid>
    </item>
    <item>
      <title>On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe</title>
      <link>https://paperswithcode.com/paper/on-the-tip-of-the-tongue-analyzing-conceptual</link>
      <description><![CDATA[Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-tip-of-the-tongue-analyzing-conceptual</guid>
    </item>
    <item>
      <title>Self-supervised Visualisation of Medical Image Datasets</title>
      <link>https://paperswithcode.com/paper/self-supervised-visualisation-of-medical</link>
      <description><![CDATA[Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/self-supervised-visualisation-of-medical</guid>
    </item>
    <item>
      <title>MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding</title>
      <link>https://paperswithcode.com/paper/mape-ppi-towards-effective-and-efficient</link>
      <description><![CDATA[In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the "vocabulary" is usually extremely small.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mape-ppi-towards-effective-and-efficient</guid>
    </item>
    <item>
      <title>Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective</title>
      <link>https://paperswithcode.com/paper/less-is-more-mitigating-multimodal</link>
      <description><![CDATA[In this paper, we explore a new angle of this issue: overly detailed training data hinders the model's ability to timely terminate generation, leading to continued outputs beyond visual perception limits.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/less-is-more-mitigating-multimodal</guid>
    </item>
    <item>
      <title>Robust Training of Federated Models with Extremely Label Deficiency</title>
      <link>https://paperswithcode.com/paper/robust-training-of-federated-models-with</link>
      <description><![CDATA[Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-training-of-federated-models-with</guid>
    </item>
    <item>
      <title>Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation</title>
      <link>https://paperswithcode.com/paper/personalized-behavior-aware-transformer-for</link>
      <description><![CDATA[To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns and multifaceted sequential collaborations in a novel way to boost recommendation performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/personalized-behavior-aware-transformer-for</guid>
    </item>
    <item>
      <title>Cleaner Pretraining Corpus Curation with Neural Web Scraping</title>
      <link>https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with</link>
      <description><![CDATA[The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cleaner-pretraining-corpus-curation-with</guid>
    </item>
    <item>
      <title>Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education</title>
      <link>https://paperswithcode.com/paper/leveraging-large-language-models-for-concept</link>
      <description><![CDATA[We assess LLMs' zero-shot performance in creating domain-specific concept graphs and introduce TutorQA, a new expert-verified NLP-focused benchmark for scientific graph reasoning and QA.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leveraging-large-language-models-for-concept</guid>
    </item>
    <item>
      <title>Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on Mediterranean Sea Water</title>
      <link>https://paperswithcode.com/paper/machine-learning-reveals-large-scale-impact</link>
      <description><![CDATA[Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that fosters biodiversity, stores carbon, releases oxygen, and provides habitat to numerous sea organisms.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/machine-learning-reveals-large-scale-impact</guid>
    </item>
    <item>
      <title>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</title>
      <link>https://paperswithcode.com/paper/modeling-3d-infant-kinetics-using-adaptive</link>
      <description><![CDATA[Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/modeling-3d-infant-kinetics-using-adaptive</guid>
    </item>
    <item>
      <title>Global Safe Sequential Learning via Efficient Knowledge Transfer</title>
      <link>https://paperswithcode.com/paper/global-safe-sequential-learning-via-efficient</link>
      <description><![CDATA[As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/global-safe-sequential-learning-via-efficient</guid>
    </item>
    <item>
      <title>SpanSeq: Similarity-based sequence data splitting method for improved development and assessment of deep learning projects</title>
      <link>https://paperswithcode.com/paper/spanseq-similarity-based-sequence-data</link>
      <description><![CDATA[The use of deep learning models in computational biology has increased massively in recent years, and is expected to do so further with the current advances in fields like Natural Language Processing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spanseq-similarity-based-sequence-data</guid>
    </item>
    <item>
      <title>Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection</title>
      <link>https://paperswithcode.com/paper/generative-adversarial-network-with-soft</link>
      <description><![CDATA[In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-adversarial-network-with-soft</guid>
    </item>
    <item>
      <title>Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition</title>
      <link>https://paperswithcode.com/paper/towards-seamless-adaptation-of-pre-trained</link>
      <description><![CDATA[Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-seamless-adaptation-of-pre-trained</guid>
    </item>
    <item>
      <title>DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</link>
      <description><![CDATA[We present DualFocus, a novel framework for integrating macro and micro perspectives within multi-modal large language models (MLLMs) to enhance vision-language task performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dualfocus-integrating-macro-and-micro</guid>
    </item>
    <item>
      <title>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</title>
      <link>https://paperswithcode.com/paper/q-probe-a-lightweight-approach-to-reward</link>
      <description><![CDATA[The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/q-probe-a-lightweight-approach-to-reward</guid>
    </item>
    <item>
      <title>UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models</title>
      <link>https://paperswithcode.com/paper/ufo-a-unified-and-flexible-framework-for</link>
      <description><![CDATA[To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ufo-a-unified-and-flexible-framework-for</guid>
    </item>
    <item>
      <title>Visual Hallucinations of Multi-modal Large Language Models</title>
      <link>https://paperswithcode.com/paper/visual-hallucinations-of-multi-modal-large</link>
      <description><![CDATA[We find that existing MLLMs such as GPT-4V, LLaVA-1. 5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-hallucinations-of-multi-modal-large</guid>
    </item>
    <item>
      <title>Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge</title>
      <link>https://paperswithcode.com/paper/hint-before-solving-prompting-guiding-llms-to</link>
      <description><![CDATA[Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hint-before-solving-prompting-guiding-llms-to</guid>
    </item>
    <item>
      <title>Swin3D++: Effective Multi-Source Pretraining for 3D Indoor Scene Understanding</title>
      <link>https://paperswithcode.com/paper/swin3d-effective-multi-source-pretraining-for</link>
      <description><![CDATA[Data diversity and abundance are essential for improving the performance and generalization of models in natural language processing and 2D vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/swin3d-effective-multi-source-pretraining-for</guid>
    </item>
    <item>
      <title>Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging</title>
      <link>https://paperswithcode.com/paper/demographic-bias-of-expert-level-vision</link>
      <description><![CDATA[Such demographic biases present over a wide range of pathologies and demographic attributes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demographic-bias-of-expert-level-vision</guid>
    </item>
    <item>
      <title>Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems</title>
      <link>https://paperswithcode.com/paper/scalable-and-provably-fair-exposure-control</link>
      <description><![CDATA[Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e. g., products, jobs, news, video) and their providers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-and-provably-fair-exposure-control</guid>
    </item>
    <item>
      <title>Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</title>
      <link>https://paperswithcode.com/paper/not-all-experts-are-equal-efficient-expert</link>
      <description><![CDATA[Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/not-all-experts-are-equal-efficient-expert</guid>
    </item>
    <item>
      <title>CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations</title>
      <link>https://paperswithcode.com/paper/cev-lm-controlled-edit-vector-language-model</link>
      <description><![CDATA[As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cev-lm-controlled-edit-vector-language-model</guid>
    </item>
    <item>
      <title>Uncertainty-Aware Evaluation for Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/uncertainty-aware-evaluation-for-vision</link>
      <description><![CDATA[Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uncertainty-aware-evaluation-for-vision</guid>
    </item>
    <item>
      <title>Transformable Gaussian Reward Function for Socially-Aware Navigation with Deep Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/transformable-gaussian-reward-function-for</link>
      <description><![CDATA[Although reinforcement learning technique has fostered the advancement of socially aware navigation, defining appropriate reward functions, especially in congested environments, has posed a significant challenge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transformable-gaussian-reward-function-for</guid>
    </item>
    <item>
      <title>Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot</title>
      <link>https://paperswithcode.com/paper/multi-hmr-multi-person-whole-body-human-mesh</link>
      <description><![CDATA[We present Multi-HMR, a strong single-shot model for multi-person 3D human mesh recovery from a single RGB image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-hmr-multi-person-whole-body-human-mesh</guid>
    </item>
    <item>
      <title>Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?</title>
      <link>https://paperswithcode.com/paper/do-llms-implicitly-determine-the-suitable</link>
      <description><![CDATA[Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-llms-implicitly-determine-the-suitable</guid>
    </item>
    <item>
      <title>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</title>
      <link>https://paperswithcode.com/paper/criticbench-benchmarking-llms-for-critique</link>
      <description><![CDATA[Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i. e., GQC reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/criticbench-benchmarking-llms-for-critique</guid>
    </item>
    <item>
      <title>MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems</title>
      <link>https://paperswithcode.com/paper/merrec-a-large-scale-multipurpose-mercari</link>
      <description><![CDATA[In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/merrec-a-large-scale-multipurpose-mercari</guid>
    </item>
    <item>
      <title>A Simple Framework Uniting Visual In-context Learning with Masked Image Modeling to Improve Ultrasound Segmentation</title>
      <link>https://paperswithcode.com/paper/a-simple-framework-uniting-visual-in-context</link>
      <description><![CDATA[Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-simple-framework-uniting-visual-in-context</guid>
    </item>
  </channel>
</rss>
