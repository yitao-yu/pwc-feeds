<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 12 Nov 2024 09:16:08 +0000</lastBuildDate>
    <item>
      <title>Fast and Efficient Transformer-based Method for Bird's Eye View Instance Prediction</title>
      <link>https://paperswithcode.com/paper/fast-and-efficient-transformer-based-method</link>
      <description><![CDATA[Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fast-and-efficient-transformer-based-method</guid>
    </item>
    <item>
      <title>'Explaining RL Decisions with Trajectories': A Reproducibility Study</title>
      <link>https://paperswithcode.com/paper/explaining-rl-decisions-with-trajectories-a</link>
      <description><![CDATA[This work investigates the reproducibility of the paper 'Explaining RL decisions with trajectories'.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/explaining-rl-decisions-with-trajectories-a</guid>
    </item>
    <item>
      <title>AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant</title>
      <link>https://paperswithcode.com/paper/assistrag-boosting-the-potential-of-large</link>
      <description><![CDATA[The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as "hallucination".]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/assistrag-boosting-the-potential-of-large</guid>
    </item>
    <item>
      <title>The Super Weight in Large Language Models</title>
      <link>https://paperswithcode.com/paper/the-super-weight-in-large-language-models</link>
      <description><![CDATA[For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-super-weight-in-large-language-models</guid>
    </item>
    <item>
      <title>White-Box Diffusion Transformer for single-cell RNA-seq generation</title>
      <link>https://paperswithcode.com/paper/white-box-diffusion-transformer-for-single</link>
      <description><![CDATA[Our White-Box Diffusion Transformer combines the generative capabilities of Diffusion model with the mathematical interpretability of White-Box transformer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/white-box-diffusion-transformer-for-single</guid>
    </item>
    <item>
      <title>Large-scale moral machine experiment on large language models</title>
      <link>https://paperswithcode.com/paper/large-scale-moral-machine-experiment-on-large</link>
      <description><![CDATA[Here, we evaluate moral judgments across 51 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/large-scale-moral-machine-experiment-on-large</guid>
    </item>
    <item>
      <title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title>
      <link>https://paperswithcode.com/paper/convmixformer-a-resource-efficient</link>
      <description><![CDATA[We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/convmixformer-a-resource-efficient</guid>
    </item>
    <item>
      <title>Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis</title>
      <link>https://paperswithcode.com/paper/token-merging-for-training-free-semantic</link>
      <description><![CDATA[In this paper, we define semantic binding as the task of associating a given object with its attribute, termed attribute binding, or linking it to other related sub-objects, referred to as object binding.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/token-merging-for-training-free-semantic</guid>
    </item>
    <item>
      <title>ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis</title>
      <link>https://paperswithcode.com/paper/enat-rethinking-spatial-temporal-interactions</link>
      <description><![CDATA[At the spatial level, we disentangle the computations of visible and mask tokens by encoding visible tokens independently, while decoding mask tokens conditioned on the fully encoded visible tokens.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/enat-rethinking-spatial-temporal-interactions</guid>
    </item>
    <item>
      <title>SAMPart3D: Segment Any Part in 3D Objects</title>
      <link>https://paperswithcode.com/paper/sampart3d-segment-any-part-in-3d-objects</link>
      <description><![CDATA[For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sampart3d-segment-any-part-in-3d-objects</guid>
    </item>
    <item>
      <title>UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</title>
      <link>https://paperswithcode.com/paper/unihr-hierarchical-representation-learning</link>
      <description><![CDATA[Experimental results across 7 datasets from 3 types of KGs demonstrate that our UniHR outperforms baselines designed for one specific kind of KG, indicating strong generalization capability of HiDR form and the effectiveness of HiSL module.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unihr-hierarchical-representation-learning</guid>
    </item>
    <item>
      <title>Variational Graph Contrastive Learning</title>
      <link>https://paperswithcode.com/paper/variational-graph-contrastive-learning</link>
      <description><![CDATA[Our approach introduces a subgraph Gaussian embedding module, which adaptively maps subgraphs to a structured Gaussian space, ensuring the preservation of graph characteristics while controlling the distribution of generated subgraphs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/variational-graph-contrastive-learning</guid>
    </item>
    <item>
      <title>United Domain Cognition Network for Salient Object Detection in Optical Remote Sensing Images</title>
      <link>https://paperswithcode.com/paper/united-domain-cognition-network-for-salient</link>
      <description><![CDATA[Technically, we first design a frequency-spatial domain transformer block that mutually amalgamates the complementary local spatial and global frequency features to strength the capability of initial input features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/united-domain-cognition-network-for-salient</guid>
    </item>
    <item>
      <title>Model Fusion through Bayesian Optimization in Language Model Fine-Tuning</title>
      <link>https://paperswithcode.com/paper/model-fusion-through-bayesian-optimization-in</link>
      <description><![CDATA[Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/model-fusion-through-bayesian-optimization-in</guid>
    </item>
    <item>
      <title>Training Neural Networks as Recognizers of Formal Languages</title>
      <link>https://paperswithcode.com/paper/training-neural-networks-as-recognizers-of</link>
      <description><![CDATA[We provide results on a variety of languages across the Chomsky hierarchy for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/training-neural-networks-as-recognizers-of</guid>
    </item>
    <item>
      <title>Neuromodulated Meta-Learning</title>
      <link>https://paperswithcode.com/paper/neuromodulated-meta-learning</link>
      <description><![CDATA[To investigate the role of flexible network structure (FNS) in meta-learning, we conduct extensive empirical and theoretical analyses, finding that model performance is tied to structure, with no universally optimal pattern across tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neuromodulated-meta-learning</guid>
    </item>
    <item>
      <title>More Expressive Attention with Negative Weights</title>
      <link>https://paperswithcode.com/paper/more-expressive-attention-with-negative</link>
      <description><![CDATA[We propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention can shift the token deletion and copying function from a static OV matrix to dynamic QK inner products, with the OV matrix now focusing more on refinement or modification.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/more-expressive-attention-with-negative</guid>
    </item>
    <item>
      <title>Benchmarking LLMs' Judgments with No Gold Standard</title>
      <link>https://paperswithcode.com/paper/benchmarking-llms-judgments-with-no-gold</link>
      <description><![CDATA[We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/benchmarking-llms-judgments-with-no-gold</guid>
    </item>
    <item>
      <title>Generative Feature Training of Thin 2-Layer Networks</title>
      <link>https://paperswithcode.com/paper/generative-feature-training-of-thin-2-layer</link>
      <description><![CDATA[We consider the approximation of functions by 2-layer neural networks with a small number of hidden weights based on the squared loss and small datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-feature-training-of-thin-2-layer</guid>
    </item>
    <item>
      <title>Decoding Visual Experience and Mapping Semantics through Whole-Brain Analysis Using fMRI Foundation Models</title>
      <link>https://paperswithcode.com/paper/decoding-visual-experience-and-mapping</link>
      <description><![CDATA[Neural decoding, the process of understanding how brain activity corresponds to different stimuli, has been a primary objective in cognitive sciences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decoding-visual-experience-and-mapping</guid>
    </item>
    <item>
      <title>ScaleKD: Strong Vision Transformers Could Be Excellent Teachers</title>
      <link>https://paperswithcode.com/paper/scalekd-strong-vision-transformers-could-be</link>
      <description><![CDATA[The student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalekd-strong-vision-transformers-could-be</guid>
    </item>
    <item>
      <title>Revisiting Ensembling in One-Shot Federated Learning</title>
      <link>https://paperswithcode.com/paper/revisiting-ensembling-in-one-shot-federated</link>
      <description><![CDATA[One-shot federated learning (OFL) trades the iterative exchange of models between clients and the server with a single round of communication, thereby saving substantially on communication costs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/revisiting-ensembling-in-one-shot-federated</guid>
    </item>
    <item>
      <title>Counterfactual Generation from Language Models</title>
      <link>https://paperswithcode.com/paper/counterfactual-generation-from-language</link>
      <description><![CDATA[Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/counterfactual-generation-from-language</guid>
    </item>
    <item>
      <title>StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification</title>
      <link>https://paperswithcode.com/paper/storyteller-improving-long-video-description</link>
      <description><![CDATA[We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/storyteller-improving-long-video-description</guid>
    </item>
    <item>
      <title>Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching</title>
      <link>https://paperswithcode.com/paper/non-adversarial-inverse-reinforcement</link>
      <description><![CDATA[In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/non-adversarial-inverse-reinforcement</guid>
    </item>
    <item>
      <title>An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning</title>
      <link>https://paperswithcode.com/paper/an-efficient-memory-module-for-graph-few-shot</link>
      <description><![CDATA[Knowledge is then distilled back into the GNN through a Graph Knowledge Distillation Module, preserving the model's memory.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-efficient-memory-module-for-graph-few-shot</guid>
    </item>
    <item>
      <title>Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training</title>
      <link>https://paperswithcode.com/paper/zeroth-order-adaptive-neuron-alignment-based</link>
      <description><![CDATA[Hence, we propose \textsc{NeuroAl}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, that modifies the block-wise and row-wise sparsity ratios to maximize the \emph{neuron alignment} among activations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/zeroth-order-adaptive-neuron-alignment-based</guid>
    </item>
    <item>
      <title>Increasing Rosacea Awareness Among Population Using Deep Learning and Statistical Approaches</title>
      <link>https://paperswithcode.com/paper/increasing-rosacea-awareness-among-population</link>
      <description><![CDATA[To increase rosacea awareness, automatic rosacea detection methods using deep learning and explainable statistical approaches are presented in this paper.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/increasing-rosacea-awareness-among-population</guid>
    </item>
    <item>
      <title>Model Editing for LLMs4Code: How Far are We?</title>
      <link>https://paperswithcode.com/paper/model-editing-for-llms4code-how-far-are-we</link>
      <description><![CDATA[Despite that, a comprehensive study that thoroughly compares and analyzes the performance of the state-of-the-art model editing techniques for adapting the knowledge within LLMs4Code across various code-related tasks is notably absent.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/model-editing-for-llms4code-how-far-are-we</guid>
    </item>
    <item>
      <title>Spatially Constrained Transformer with Efficient Global Relation Modelling for Spatio-Temporal Prediction</title>
      <link>https://paperswithcode.com/paper/spatially-constrained-transformer-with</link>
      <description><![CDATA[To address this limitation, we propose ST-SampleNet, a novel transformer-based architecture that combines CNNs with self-attention mechanisms to capture both local and global relations effectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spatially-constrained-transformer-with</guid>
    </item>
    <item>
      <title>Scaling Mesh Generation via Compressive Tokenization</title>
      <link>https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</link>
      <description><![CDATA[We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaling-mesh-generation-via-compressive</guid>
    </item>
    <item>
      <title>DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID</title>
      <link>https://paperswithcode.com/paper/dlcr-a-generative-data-expansion-framework</link>
      <description><![CDATA[To address this issue we propose DLCR, a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dlcr-a-generative-data-expansion-framework</guid>
    </item>
    <item>
      <title>Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data</title>
      <link>https://paperswithcode.com/paper/understanding-scaling-laws-with-statistical</link>
      <description><![CDATA[Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension $d$ of the training data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/understanding-scaling-laws-with-statistical</guid>
    </item>
    <item>
      <title>Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI</title>
      <link>https://paperswithcode.com/paper/gaussian-process-emulators-for-few-shot</link>
      <description><![CDATA[Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gaussian-process-emulators-for-few-shot</guid>
    </item>
    <item>
      <title>Renaissance: Investigating the Pretraining of Vision-Language Encoders</title>
      <link>https://paperswithcode.com/paper/renaissance-investigating-the-pretraining-of</link>
      <description><![CDATA[In this paper we seek to answer several questions related to the pretraining of vision-language encoders through meta-analysis.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/renaissance-investigating-the-pretraining-of</guid>
    </item>
    <item>
      <title>PRISM: Privacy-preserving Inter-Site MRI Harmonization via Disentangled Representation Learning</title>
      <link>https://paperswithcode.com/paper/prism-privacy-preserving-inter-site-mri</link>
      <description><![CDATA[Multi-site MRI studies often suffer from site-specific variations arising from differences in methodology, hardware, and acquisition protocols, thereby compromising accuracy and reliability in clinical AI/ML tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prism-privacy-preserving-inter-site-mri</guid>
    </item>
    <item>
      <title>Activation Map Compression through Tensor Decomposition for Deep Learning</title>
      <link>https://paperswithcode.com/paper/activation-map-compression-through-tensor</link>
      <description><![CDATA[Internet of Things and Deep Learning are synergetically and exponentially growing industrial fields with a massive call for their unification into a common framework called Edge AI.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/activation-map-compression-through-tensor</guid>
    </item>
    <item>
      <title>PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</title>
      <link>https://paperswithcode.com/paper/pkf-probabilistic-data-association-kalman</link>
      <description><![CDATA[We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pkf-probabilistic-data-association-kalman</guid>
    </item>
    <item>
      <title>Understanding the Role of Equivariance in Self-supervised Learning</title>
      <link>https://paperswithcode.com/paper/understanding-the-role-of-equivariance-in</link>
      <description><![CDATA[Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/understanding-the-role-of-equivariance-in</guid>
    </item>
    <item>
      <title>Optimized Inference for 1.58-bit LLMs: A Time and Memory-Efficient Algorithm for Binary and Ternary Matrix Multiplication</title>
      <link>https://paperswithcode.com/paper/optimized-inference-for-1-58-bit-llms-a-time</link>
      <description><![CDATA[To address these challenges and make LLMs more accessible and cost-effective, in this paper, we propose algorithms to improve the inference time and memory efficiency of 1. 58-bit LLMs with ternary weight matrices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/optimized-inference-for-1-58-bit-llms-a-time</guid>
    </item>
    <item>
      <title>Offline Handwritten Signature Verification Using a Stream-Based Approach</title>
      <link>https://paperswithcode.com/paper/offline-handwritten-signature-verification-1</link>
      <description><![CDATA[Handwritten Signature Verification (HSV) systems distinguish between genuine and forged signatures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/offline-handwritten-signature-verification-1</guid>
    </item>
    <item>
      <title>In-Context Learning for Preserving Patient Privacy: A Framework for Synthesizing Realistic Patient Portal Messages</title>
      <link>https://paperswithcode.com/paper/in-context-learning-for-preserving-patient</link>
      <description><![CDATA[We believe this work provides a path forward for (i) the release of large-scale synthetic patient message datasets that are stylistically similar to ground-truth samples and (ii) HIPAA-friendly data generation which requires minimal human de-identification efforts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-context-learning-for-preserving-patient</guid>
    </item>
    <item>
      <title>RL-Pruner: Structured Pruning Using Reinforcement Learning for CNN Compression and Acceleration</title>
      <link>https://paperswithcode.com/paper/rl-pruner-structured-pruning-using</link>
      <description><![CDATA[Our method is based on a key observation: filters in different layers of a neural network have varying importance to the model's performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rl-pruner-structured-pruning-using</guid>
    </item>
    <item>
      <title>Reinforcement learning for Quantum Tiq-Taq-Toe</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-for-quantum-tiq-taq</link>
      <description><![CDATA[Quantum Tiq-Taq-Toe is a well-known benchmark and playground for both quantum computing and machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-for-quantum-tiq-taq</guid>
    </item>
    <item>
      <title>Fitting Multiple Machine Learning Models with Performance Based Clustering</title>
      <link>https://paperswithcode.com/paper/fitting-multiple-machine-learning-models-with</link>
      <description><![CDATA[Traditional machine learning approaches assume that data comes from a single generating mechanism, which may not hold for most real life data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fitting-multiple-machine-learning-models-with</guid>
    </item>
    <item>
      <title>SEM-Net: Efficient Pixel Modelling for image inpainting with Spatially Enhanced SSM</title>
      <link>https://paperswithcode.com/paper/sem-net-efficient-pixel-modelling-for-image</link>
      <description><![CDATA[Image inpainting aims to repair a partially damaged image based on the information from known regions of the images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sem-net-efficient-pixel-modelling-for-image</guid>
    </item>
    <item>
      <title>SAN: Structure-Aware Network for Complex and Long-tailed Chinese Text Recognition</title>
      <link>https://paperswithcode.com/paper/san-structure-aware-network-for-complex-and</link>
      <description><![CDATA[Hence in this work, we propose a structure-aware network utilizing the hierarchical composition information to improve the recognition performance of complex characters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/san-structure-aware-network-for-complex-and</guid>
    </item>
    <item>
      <title>SplatFormer: Point Transformer for Robust 3D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</link>
      <description><![CDATA[To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/splatformer-point-transformer-for-robust-3d</guid>
    </item>
    <item>
      <title>Local Implicit Wavelet Transformer for Arbitrary-Scale Super-Resolution</title>
      <link>https://paperswithcode.com/paper/local-implicit-wavelet-transformer-for</link>
      <description><![CDATA[Most existing methods predict the pixel in the SR image based on the queried coordinate and ensemble nearby features, overlooking the importance of incorporating high-frequency prior information in images, which results in limited performance in reconstructing high-frequency texture details in images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/local-implicit-wavelet-transformer-for</guid>
    </item>
    <item>
      <title>Does This Summary Answer My Question? Modeling Query-Focused Summary Readers with Rational Speech Acts</title>
      <link>https://paperswithcode.com/paper/does-this-summary-answer-my-question-modeling</link>
      <description><![CDATA[Query-focused summarization (QFS) is the task of generating a summary in response to a user-written query.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/does-this-summary-answer-my-question-modeling</guid>
    </item>
  </channel>
</rss>
