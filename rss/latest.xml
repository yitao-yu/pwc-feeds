<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 17 Oct 2023 21:06:37 +0000</lastBuildDate>
    <item>
      <title>An active learning convolutional neural network for predicting river flow in a human impacted system</title>
      <link>https://paperswithcode.com/paper/an-active-learning-convolutional-neural</link>
      <description><![CDATA[Here we find that a convolutional Long Short-Term Memory (LSTM) network is well suited to modeling flow in parts of this basin that are strongly impacted by water projects as well as ones that are relatively free from direct human modifications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-active-learning-convolutional-neural</guid>
    </item>
    <item>
      <title>A Survey on Video Diffusion Models</title>
      <link>https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</link>
      <description><![CDATA[However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-video-diffusion-models</guid>
    </item>
    <item>
      <title>A cross Transformer for image denoising</title>
      <link>https://paperswithcode.com/paper/a-cross-transformer-for-image-denoising</link>
      <description><![CDATA[To avoid loss of key information, PB uses three heterogeneous networks to implement multiple interactions of multi-level features to broadly search for extra information for improving the adaptability of an obtained denoiser for complex scenes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-cross-transformer-for-image-denoising</guid>
    </item>
    <item>
      <title>OpenAgents: An Open Platform for Language Agents in the Wild</title>
      <link>https://paperswithcode.com/paper/openagents-an-open-platform-for-language</link>
      <description><![CDATA[Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/openagents-an-open-platform-for-language</guid>
    </item>
    <item>
      <title>Can Word Sense Distribution Detect Semantic Changes of Words?</title>
      <link>https://paperswithcode.com/paper/can-word-sense-distribution-detect-semantic</link>
      <description><![CDATA[Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-word-sense-distribution-detect-semantic</guid>
    </item>
    <item>
      <title>TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields</title>
      <link>https://paperswithcode.com/paper/tram-nerf-tracing-mirror-and-near-perfect</link>
      <description><![CDATA[Implicit representations like Neural Radiance Fields (NeRF) showed impressive results for photorealistic rendering of complex scenes with fine details.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tram-nerf-tracing-mirror-and-near-perfect</guid>
    </item>
    <item>
      <title>G-SPEED: General SParse Efficient Editing MoDel</title>
      <link>https://paperswithcode.com/paper/g-speed-general-sparse-efficient-editing</link>
      <description><![CDATA[Large Language Models~(LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/g-speed-general-sparse-efficient-editing</guid>
    </item>
    <item>
      <title>On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data</title>
      <link>https://paperswithcode.com/paper/on-the-transferability-of-learning-models-for</link>
      <description><![CDATA[Yet, there is no comprehensive analysis of their transferability, i. e., to which extent a model trained on a source domain can be readily applicable to a target domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-transferability-of-learning-models-for</guid>
    </item>
    <item>
      <title>Image super-resolution via dynamic network</title>
      <link>https://paperswithcode.com/paper/image-super-resolution-via-dynamic-network</link>
      <description><![CDATA[In this paper, we present a dynamic network for image super-resolution (DSRNet), which contains a residual enhancement block, wide enhancement block, feature refinement block and construction block.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/image-super-resolution-via-dynamic-network</guid>
    </item>
    <item>
      <title>Learning visual-based deformable object rearrangement with local graph neural networks</title>
      <link>https://paperswithcode.com/paper/learning-visual-based-deformable-object</link>
      <description><![CDATA[Our method reaches much higher success rates on a variety of deformable rearrangement tasks (96. 3% on average) than state-of-the-art method in simulation experiments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-visual-based-deformable-object</guid>
    </item>
    <item>
      <title>Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization</title>
      <link>https://paperswithcode.com/paper/multi-stage-pre-training-enhanced-by-chatgpt</link>
      <description><![CDATA[Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-stage-pre-training-enhanced-by-chatgpt</guid>
    </item>
    <item>
      <title>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</title>
      <link>https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</link>
      <description><![CDATA[Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/real-time-photorealistic-dynamic-scene</guid>
    </item>
    <item>
      <title>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</title>
      <link>https://paperswithcode.com/paper/bioplanner-automatic-evaluation-of-llms-on</link>
      <description><![CDATA[Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bioplanner-automatic-evaluation-of-llms-on</guid>
    </item>
    <item>
      <title>Llemma: An Open Language Model For Mathematics</title>
      <link>https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</link>
      <description><![CDATA[We present Llemma, a large language model for mathematics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/llemma-an-open-language-model-for-mathematics</guid>
    </item>
    <item>
      <title>Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems</title>
      <link>https://paperswithcode.com/paper/exploring-the-power-of-graph-neural-networks</link>
      <description><![CDATA[Recently, machine learning, particularly message-passing graph neural networks (MPNNs), has gained traction in enhancing exact optimization algorithms.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-the-power-of-graph-neural-networks</guid>
    </item>
    <item>
      <title>One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer</title>
      <link>https://paperswithcode.com/paper/one-for-all-all-for-one-bypassing</link>
      <description><![CDATA[Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/one-for-all-all-for-one-bypassing</guid>
    </item>
    <item>
      <title>UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</title>
      <link>https://paperswithcode.com/paper/uno-dst-leveraging-unlabelled-data-in-zero</link>
      <description><![CDATA[Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uno-dst-leveraging-unlabelled-data-in-zero</guid>
    </item>
    <item>
      <title>Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective</title>
      <link>https://paperswithcode.com/paper/repetition-in-repetition-out-towards</link>
      <description><![CDATA[There are a number of diverging hypotheses about the neural text degeneration problem, i. e., generating repetitive and dull loops, which makes this problem both interesting and confusing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/repetition-in-repetition-out-towards</guid>
    </item>
    <item>
      <title>Generative Calibration for In-context Learning</title>
      <link>https://paperswithcode.com/paper/generative-calibration-for-in-context</link>
      <description><![CDATA[In this paper, we for the first time theoretically and empirically identify that such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-calibration-for-in-context</guid>
    </item>
    <item>
      <title>$\textit{Swap and Predict}$ -- Predicting the Semantic Changes in Words across Corpora by Context Swapping</title>
      <link>https://paperswithcode.com/paper/textit-swap-and-predict-predicting-the</link>
      <description><![CDATA[Intuitively, if the meaning of $w$ does not change between $\mathcal{C}_1$ and $\mathcal{C}_2$, we would expect the distributions of contextualised word embeddings of $w$ to remain the same before and after this random swapping process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/textit-swap-and-predict-predicting-the</guid>
    </item>
    <item>
      <title>Decomposed Prompt Tuning via Low-Rank Reparameterization</title>
      <link>https://paperswithcode.com/paper/decomposed-prompt-tuning-via-low-rank</link>
      <description><![CDATA[While prompt tuning approaches have achieved competitive performance with high efficiency, we observe that they invariably employ the same initialization process, wherein the soft prompt is either randomly initialized or derived from an existing embedding vocabulary.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/decomposed-prompt-tuning-via-low-rank</guid>
    </item>
    <item>
      <title>Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification</title>
      <link>https://paperswithcode.com/paper/taming-the-sigmoid-bottleneck-provably</link>
      <description><![CDATA[We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/taming-the-sigmoid-bottleneck-provably</guid>
    </item>
    <item>
      <title>SoTTA: Robust Test-Time Adaptation on Noisy Data Streams</title>
      <link>https://paperswithcode.com/paper/sotta-robust-test-time-adaptation-on-noisy</link>
      <description><![CDATA[To address this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel TTA algorithm that is robust to noisy samples.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sotta-robust-test-time-adaptation-on-noisy</guid>
    </item>
    <item>
      <title>On Generative Agents in Recommendation</title>
      <link>https://paperswithcode.com/paper/on-generative-agents-in-recommendation</link>
      <description><![CDATA[Recommender systems are the cornerstone of today's information dissemination, yet a disconnect between offline metrics and online performance greatly hinders their development.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-generative-agents-in-recommendation</guid>
    </item>
    <item>
      <title>The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions</title>
      <link>https://paperswithcode.com/paper/the-mixtures-and-the-neural-critics-on-the</link>
      <description><![CDATA[Mutual information quantifies the dependence between two random variables and remains invariant under diffeomorphisms.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-mixtures-and-the-neural-critics-on-the</guid>
    </item>
    <item>
      <title>Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance</title>
      <link>https://paperswithcode.com/paper/towards-a-better-understanding-of-variations</link>
      <description><![CDATA[Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-a-better-understanding-of-variations</guid>
    </item>
    <item>
      <title>Data Contamination Through the Lens of Time</title>
      <link>https://paperswithcode.com/paper/data-contamination-through-the-lens-of-time</link>
      <description><![CDATA[Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/data-contamination-through-the-lens-of-time</guid>
    </item>
    <item>
      <title>Learning optimal integration of spatial and temporal information in noisy chemotaxis</title>
      <link>https://paperswithcode.com/paper/learning-optimal-integration-of-spatial-and</link>
      <description><![CDATA[We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-optimal-integration-of-spatial-and</guid>
    </item>
    <item>
      <title>NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models</title>
      <link>https://paperswithcode.com/paper/nash-a-simple-unified-framework-of-structured</link>
      <description><![CDATA[Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/nash-a-simple-unified-framework-of-structured</guid>
    </item>
    <item>
      <title>Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends</title>
      <link>https://paperswithcode.com/paper/matching-the-neuronal-representations-of-v1</link>
      <description><![CDATA[Here, we further explore this result and show that the neuronal representations that emerge from precisely matching the distribution of RF properties found in primate V1 is key for this improvement in robustness.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/matching-the-neuronal-representations-of-v1</guid>
    </item>
    <item>
      <title>ViPE: Visualise Pretty-much Everything</title>
      <link>https://paperswithcode.com/paper/vipe-visualise-pretty-much-everything</link>
      <description><![CDATA[Figurative and non-literal expressions are profoundly integrated in human communication.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vipe-visualise-pretty-much-everything</guid>
    </item>
    <item>
      <title>Scene Graph Conditioning in Latent Diffusion</title>
      <link>https://paperswithcode.com/paper/scene-graph-conditioning-in-latent-diffusion</link>
      <description><![CDATA[Diffusion models excel in image generation but lack detailed semantic control using text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scene-graph-conditioning-in-latent-diffusion</guid>
    </item>
    <item>
      <title>Interpreting and Controlling Vision Foundation Models via Text Explanations</title>
      <link>https://paperswithcode.com/paper/interpreting-and-controlling-vision</link>
      <description><![CDATA[Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/interpreting-and-controlling-vision</guid>
    </item>
    <item>
      <title>Implicit regularization via soft ascent-descent</title>
      <link>https://paperswithcode.com/paper/implicit-regularization-via-soft-ascent</link>
      <description><![CDATA[As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/implicit-regularization-via-soft-ascent</guid>
    </item>
    <item>
      <title>Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques</title>
      <link>https://paperswithcode.com/paper/investigating-bias-in-multilingual-language</link>
      <description><![CDATA[This paper investigates the transferability of debiasing techniques across different languages within multilingual models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/investigating-bias-in-multilingual-language</guid>
    </item>
    <item>
      <title>Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction</title>
      <link>https://paperswithcode.com/paper/node-based-knowledge-graph-contrastive</link>
      <description><![CDATA[Particularly in biomedical relationship prediction tasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28, DRKG17k-21, and BioKG72k-14, especially in predicting drug combination relationships.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/node-based-knowledge-graph-contrastive</guid>
    </item>
    <item>
      <title>DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task</title>
      <link>https://paperswithcode.com/paper/demonsf-a-multi-task-demonstration-based</link>
      <description><![CDATA[Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/demonsf-a-multi-task-demonstration-based</guid>
    </item>
    <item>
      <title>RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models</title>
      <link>https://paperswithcode.com/paper/robollm-robotic-vision-tasks-grounded-on</link>
      <description><![CDATA[Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robollm-robotic-vision-tasks-grounded-on</guid>
    </item>
    <item>
      <title>KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training</title>
      <link>https://paperswithcode.com/paper/kakurenbo-adaptively-hiding-samples-in-deep</link>
      <description><![CDATA[This paper proposes a method for hiding the least-important samples during the training of deep neural networks to increase efficiency, i. e., to reduce the cost of training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kakurenbo-adaptively-hiding-samples-in-deep</guid>
    </item>
    <item>
      <title>PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising</title>
      <link>https://paperswithcode.com/paper/puca-patch-unshuffle-and-channel-attention</link>
      <description><![CDATA[Blind-spot networks (BSNs) have been a prevalent choice to ensure J-invariance in self-supervised image denoising.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/puca-patch-unshuffle-and-channel-attention</guid>
    </item>
    <item>
      <title>Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers</title>
      <link>https://paperswithcode.com/paper/factored-verification-detecting-and-reducing</link>
      <description><![CDATA[Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/factored-verification-detecting-and-reducing</guid>
    </item>
    <item>
      <title>Prior-Free Continual Learning with Unlabeled Data in the Wild</title>
      <link>https://paperswithcode.com/paper/prior-free-continual-learning-with-unlabeled</link>
      <description><![CDATA[Existing CL methods usually reduce forgetting with task priors, \ie using task identity or a subset of previously seen samples for model training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prior-free-continual-learning-with-unlabeled</guid>
    </item>
    <item>
      <title>Character-LLM: A Trainable Agent for Role-Playing</title>
      <link>https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</link>
      <description><![CDATA[Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/character-llm-a-trainable-agent-for-role</guid>
    </item>
    <item>
      <title>Towards Unified and Effective Domain Generalization</title>
      <link>https://paperswithcode.com/paper/towards-unified-and-effective-domain</link>
      <description><![CDATA[We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for $\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-unified-and-effective-domain</guid>
    </item>
    <item>
      <title>FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</title>
      <link>https://paperswithcode.com/paper/fate-llm-a-industrial-grade-federated</link>
      <description><![CDATA[FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fate-llm-a-industrial-grade-federated</guid>
    </item>
    <item>
      <title>RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</title>
      <link>https://paperswithcode.com/paper/roomdesigner-encoding-anchor-latents-for</link>
      <description><![CDATA[Indoor scene generation aims at creating shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/roomdesigner-encoding-anchor-latents-for</guid>
    </item>
    <item>
      <title>GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers</title>
      <link>https://paperswithcode.com/paper/gta-a-geometry-aware-attention-mechanism-for</link>
      <description><![CDATA[As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gta-a-geometry-aware-attention-mechanism-for</guid>
    </item>
    <item>
      <title>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models</title>
      <link>https://paperswithcode.com/paper/cross-lingual-consistency-of-factual</link>
      <description><![CDATA[Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cross-lingual-consistency-of-factual</guid>
    </item>
    <item>
      <title>A Computational Framework for Solving Wasserstein Lagrangian Flows</title>
      <link>https://paperswithcode.com/paper/a-computational-framework-for-solving</link>
      <description><![CDATA[The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-computational-framework-for-solving</guid>
    </item>
    <item>
      <title>Generalizing Medical Image Representations via Quaternion Wavelet Networks</title>
      <link>https://paperswithcode.com/paper/generalizing-medical-image-representations</link>
      <description><![CDATA[The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generalizing-medical-image-representations</guid>
    </item>
  </channel>
</rss>
