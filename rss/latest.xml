<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Sun, 26 May 2024 09:13:19 +0000</lastBuildDate>
    <item>
      <title>Evaluating deep learning methods applied to Landsat time series subsequences to detect and classify boreal forest disturbances events: The challenge of partial and progressive disturbances</title>
      <link>https://paperswithcode.com/paper/evaluating-deep-learning-methods-applied-to</link>
      <description><![CDATA[The goal of this paper is to explore the use of a subset of Landsat time series and deep learning models to identify both the type and the year of disturbances, including low-severity and gradual disturbances, in the boreal forest of eastern Canada at the pixel level.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-deep-learning-methods-applied-to</guid>
    </item>
    <item>
      <title>CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments</title>
      <link>https://paperswithcode.com/paper/coped-advancing-multi-robot-collaborative</link>
      <description><![CDATA[We believe this work will unlock the potential research of high-level scene understanding through multi-modal collaborative perception in multi-robot settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/coped-advancing-multi-robot-collaborative</guid>
    </item>
    <item>
      <title>FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models</title>
      <link>https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</link>
      <description><![CDATA[As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/finrobot-an-open-source-ai-agent-platform-for</guid>
    </item>
    <item>
      <title>Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution</title>
      <link>https://paperswithcode.com/paper/differentiable-annealed-importance-sampling-1</link>
      <description><![CDATA[Differentiable annealed importance sampling (DAIS), proposed by Geffner & Domke (2021) and Zhang et al. (2021), allows optimizing, among others, over the initial distribution of AIS.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/differentiable-annealed-importance-sampling-1</guid>
    </item>
    <item>
      <title>ViHateT5: Enhancing Hate Speech Detection in Vietnamese With A Unified Text-to-Text Transformer Model</title>
      <link>https://paperswithcode.com/paper/vihatet5-enhancing-hate-speech-detection-in</link>
      <description><![CDATA[Recent advancements in hate speech detection (HSD) in Vietnamese have made significant progress, primarily attributed to the emergence of transformer-based pre-trained language models, particularly those built on the BERT architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/vihatet5-enhancing-hate-speech-detection-in</guid>
    </item>
    <item>
      <title>YOLOv10: Real-Time End-to-End Object Detection</title>
      <link>https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</link>
      <description><![CDATA[In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/yolov10-real-time-end-to-end-object-detection</guid>
    </item>
    <item>
      <title>MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability</title>
      <link>https://paperswithcode.com/paper/mogu-a-framework-for-enhancing-safety-of-open</link>
      <description><![CDATA[When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mogu-a-framework-for-enhancing-safety-of-open</guid>
    </item>
    <item>
      <title>Scalable Optimization in the Modular Norm</title>
      <link>https://paperswithcode.com/paper/scalable-optimization-in-the-modular-norm</link>
      <description><![CDATA[When ramping up the width of a single layer, graceful scaling of training has been linked to the need to normalize the weights and their updates in the "natural norm" particular to that layer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scalable-optimization-in-the-modular-norm</guid>
    </item>
    <item>
      <title>GCondenser: Benchmarking Graph Condensation</title>
      <link>https://paperswithcode.com/paper/gcondenser-benchmarking-graph-condensation</link>
      <description><![CDATA[Large-scale graphs are valuable for graph representation learning, yet the abundant data in these graphs hinders the efficiency of the training process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gcondenser-benchmarking-graph-condensation</guid>
    </item>
    <item>
      <title>Prediction of cancer dynamics under treatment using Bayesian neural networks: A simulated study</title>
      <link>https://paperswithcode.com/paper/prediction-of-cancer-dynamics-under-treatment</link>
      <description><![CDATA[In this work, we develop a hierarchical Bayesian model of subpopulation dynamics that uses baseline covariate information to predict cancer dynamics under treatment, inspired by cancer dynamics in multiple myeloma (MM), where serum M protein is a well-known proxy of tumor burden.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prediction-of-cancer-dynamics-under-treatment</guid>
    </item>
    <item>
      <title>Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title>
      <link>https://paperswithcode.com/paper/markovian-flow-matching-accelerating-mcmc</link>
      <description><![CDATA[Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/markovian-flow-matching-accelerating-mcmc</guid>
    </item>
    <item>
      <title>Newton Informed Neural Operator for Computing Multiple Solutions of Nonlinear Partials Differential Equations</title>
      <link>https://paperswithcode.com/paper/newton-informed-neural-operator-for-computing</link>
      <description><![CDATA[Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/newton-informed-neural-operator-for-computing</guid>
    </item>
    <item>
      <title>TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting</title>
      <link>https://paperswithcode.com/paper/timemixer-decomposable-multiscale-mixing-for-1</link>
      <description><![CDATA[Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, which is based on an intuitive but important observation that time series present distinct patterns in different sampling scales.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/timemixer-decomposable-multiscale-mixing-for-1</guid>
    </item>
    <item>
      <title>Which Information Matters? Dissecting Human-written Multi-document Summaries with Partial Information Decomposition</title>
      <link>https://paperswithcode.com/paper/which-information-matters-dissecting-human</link>
      <description><![CDATA[Understanding the nature of high-quality summaries is crucial to further improve the performance of multi-document summarization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/which-information-matters-dissecting-human</guid>
    </item>
    <item>
      <title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title>
      <link>https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</link>
      <description><![CDATA[In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hipporag-neurobiologically-inspired-long-term</guid>
    </item>
    <item>
      <title>Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs</title>
      <link>https://paperswithcode.com/paper/mitigating-quantization-errors-due-to</link>
      <description><![CDATA[In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mitigating-quantization-errors-due-to</guid>
    </item>
    <item>
      <title>A Watermark for Low-entropy and Unbiased Generation in Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-watermark-for-low-entropy-and-unbiased</link>
      <description><![CDATA[This study proposes the Sampling One Then Accepting (STA-1) method, an unbiased watermark that does not require access to LLMs nor prompts during detection and has statistical guarantees for the type II error.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-watermark-for-low-entropy-and-unbiased</guid>
    </item>
    <item>
      <title>Impact of Non-Standard Unicode Characters on Security and Comprehension in Large Language Models</title>
      <link>https://paperswithcode.com/paper/impact-of-non-standard-unicode-characters-on</link>
      <description><![CDATA[Our work exposes these models' inherent vulnerabilities and challenges the notion of human-level language comprehension of these models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/impact-of-non-standard-unicode-characters-on</guid>
    </item>
    <item>
      <title>Controllable Continual Test-Time Adaptation</title>
      <link>https://paperswithcode.com/paper/controllable-continual-test-time-adaptation</link>
      <description><![CDATA[Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controllable-continual-test-time-adaptation</guid>
    </item>
    <item>
      <title>U-TELL: Unsupervised Task Expert Lifelong Learning</title>
      <link>https://paperswithcode.com/paper/u-tell-unsupervised-task-expert-lifelong</link>
      <description><![CDATA[To address these issues, we propose an unsupervised CL model with task experts called Unsupervised Task Expert Lifelong Learning (U-TELL) to continually learn the data arriving in a sequence addressing catastrophic forgetting.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/u-tell-unsupervised-task-expert-lifelong</guid>
    </item>
    <item>
      <title>Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models</title>
      <link>https://paperswithcode.com/paper/combining-denoising-autoencoders-with</link>
      <description><![CDATA[Recently, using large pretrained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters or combinations with unsupervised approaches, among many others.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/combining-denoising-autoencoders-with</guid>
    </item>
    <item>
      <title>EHRMamba: Towards Generalizable and Scalable Foundation Models for Electronic Health Records</title>
      <link>https://paperswithcode.com/paper/ehrmamba-towards-generalizable-and-scalable</link>
      <description><![CDATA[We also introduce a novel approach to Multitask Prompted Finetuning (MTF) for EHR data, which enables EHRMamba to simultaneously learn multiple clinical tasks in a single finetuning phase, significantly enhancing deployment and cross-task generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/ehrmamba-towards-generalizable-and-scalable</guid>
    </item>
    <item>
      <title>TerDiT: Ternary Diffusion Models with Transformers</title>
      <link>https://paperswithcode.com/paper/terdit-ternary-diffusion-models-with</link>
      <description><![CDATA[Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/terdit-ternary-diffusion-models-with</guid>
    </item>
    <item>
      <title>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</title>
      <link>https://paperswithcode.com/paper/rectifid-personalizing-rectified-flow-with</link>
      <description><![CDATA[Our study shows that based on a recent rectified flow framework, the major limitation of vanilla classifier guidance in requiring a special classifier can be resolved with a simple fixed-point solution, allowing flexible personalization with off-the-shelf image discriminators.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rectifid-personalizing-rectified-flow-with</guid>
    </item>
    <item>
      <title>ReactXT: Understanding Molecular "Reaction-ship" via Reaction-Contextualized Molecule-Text Pretraining</title>
      <link>https://paperswithcode.com/paper/reactxt-understanding-molecular-reaction-ship</link>
      <description><![CDATA[To resolve the challenges above, we propose a new pretraining method, ReactXT, for reaction-text modeling, and a new dataset, OpenExp, for experimental procedure prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reactxt-understanding-molecular-reaction-ship</guid>
    </item>
    <item>
      <title>SimPO: Simple Preference Optimization with a Reference-Free Reward</title>
      <link>https://paperswithcode.com/paper/simpo-simple-preference-optimization-with-a</link>
      <description><![CDATA[Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44. 7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33. 8 win rate on Arena-Hard -- making it the strongest 8B open-source model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simpo-simple-preference-optimization-with-a</guid>
    </item>
    <item>
      <title>RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models</title>
      <link>https://paperswithcode.com/paper/refchecker-reference-based-fine-grained</link>
      <description><![CDATA[In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/refchecker-reference-based-fine-grained</guid>
    </item>
    <item>
      <title>LoRA-Ensemble: Efficient Uncertainty Modelling for Self-attention Networks</title>
      <link>https://paperswithcode.com/paper/lora-ensemble-efficient-uncertainty-modelling</link>
      <description><![CDATA[We introduce LoRA-Ensemble, a parameter-efficient deep ensemble method for self-attention networks, which is based on Low-Rank Adaptation (LoRA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-ensemble-efficient-uncertainty-modelling</guid>
    </item>
    <item>
      <title>Joining up the scattered anticancer knowledge on auraptene and umbelliprenin: a meta-analysis</title>
      <link>https://paperswithcode.com/paper/joining-up-the-scattered-anticancer-knowledge</link>
      <description><![CDATA[Mixed-effects models revealed significant negative associations between coumarin dose and viability for AUR (est.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/joining-up-the-scattered-anticancer-knowledge</guid>
    </item>
    <item>
      <title>D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup</title>
      <link>https://paperswithcode.com/paper/d-miso-editing-dynamic-3d-scenes-using-multi</link>
      <description><![CDATA[Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d-miso-editing-dynamic-3d-scenes-using-multi</guid>
    </item>
    <item>
      <title>Not All Language Model Features Are Linear</title>
      <link>https://paperswithcode.com/paper/not-all-language-model-features-are-linear</link>
      <description><![CDATA[Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/not-all-language-model-features-are-linear</guid>
    </item>
    <item>
      <title>WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models</title>
      <link>https://paperswithcode.com/paper/wise-rethinking-the-knowledge-memory-for</link>
      <description><![CDATA[In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wise-rethinking-the-knowledge-memory-for</guid>
    </item>
    <item>
      <title>Adaptive Teaching in Heterogeneous Agents: Balancing Surprise in Sparse Reward Scenarios</title>
      <link>https://paperswithcode.com/paper/adaptive-teaching-in-heterogeneous-agents</link>
      <description><![CDATA[Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adaptive-teaching-in-heterogeneous-agents</guid>
    </item>
    <item>
      <title>DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis</title>
      <link>https://paperswithcode.com/paper/dim-diffusion-mamba-for-efficient-high</link>
      <description><![CDATA[In addition, to further improve training efficiency for high-resolution image generation with DiM, we investigate ``weak-to-strong'' training strategy that pretrains DiM on low-resolution images ($256\times 256$) and then finetune it on high-resolution images ($512 \times 512$).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dim-diffusion-mamba-for-efficient-high</guid>
    </item>
    <item>
      <title>An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models</title>
      <link>https://paperswithcode.com/paper/an-empirical-study-of-training-state-of-the</link>
      <description><![CDATA[In the rapidly evolving field of autonomous driving, precise segmentation of LiDAR data is crucial for understanding complex 3D environments.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-empirical-study-of-training-state-of-the</guid>
    </item>
    <item>
      <title>CAPE: Context-Adaptive Positional Encoding for Length Extrapolation</title>
      <link>https://paperswithcode.com/paper/cape-context-adaptive-positional-encoding-for</link>
      <description><![CDATA[Positional encoding plays a crucial role in transformers, significantly impacting model performance and length generalization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cape-context-adaptive-positional-encoding-for</guid>
    </item>
    <item>
      <title>A Neighbor-Searching Discrepancy-based Drift Detection Scheme for Learning Evolving Data</title>
      <link>https://paperswithcode.com/paper/a-neighbor-searching-discrepancy-based-drift</link>
      <description><![CDATA[Uncertain changes in data streams present challenges for machine learning models to dynamically adapt and uphold performance in real-time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-neighbor-searching-discrepancy-based-drift</guid>
    </item>
    <item>
      <title>Efficiency for Free: Ideal Data Are Transportable Representations</title>
      <link>https://paperswithcode.com/paper/efficiency-for-free-ideal-data-are</link>
      <description><![CDATA[Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficiency-for-free-ideal-data-are</guid>
    </item>
    <item>
      <title>Metadata Integration for Spam Reviews Detection on Vietnamese E-commerce Websites</title>
      <link>https://paperswithcode.com/paper/metadata-integration-for-spam-reviews</link>
      <description><![CDATA[The problem of detecting spam reviews (opinions) has received significant attention in recent years, especially with the rapid development of e-commerce.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/metadata-integration-for-spam-reviews</guid>
    </item>
    <item>
      <title>Reinforcement Learning for Adaptive MCMC</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-for-adaptive-mcmc</link>
      <description><![CDATA[An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-for-adaptive-mcmc</guid>
    </item>
    <item>
      <title>Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation</title>
      <link>https://paperswithcode.com/paper/do-language-models-enjoy-their-own-stories</link>
      <description><![CDATA[Storytelling is an integral part of human experience and plays a crucial role in social interactions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/do-language-models-enjoy-their-own-stories</guid>
    </item>
    <item>
      <title>What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions</title>
      <link>https://paperswithcode.com/paper/what-is-your-data-worth-to-gpt-llm-scale-data</link>
      <description><![CDATA[Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/what-is-your-data-worth-to-gpt-llm-scale-data</guid>
    </item>
    <item>
      <title>Mosaic IT: Enhancing Instruction Tuning with Data Mosaics</title>
      <link>https://paperswithcode.com/paper/mosaic-it-enhancing-instruction-tuning-with</link>
      <description><![CDATA[Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mosaic-it-enhancing-instruction-tuning-with</guid>
    </item>
    <item>
      <title>Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization</title>
      <link>https://paperswithcode.com/paper/adversarial-training-of-two-layer-polynomial</link>
      <description><![CDATA[Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of polynomial activation networks via the S-procedure.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/adversarial-training-of-two-layer-polynomial</guid>
    </item>
    <item>
      <title>DirectMultiStep: Direct Route Generation for Multi-Step Retrosynthesis</title>
      <link>https://paperswithcode.com/paper/directmultistep-direct-route-generation-for</link>
      <description><![CDATA[Traditional computer-aided synthesis planning (CASP) methods rely on iterative single-step predictions, leading to exponential search space growth that limits efficiency and scalability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/directmultistep-direct-route-generation-for</guid>
    </item>
    <item>
      <title>A Multilingual Similarity Dataset for News Article Frame</title>
      <link>https://paperswithcode.com/paper/a-multilingual-similarity-dataset-for-news</link>
      <description><![CDATA[Overall we introduce the most extensive cross-lingual news article similarity dataset available to date with 26, 555 labeled news article pairs across 10 languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-multilingual-similarity-dataset-for-news</guid>
    </item>
    <item>
      <title>DeTox: Toxic Subspace Projection for Model Editing</title>
      <link>https://paperswithcode.com/paper/detox-toxic-subspace-projection-for-model</link>
      <description><![CDATA[Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/detox-toxic-subspace-projection-for-model</guid>
    </item>
    <item>
      <title>Identifiability of Differential-Algebraic Systems</title>
      <link>https://paperswithcode.com/paper/identifiability-of-differential-algebraic</link>
      <description><![CDATA[A fundamental requirement is the existence of a unique set of parameters for a chosen model structure, an issue commonly referred to as identifiability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/identifiability-of-differential-algebraic</guid>
    </item>
    <item>
      <title>DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation</title>
      <link>https://paperswithcode.com/paper/diffnorm-self-supervised-normalization-for</link>
      <description><![CDATA[Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffnorm-self-supervised-normalization-for</guid>
    </item>
    <item>
      <title>Leveraging World Events to Predict E-Commerce Consumer Demand under Anomaly</title>
      <link>https://paperswithcode.com/paper/leveraging-world-events-to-predict-e-commerce</link>
      <description><![CDATA[However, reliable time series sales forecasting for e-commerce is difficult, especially during periods with many anomalies, as can often happen during pandemics, abnormal weather, or sports events.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leveraging-world-events-to-predict-e-commerce</guid>
    </item>
  </channel>
</rss>
