<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 13 Apr 2023 21:06:16 +0000</lastBuildDate>
    <item>
      <title>Rethinking Dense Retrieval's Few-Shot Ability</title>
      <link>https://paperswithcode.com/paper/rethinking-dense-retrieval-s-few-shot-ability</link>
      <description><![CDATA[Moreover, the dataset is disjointed into base and novel classes, allowing DR models to be continuously trained on ample data from base classes and a few samples in novel classes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rethinking-dense-retrieval-s-few-shot-ability</guid>
    </item>
    <item>
      <title>Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera</title>
      <link>https://paperswithcode.com/paper/generating-aligned-pseudo-supervision-from</link>
      <description><![CDATA[Due to the difficulty in collecting large-scale and perfectly aligned paired training data for Under-Display Camera (UDC) image restoration, previous methods resort to monitor-based image systems or simulation-based methods, sacrificing the realness of the data and introducing domain gaps.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generating-aligned-pseudo-supervision-from</guid>
    </item>
    <item>
      <title>GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning</title>
      <link>https://paperswithcode.com/paper/gpr-net-geometric-prototypical-network-for</link>
      <description><![CDATA[In the realm of 3D-computer vision applications, point cloud few-shot learning plays a critical role.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gpr-net-geometric-prototypical-network-for</guid>
    </item>
    <item>
      <title>Neural Attention Forests: Transformer-Based Forest Improvement</title>
      <link>https://paperswithcode.com/paper/neural-attention-forests-transformer-based</link>
      <description><![CDATA[The main idea behind the proposed NAF model is to introduce the attention mechanism into the random forest by assigning attention weights calculated by neural networks of a specific form to data in leaves of decision trees and to the random forest itself in the framework of the Nadaraya-Watson kernel regression.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neural-attention-forests-transformer-based</guid>
    </item>
    <item>
      <title>Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems</title>
      <link>https://paperswithcode.com/paper/maximum-likelihood-estimators-in-physics</link>
      <description><![CDATA[Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/maximum-likelihood-estimators-in-physics</guid>
    </item>
    <item>
      <title>OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for Real-time 3D Hand Gesture Classification and Segmentation</title>
      <link>https://paperswithcode.com/paper/oo-dmvmt-a-deep-multi-view-multi-task</link>
      <description><![CDATA[Continuous mid-air hand gesture recognition based on captured hand pose streams is fundamental for human-computer interaction, particularly in AR / VR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/oo-dmvmt-a-deep-multi-view-multi-task</guid>
    </item>
    <item>
      <title>Unifying and Personalizing Weakly-supervised Federated Medical Image Segmentation via Adaptive Representation and Aggregation</title>
      <link>https://paperswithcode.com/paper/unifying-and-personalizing-weakly-supervised</link>
      <description><![CDATA[The statistical heterogeneity (e. g., non-IID data and domain shifts) is a primary obstacle in FL, impairing the generalization performance of the global model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unifying-and-personalizing-weakly-supervised</guid>
    </item>
    <item>
      <title>Instance-Aware Domain Generalization for Face Anti-Spoofing</title>
      <link>https://paperswithcode.com/paper/instance-aware-domain-generalization-for-face</link>
      <description><![CDATA[To address these issues, we propose a novel perspective for DG FAS that aligns features on the instance level without the need for domain labels.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/instance-aware-domain-generalization-for-face</guid>
    </item>
    <item>
      <title>SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM</title>
      <link>https://paperswithcode.com/paper/samm-segment-any-medical-model-a-3d-slicer</link>
      <description><![CDATA[To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/samm-segment-any-medical-model-a-3d-slicer</guid>
    </item>
    <item>
      <title>Towards Understanding How Data Augmentation Works with Imbalanced Data</title>
      <link>https://paperswithcode.com/paper/towards-understanding-how-data-augmentation</link>
      <description><![CDATA[Data augmentation forms the cornerstone of many modern machine learning training pipelines; yet, the mechanisms by which it works are not clearly understood.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-understanding-how-data-augmentation</guid>
    </item>
    <item>
      <title>Unicom: Universal and Compact Representation Learning for Image Retrieval</title>
      <link>https://paperswithcode.com/paper/unicom-universal-and-compact-representation</link>
      <description><![CDATA[To further enhance the low-dimensional feature representation, we randomly select partial feature dimensions when calculating the similarities between embeddings and class-wise prototypes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unicom-universal-and-compact-representation</guid>
    </item>
    <item>
      <title>Rail Detection: An Efficient Row-based Network and A New Benchmark</title>
      <link>https://paperswithcode.com/paper/rail-detection-an-efficient-row-based-network</link>
      <description><![CDATA[Inspired by the growth of lane detection, we propose a rail database and a row-based rail detection method.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rail-detection-an-efficient-row-based-network</guid>
    </item>
    <item>
      <title>Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation</title>
      <link>https://paperswithcode.com/paper/factorized-inverse-path-tracing-for-efficient</link>
      <description><![CDATA[Inverse path tracing has recently been applied to joint material and lighting estimation, given geometry and multi-view HDR observations of an indoor scene.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/factorized-inverse-path-tracing-for-efficient</guid>
    </item>
    <item>
      <title>ALADIN-NST: Self-supervised disentangled representation learning of artistic style through Neural Style Transfer</title>
      <link>https://paperswithcode.com/paper/aladin-nst-self-supervised-disentangled</link>
      <description><![CDATA[Representation learning aims to discover individual salient features of a domain in a compact and descriptive form that strongly identifies the unique characteristics of a given sample respective to its domain.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/aladin-nst-self-supervised-disentangled</guid>
    </item>
    <item>
      <title>Semantic-Aware Mixup for Domain Generalization</title>
      <link>https://paperswithcode.com/paper/semantic-aware-mixup-for-domain</link>
      <description><![CDATA[To mitigate the hard-fitting issue, we propose to perform a semantic-aware mixup (SAM) for domain generalization, where whether to perform mixup depends on the semantic and domain information.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-aware-mixup-for-domain</guid>
    </item>
    <item>
      <title>An Image Quality Assessment Dataset for Portraits</title>
      <link>https://paperswithcode.com/paper/an-image-quality-assessment-dataset-for</link>
      <description><![CDATA[This costly procedure can be partially replaced by automated learning-based methods for image quality assessment (IQA).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-image-quality-assessment-dataset-for</guid>
    </item>
    <item>
      <title>Scale-Equivariant Deep Learning for 3D Data</title>
      <link>https://paperswithcode.com/paper/scale-equivariant-deep-learning-for-3d-data</link>
      <description><![CDATA[The ability of convolutional neural networks (CNNs) to recognize objects regardless of their position in the image is due to the translation-equivariance of the convolutional operation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scale-equivariant-deep-learning-for-3d-data</guid>
    </item>
    <item>
      <title>Are Local Features All You Need for Cross-Domain Visual Place Recognition?</title>
      <link>https://paperswithcode.com/paper/are-local-features-all-you-need-for-cross</link>
      <description><![CDATA[Despite recent advances, recognizing the same place when the query comes from a significantly different distribution is still a major hurdle for state of the art retrieval methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/are-local-features-all-you-need-for-cross</guid>
    </item>
    <item>
      <title>Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL</title>
      <link>https://paperswithcode.com/paper/representation-learning-with-multi-step</link>
      <description><![CDATA[We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/representation-learning-with-multi-step</guid>
    </item>
    <item>
      <title>DynamicDet: A Unified Dynamic Architecture for Object Detection</title>
      <link>https://paperswithcode.com/paper/dynamicdet-a-unified-dynamic-architecture-for</link>
      <description><![CDATA[We also present a novel optimization strategy with an exiting criterion based on the detection losses for our dynamic detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dynamicdet-a-unified-dynamic-architecture-for</guid>
    </item>
    <item>
      <title>ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/imagereward-learning-and-evaluating-human</link>
      <description><![CDATA[We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/imagereward-learning-and-evaluating-human</guid>
    </item>
    <item>
      <title>Hard Patches Mining for Masked Image Modeling</title>
      <link>https://paperswithcode.com/paper/hard-patches-mining-for-masked-image-modeling</link>
      <description><![CDATA[We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hard-patches-mining-for-masked-image-modeling</guid>
    </item>
    <item>
      <title>CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks</title>
      <link>https://paperswithcode.com/paper/clip-surgery-for-better-explainability-with</link>
      <description><![CDATA[Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal large vision model that has demonstrated significant benefits for downstream tasks, including many zero-shot learning and text-guided vision tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/clip-surgery-for-better-explainability-with</guid>
    </item>
    <item>
      <title>Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series</title>
      <link>https://paperswithcode.com/paper/proximity-forest-2-0-a-new-effective-and</link>
      <description><![CDATA[Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/proximity-forest-2-0-a-new-effective-and</guid>
    </item>
    <item>
      <title>InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions</title>
      <link>https://paperswithcode.com/paper/intergen-diffusion-based-multi-human-motion</link>
      <description><![CDATA[Then, we propose a novel representation for motion input in our interaction diffusion model, which explicitly formulates the global relations between the two performers in the world frame.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/intergen-diffusion-based-multi-human-motion</guid>
    </item>
    <item>
      <title>Precise localization of corneal reflections in eye images using deep learning trained on synthetic data</title>
      <link>https://paperswithcode.com/paper/precise-localization-of-corneal-reflections</link>
      <description><![CDATA[Our method outperformed state-of-the-art algorithmic methods on real eye images with a 35% reduction in terms of spatial precision, and performed on par with state-of-the-art on simulated images in terms of spatial accuracy. We conclude that our method provides a precise method for CR center localization and provides a solution to the data availability problem which is one of the important common roadblocks in the development of deep learning models for gaze estimation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/precise-localization-of-corneal-reflections</guid>
    </item>
    <item>
      <title>Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results</title>
      <link>https://paperswithcode.com/paper/wild-face-anti-spoofing-challenge-2023</link>
      <description><![CDATA[Leveraging the WFAS dataset and Protocol 1 (Known-Type), we host the Wild Face Anti-Spoofing Challenge at the CVPR2023 workshop.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/wild-face-anti-spoofing-challenge-2023</guid>
    </item>
    <item>
      <title>Best Practices for 2-Body Pose Forecasting</title>
      <link>https://paperswithcode.com/paper/best-practices-for-2-body-pose-forecasting</link>
      <description><![CDATA[The task of collaborative human pose forecasting stands for predicting the future poses of multiple interacting people, given those in previous frames.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/best-practices-for-2-body-pose-forecasting</guid>
    </item>
    <item>
      <title>Localizing Model Behavior with Path Patching</title>
      <link>https://paperswithcode.com/paper/localizing-model-behavior-with-path-patching</link>
      <description><![CDATA[Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/localizing-model-behavior-with-path-patching</guid>
    </item>
    <item>
      <title>Boosted Prompt Ensembles for Large Language Models</title>
      <link>https://paperswithcode.com/paper/boosted-prompt-ensembles-for-large-language</link>
      <description><![CDATA[Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/boosted-prompt-ensembles-for-large-language</guid>
    </item>
    <item>
      <title>FALQU: Finding Answers to Legal Questions</title>
      <link>https://paperswithcode.com/paper/falqu-finding-answers-to-legal-questions</link>
      <description><![CDATA[This paper presents a new test collection for Legal IR, FALQU: Finding Answers to Legal Questions, where questions and answers were obtained from Law Stack Exchange (LawSE), a Q&A website for legal professionals, and others with experience in law.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/falqu-finding-answers-to-legal-questions</guid>
    </item>
    <item>
      <title>Diffusion Recommender Model</title>
      <link>https://paperswithcode.com/paper/diffusion-recommender-model</link>
      <description><![CDATA[In light of the impressive advantages of Diffusion Models (DMs) over traditional generative models in image synthesis, we propose a novel Diffusion Recommender Model (named DiffRec) to learn the generative process in a denoising manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-recommender-model</guid>
    </item>
    <item>
      <title>Bayesian Optimization of Catalysts With In-context Learning</title>
      <link>https://paperswithcode.com/paper/bayesian-optimization-of-catalysts-with-in</link>
      <description><![CDATA[We show a prompting system that enables regression with uncertainty for in-context learning with frozen LLM (GPT-3, GPT-3. 5, and GPT-4) models, allowing predictions without features or architecture tuning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bayesian-optimization-of-catalysts-with-in</guid>
    </item>
    <item>
      <title>Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation</title>
      <link>https://paperswithcode.com/paper/transfer-learning-across-heterogeneous</link>
      <description><![CDATA[Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/transfer-learning-across-heterogeneous</guid>
    </item>
    <item>
      <title>Towards preserving word order importance through Forced Invalidation</title>
      <link>https://paperswithcode.com/paper/towards-preserving-word-order-importance</link>
      <description><![CDATA[Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-preserving-word-order-importance</guid>
    </item>
    <item>
      <title>Topology Reasoning for Driving Scenes</title>
      <link>https://paperswithcode.com/paper/topology-reasoning-for-driving-scenes</link>
      <description><![CDATA[Understanding the road genome is essential to realize autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/topology-reasoning-for-driving-scenes</guid>
    </item>
    <item>
      <title>Fracture Detection in Pediatric Wrist Trauma X-ray Images Using YOLOv8 Algorithm</title>
      <link>https://paperswithcode.com/paper/fracture-detection-in-pediatric-wrist-trauma</link>
      <description><![CDATA[In this paper, YOLOv8 algorithm is used to train models on the GRAZPEDWRI-DX dataset, which includes X-ray images from 6, 091 pediatric patients with wrist trauma.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fracture-detection-in-pediatric-wrist-trauma</guid>
    </item>
    <item>
      <title>Cooperative Coevolution for Non-Separable Large-Scale Black-Box Optimization: Convergence Analyses and Distributed Accelerations</title>
      <link>https://paperswithcode.com/paper/cooperative-coevolution-for-non-separable</link>
      <description><![CDATA[Given the ubiquity of non-separable optimization problems in real worlds, in this paper we analyze and extend the large-scale version of the well-known cooperative coevolution (CC), a divide-and-conquer optimization framework, on non-separable functions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cooperative-coevolution-for-non-separable</guid>
    </item>
    <item>
      <title>DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification</title>
      <link>https://paperswithcode.com/paper/disthd-a-learner-aware-dynamic-encoding</link>
      <description><![CDATA[Brain-inspired hyperdimensional computing (HDC) has been recently considered a promising learning approach for resource-constrained devices.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/disthd-a-learner-aware-dynamic-encoding</guid>
    </item>
    <item>
      <title>SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge Distillation in Magnetic Resonance Image Reconstruction</title>
      <link>https://paperswithcode.com/paper/sft-kd-recon-learning-a-student-friendly</link>
      <description><![CDATA[We propose SFT-KD-Recon, a student-friendly teacher training approach along with the student as a prior step to KD to make the teacher aware of the structure and capacity of the student and enable aligning the representations of the teacher with the student.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sft-kd-recon-learning-a-student-friendly</guid>
    </item>
    <item>
      <title>Controllable Textual Inversion for Personalized Text-to-Image Generation</title>
      <link>https://paperswithcode.com/paper/controllable-textual-inversion-for</link>
      <description><![CDATA[The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/controllable-textual-inversion-for</guid>
    </item>
    <item>
      <title>Efficient Feature Description for Small Body Relative Navigation using Binary Convolutional Neural Networks</title>
      <link>https://paperswithcode.com/paper/efficient-feature-description-for-small-body</link>
      <description><![CDATA[We train and test our models on real images of small bodies from legacy and ongoing missions and demonstrate increased performance relative to traditional handcrafted methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-feature-description-for-small-body</guid>
    </item>
    <item>
      <title>Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies</title>
      <link>https://paperswithcode.com/paper/pinpointing-why-object-recognition</link>
      <description><![CDATA[As an example, we show that mitigating a model's vulnerability to texture can improve performance on the lower income level.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pinpointing-why-object-recognition</guid>
    </item>
    <item>
      <title>Teaching Large Language Models to Self-Debug</title>
      <link>https://paperswithcode.com/paper/teaching-large-language-models-to-self-debug</link>
      <description><![CDATA[In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i. e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/teaching-large-language-models-to-self-debug</guid>
    </item>
    <item>
      <title>CamDiff: Camouflage Image Augmentation via Diffusion Model</title>
      <link>https://paperswithcode.com/paper/camdiff-camouflage-image-augmentation-via</link>
      <description><![CDATA[Specifically, we leverage the latent diffusion model to synthesize salient objects in camouflaged scenes, while using the zero-shot image classification ability of the Contrastive Language-Image Pre-training (CLIP) model to prevent synthesis failures and ensure the synthesized object aligns with the input prompt.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/camdiff-camouflage-image-augmentation-via</guid>
    </item>
    <item>
      <title>FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training</title>
      <link>https://paperswithcode.com/paper/fashionsap-symbols-and-attributes-prompt-for</link>
      <description><![CDATA[We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fashionsap-symbols-and-attributes-prompt-for</guid>
    </item>
    <item>
      <title>Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing</title>
      <link>https://paperswithcode.com/paper/survey-on-leveraging-uncertainty-estimation</link>
      <description><![CDATA[In this paper, we present a systematic review of the prediction with the reject option in the context of various neural networks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/survey-on-leveraging-uncertainty-estimation</guid>
    </item>
    <item>
      <title>Another Vertical View: A Hierarchical Network for Heterogeneous Trajectory Prediction via Spectrums</title>
      <link>https://paperswithcode.com/paper/another-vertical-view-a-hierarchical-network</link>
      <description><![CDATA[In this paper, we bring a new ``view'' for trajectory prediction to model and forecast trajectories hierarchically according to different frequency portions from the spectral domain to learn to forecast trajectories by considering their frequency responses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/another-vertical-view-a-hierarchical-network</guid>
    </item>
    <item>
      <title>NeAT: Neural Artistic Tracing for Beautiful Style Transfer</title>
      <link>https://paperswithcode.com/paper/neat-neural-artistic-tracing-for-beautiful</link>
      <description><![CDATA[As a component of curating this data, we present a novel model able to classify if an image is stylistic.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/neat-neural-artistic-tracing-for-beautiful</guid>
    </item>
    <item>
      <title>Pixel-wise Guidance for Utilizing Auxiliary Features in Monte Carlo Denoising</title>
      <link>https://paperswithcode.com/paper/pixel-wise-guidance-for-utilizing-auxiliary</link>
      <description><![CDATA[Then we design our ensembling network to obtain per-pixel ensembling weight maps, which represent pixel-wise guidance for which auxiliary feature should be dominant at reconstructing each individual pixel and use them to ensemble the two denoised results of our denosiers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pixel-wise-guidance-for-utilizing-auxiliary</guid>
    </item>
  </channel>
</rss>
