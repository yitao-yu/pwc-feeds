<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 23 May 2025 09:19:19 +0000</lastBuildDate>
    <item>
      <title>PromptTAD: Object-Prompt Enhanced Traffic Anomaly Detection</title>
      <link>https://paperswithcode.com/paper/prompttad-object-prompt-enhanced-traffic</link>
      <description><![CDATA[Previous frame-level methods are often vulnerable to interference from these dynamic backgrounds and struggle to detect small objects located at a distance or off-center.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompttad-object-prompt-enhanced-traffic</guid>
    </item>
    <item>
      <title>HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora</title>
      <link>https://paperswithcode.com/paper/hopweaver-synthesizing-authentic-multi-hop</link>
      <description><![CDATA[Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's capability to integrate information from diverse sources.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hopweaver-synthesizing-authentic-multi-hop</guid>
    </item>
    <item>
      <title>How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study</title>
      <link>https://paperswithcode.com/paper/how-should-we-enhance-the-safety-of-large</link>
      <description><![CDATA[In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-should-we-enhance-the-safety-of-large</guid>
    </item>
    <item>
      <title>SoftHGNN: Soft Hypergraph Neural Networks for General Visual Recognition</title>
      <link>https://paperswithcode.com/paper/softhgnn-soft-hypergraph-neural-networks-for</link>
      <description><![CDATA[However, existing hypergraph neural networks typically rely on static and hard hyperedge assignments, leading to excessive and redundant hyperedges with hard binary vertex memberships that overlook the continuity of visual semantics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/softhgnn-soft-hypergraph-neural-networks-for</guid>
    </item>
    <item>
      <title>LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing</title>
      <link>https://paperswithcode.com/paper/lyaplock-bounded-knowledge-preservation-in</link>
      <description><![CDATA[To tackle this, we model the sequential editing as a constrained stochastic programming.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lyaplock-bounded-knowledge-preservation-in</guid>
    </item>
    <item>
      <title>Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use</title>
      <link>https://paperswithcode.com/paper/exploring-llm-generated-feedback-for</link>
      <description><![CDATA[For a randomly selected set of essays that they had graded, we used our feedback engine to generate feedback and displayed the feedback as in-text comments in a Word document.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-llm-generated-feedback-for</guid>
    </item>
    <item>
      <title>Know When to Abstain: Optimal Selective Classification with Likelihood Ratios</title>
      <link>https://paperswithcode.com/paper/know-when-to-abstain-optimal-selective</link>
      <description><![CDATA[Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/know-when-to-abstain-optimal-selective</guid>
    </item>
    <item>
      <title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
      <link>https://paperswithcode.com/paper/traveling-across-languages-benchmarking-cross</link>
      <description><![CDATA[The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/traveling-across-languages-benchmarking-cross</guid>
    </item>
    <item>
      <title>Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features</title>
      <link>https://paperswithcode.com/paper/robust-multi-modal-forecasting-integrating</link>
      <description><![CDATA[Recent work has explored a top-down approach to bi-level transparency, focusing on understanding trends and properties of predicted time series using static features.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-multi-modal-forecasting-integrating</guid>
    </item>
    <item>
      <title>Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space</title>
      <link>https://paperswithcode.com/paper/soft-thinking-unlocking-the-reasoning</link>
      <description><![CDATA[In this work, we introduce Soft Thinking, a training-free method that emulates human-like "soft" reasoning by generating soft, abstract concept tokens in a continuous concept space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/soft-thinking-unlocking-the-reasoning</guid>
    </item>
    <item>
      <title>Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models</title>
      <link>https://paperswithcode.com/paper/audio-jailbreak-an-open-comprehensive</link>
      <description><![CDATA[To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/audio-jailbreak-an-open-comprehensive</guid>
    </item>
    <item>
      <title>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping</title>
      <link>https://paperswithcode.com/paper/learn-to-reason-efficiently-with-adaptive</link>
      <description><![CDATA[Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learn-to-reason-efficiently-with-adaptive</guid>
    </item>
    <item>
      <title>From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning</title>
      <link>https://paperswithcode.com/paper/from-problem-solving-to-teaching-problem</link>
      <description><![CDATA[Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/from-problem-solving-to-teaching-problem</guid>
    </item>
    <item>
      <title>An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents</title>
      <link>https://paperswithcode.com/paper/an-empirical-study-on-reinforcement-learning</link>
      <description><![CDATA[Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/an-empirical-study-on-reinforcement-learning</guid>
    </item>
    <item>
      <title>UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset</title>
      <link>https://paperswithcode.com/paper/uwsam-segment-anything-model-guided</link>
      <description><![CDATA[With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/uwsam-segment-anything-model-guided</guid>
    </item>
    <item>
      <title>DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer</title>
      <link>https://paperswithcode.com/paper/deepkd-a-deeply-decoupled-and-denoised</link>
      <description><![CDATA[First, through theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics in task-oriented and non-task-oriented knowledge distillation, we design independent momentum updaters for each component to prevent mutual interference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepkd-a-deeply-decoupled-and-denoised</guid>
    </item>
    <item>
      <title>lmgame-Bench: How Good are LLMs at Playing Games?</title>
      <link>https://paperswithcode.com/paper/lmgame-bench-how-good-are-llms-at-playing</link>
      <description><![CDATA[Playing video games requires perception, memory, and planning, exactly the faculties modern large language model (LLM) agents are expected to master.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lmgame-bench-how-good-are-llms-at-playing</guid>
    </item>
    <item>
      <title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
      <link>https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</link>
      <description><![CDATA[Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/r-d-agent-quant-a-multi-agent-framework-for</guid>
    </item>
    <item>
      <title>Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework</title>
      <link>https://paperswithcode.com/paper/towards-explainable-temporal-reasoning-in</link>
      <description><![CDATA[To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-explainable-temporal-reasoning-in</guid>
    </item>
    <item>
      <title>GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents</title>
      <link>https://paperswithcode.com/paper/gui-g1-understanding-r1-zero-like-training</link>
      <description><![CDATA[In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gui-g1-understanding-r1-zero-like-training</guid>
    </item>
    <item>
      <title>PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration</title>
      <link>https://paperswithcode.com/paper/piflow-principle-aware-scientific-discovery</link>
      <description><![CDATA[Overall, \texttt{PiFlow} serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/piflow-principle-aware-scientific-discovery</guid>
    </item>
    <item>
      <title>SNAP: A Benchmark for Testing the Effects of Capture Conditions on Fundamental Vision Tasks</title>
      <link>https://paperswithcode.com/paper/snap-a-benchmark-for-testing-the-effects-of</link>
      <description><![CDATA[Lastly, we conduct an experiment to establish a human baseline for the VQA task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/snap-a-benchmark-for-testing-the-effects-of</guid>
    </item>
    <item>
      <title>Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory</title>
      <link>https://paperswithcode.com/paper/lost-in-benchmarks-rethinking-large-language</link>
      <description><![CDATA[The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lost-in-benchmarks-rethinking-large-language</guid>
    </item>
    <item>
      <title>Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking</title>
      <link>https://paperswithcode.com/paper/oral-imaging-for-malocclusion-issues</link>
      <description><![CDATA[Malocclusion is a major challenge in orthodontics, and its complex presentation and diverse clinical manifestations make accurate localization and diagnosis particularly important.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/oral-imaging-for-malocclusion-issues</guid>
    </item>
    <item>
      <title>Stronger ViTs With Octic Equivariance</title>
      <link>https://paperswithcode.com/paper/stronger-vits-with-octic-equivariance</link>
      <description><![CDATA[Recent efforts at scaling computer vision models have established Vision Transformers (ViTs) as the leading architecture.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/stronger-vits-with-octic-equivariance</guid>
    </item>
    <item>
      <title>ThinkRec: Thinking-based recommendation via LLM</title>
      <link>https://paperswithcode.com/paper/thinkrec-thinking-based-recommendation-via</link>
      <description><![CDATA[Motivated by this, we propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1 to System 2 (rational system).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/thinkrec-thinking-based-recommendation-via</guid>
    </item>
    <item>
      <title>Multi-Hop Question Generation via Dual-Perspective Keyword Guidance</title>
      <link>https://paperswithcode.com/paper/multi-hop-question-generation-via-dual</link>
      <description><![CDATA[Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-hop-question-generation-via-dual</guid>
    </item>
    <item>
      <title>X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography</title>
      <link>https://paperswithcode.com/paper/x-grm-large-gaussian-reconstruction-model-for</link>
      <description><![CDATA[Existing CT reconstruction works are limited to small-capacity model architecture, inflexible volume representation, and small-scale training data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/x-grm-large-gaussian-reconstruction-model-for</guid>
    </item>
    <item>
      <title>UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking</title>
      <link>https://paperswithcode.com/paper/urdufactcheck-an-agentic-fact-checking</link>
      <description><![CDATA[The rapid use of large language models (LLMs) has raised critical concerns regarding the factual reliability of their outputs, especially in low-resource languages such as Urdu.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/urdufactcheck-an-agentic-fact-checking</guid>
    </item>
    <item>
      <title>MMaDA: Multimodal Large Diffusion Language Models</title>
      <link>https://paperswithcode.com/paper/mmada-multimodal-large-diffusion-language</link>
      <description><![CDATA[We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mmada-multimodal-large-diffusion-language</guid>
    </item>
    <item>
      <title>HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases</title>
      <link>https://paperswithcode.com/paper/hdlxgraph-bridging-large-language-models-and</link>
      <description><![CDATA[Large Language Models (LLMs) have demonstrated their potential in hardware design tasks, such as Hardware Description Language (HDL) generation and debugging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hdlxgraph-bridging-large-language-models-and</guid>
    </item>
    <item>
      <title>CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation</title>
      <link>https://paperswithcode.com/paper/cinetechbench-a-benchmark-for-cinematographic</link>
      <description><![CDATA[For the generation task, we assess advanced video generation models on their capacity to reconstruct cinema-quality camera movements given conditions such as textual prompts or keyframes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cinetechbench-a-benchmark-for-cinematographic</guid>
    </item>
    <item>
      <title>RLBenchNet: The Right Network for the Right Reinforcement Learning Task</title>
      <link>https://paperswithcode.com/paper/rlbenchnet-the-right-network-for-the-right</link>
      <description><![CDATA[Reinforcement learning (RL) has seen significant advancements through the application of various neural network architectures.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/rlbenchnet-the-right-network-for-the-right</guid>
    </item>
    <item>
      <title>AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection</title>
      <link>https://paperswithcode.com/paper/auxdet-auxiliary-metadata-matters-for-omni</link>
      <description><![CDATA[In this work, we reveal a critical oversight in existing paradigms: the neglect of readily available auxiliary metadata describing imaging parameters and acquisition conditions, such as spectral bands, sensor platforms, resolution, and observation perspectives.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/auxdet-auxiliary-metadata-matters-for-omni</guid>
    </item>
    <item>
      <title>Efficient Differentiable Approximation of Generalized Low-rank Regularization</title>
      <link>https://paperswithcode.com/paper/efficient-differentiable-approximation-of</link>
      <description><![CDATA[To overcome this difficulty, various relaxations of the rank function were studied.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-differentiable-approximation-of</guid>
    </item>
    <item>
      <title>The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization</title>
      <link>https://paperswithcode.com/paper/the-p-3-dataset-pixels-points-and-polygons</link>
      <description><![CDATA[We present the P$^3$ dataset, a large-scale multimodal benchmark for building vectorization, constructed from aerial LiDAR point clouds, high-resolution aerial imagery, and vectorized 2D building outlines, collected across three continents.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-p-3-dataset-pixels-points-and-polygons</guid>
    </item>
    <item>
      <title>MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models</title>
      <link>https://paperswithcode.com/paper/monosplat-generalizable-3d-gaussian-splatting</link>
      <description><![CDATA[Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/monosplat-generalizable-3d-gaussian-splatting</guid>
    </item>
    <item>
      <title>Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs</title>
      <link>https://paperswithcode.com/paper/deliberation-on-priors-trustworthy-reasoning</link>
      <description><![CDATA[Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deliberation-on-priors-trustworthy-reasoning</guid>
    </item>
    <item>
      <title>Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection</title>
      <link>https://paperswithcode.com/paper/multimodal-conditional-information-bottleneck</link>
      <description><![CDATA[In this paper, we propose a multimodal conditional bottleneck network to reduce feature redundancy while enhancing the discriminative power of features extracted by CLIP, thereby improving the model's generalization ability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multimodal-conditional-information-bottleneck</guid>
    </item>
    <item>
      <title>MIRB: Mathematical Information Retrieval Benchmark</title>
      <link>https://paperswithcode.com/paper/mirb-mathematical-information-retrieval</link>
      <description><![CDATA[In this paper, we introduce MIRB (Mathematical Information Retrieval Benchmark) to assess the MIR capabilities of retrieval models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mirb-mathematical-information-retrieval</guid>
    </item>
    <item>
      <title>Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives</title>
      <link>https://paperswithcode.com/paper/continuous-representation-methods-theories</link>
      <description><![CDATA[Recently, continuous representation methods emerge as novel paradigms that characterize the intrinsic structures of real-world data through function representations that map positional coordinates to their corresponding values in the continuous space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/continuous-representation-methods-theories</guid>
    </item>
    <item>
      <title>Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/visual-perturbation-and-adaptive-hard</link>
      <description><![CDATA[However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-perturbation-and-adaptive-hard</guid>
    </item>
    <item>
      <title>DC-Scene: Data-Centric Learning for 3D Scene Understanding</title>
      <link>https://paperswithcode.com/paper/dc-scene-data-centric-learning-for-3d-scene</link>
      <description><![CDATA[3D scene understanding plays a fundamental role in vision applications such as robotics, autonomous driving, and augmented reality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dc-scene-data-centric-learning-for-3d-scene</guid>
    </item>
    <item>
      <title>Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images</title>
      <link>https://paperswithcode.com/paper/lung-nodule-ssm-self-supervised-lung-nodule</link>
      <description><![CDATA[The limited availability of annotated medical imaging data remains a bottleneck in developing accurate computer-aided diagnosis (CAD) systems.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lung-nodule-ssm-self-supervised-lung-nodule</guid>
    </item>
    <item>
      <title>Streamline Without Sacrifice -- Squeeze out Computation Redundancy in LMM</title>
      <link>https://paperswithcode.com/paper/streamline-without-sacrifice-squeeze-out</link>
      <description><![CDATA[Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/streamline-without-sacrifice-squeeze-out</guid>
    </item>
    <item>
      <title>Exploring In-Image Machine Translation with Real-World Background</title>
      <link>https://paperswithcode.com/paper/exploring-in-image-machine-translation-with</link>
      <description><![CDATA[Previous research on IIMT was primarily conducted on simplified scenarios such as images of one-line text with black font in white backgrounds, which is far from reality and impractical for applications in the real world.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-in-image-machine-translation-with</guid>
    </item>
    <item>
      <title>Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts</title>
      <link>https://paperswithcode.com/paper/prompt-tuning-vision-language-models-with</link>
      <description><![CDATA[PromptMargin effectively tunes the text as well as visual prompts for this task, and has two main modules: 1) Firstly, we use a selective augmentation strategy to complement the few training samples in each task; 2) Additionally, to ensure robust training in the presence of unfamiliar class names, we increase the inter-class margin for improved class discrimination using a novel Multimodal Margin Regularizer.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/prompt-tuning-vision-language-models-with</guid>
    </item>
    <item>
      <title>Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization</title>
      <link>https://paperswithcode.com/paper/leveraging-the-powerful-attention-of-a-pre</link>
      <description><![CDATA[Specifically, we use 335 input-reference pairs from previous research, achieving an FID of 95. 27 (image quality) and an SI-FID of 5. 51 (fidelity to the reference).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leveraging-the-powerful-attention-of-a-pre</guid>
    </item>
    <item>
      <title>seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/seg-3d-by-pc2d-multi-view-projection-for</link>
      <description><![CDATA[We evaluate on the nuScenes and SemanticKITTI datasets under both the DG and UDA settings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seg-3d-by-pc2d-multi-view-projection-for</guid>
    </item>
    <item>
      <title>Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!</title>
      <link>https://paperswithcode.com/paper/be-careful-when-fine-tuning-on-open-source</link>
      <description><![CDATA[Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/be-careful-when-fine-tuning-on-open-source</guid>
    </item>
  </channel>
</rss>
