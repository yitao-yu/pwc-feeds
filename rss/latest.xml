<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 20 Feb 2024 21:06:26 +0000</lastBuildDate>
    <item>
      <title>Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models</title>
      <link>https://paperswithcode.com/paper/explain-then-rank-scale-calibration-of-neural</link>
      <description><![CDATA[The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/explain-then-rank-scale-calibration-of-neural</guid>
    </item>
    <item>
      <title>Machine-generated Text Localization</title>
      <link>https://paperswithcode.com/paper/machine-generated-text-localization</link>
      <description><![CDATA[Machine-Generated Text (MGT) detection aims to identify a piece of text as machine or human written.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/machine-generated-text-localization</guid>
    </item>
    <item>
      <title>Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents</title>
      <link>https://paperswithcode.com/paper/shall-we-talk-exploring-spontaneous</link>
      <description><![CDATA[Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/shall-we-talk-exploring-spontaneous</guid>
    </item>
    <item>
      <title>Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing</title>
      <link>https://paperswithcode.com/paper/diagonalisation-sgd-fast-convergent-sgd-for</link>
      <description><![CDATA[It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diagonalisation-sgd-fast-convergent-sgd-for</guid>
    </item>
    <item>
      <title>LoRA+: Efficient Low Rank Adaptation of Large Models</title>
      <link>https://paperswithcode.com/paper/lora-efficient-low-rank-adaptation-of-large</link>
      <description><![CDATA[In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lora-efficient-low-rank-adaptation-of-large</guid>
    </item>
    <item>
      <title>DualView: Data Attribution from the Dual Perspective</title>
      <link>https://paperswithcode.com/paper/dualview-data-attribution-from-the-dual</link>
      <description><![CDATA[In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dualview-data-attribution-from-the-dual</guid>
    </item>
    <item>
      <title>GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations</title>
      <link>https://paperswithcode.com/paper/gtbench-uncovering-the-strategic-reasoning</link>
      <description><![CDATA[As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/gtbench-uncovering-the-strategic-reasoning</guid>
    </item>
    <item>
      <title>Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model</title>
      <link>https://paperswithcode.com/paper/mini-hes-a-parallelizable-second-order-latent</link>
      <description><![CDATA[The performance of an LFA model relies heavily on its training process, which is a non-convex optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mini-hes-a-parallelizable-second-order-latent</guid>
    </item>
    <item>
      <title>Separating common from salient patterns with Contrastive Representation Learning</title>
      <link>https://paperswithcode.com/paper/separating-common-from-salient-patterns-with</link>
      <description><![CDATA[Then, we motivate a novel Mutual Information minimization strategy to prevent information leakage between common and salient distributions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/separating-common-from-salient-patterns-with</guid>
    </item>
    <item>
      <title>Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text</title>
      <link>https://paperswithcode.com/paper/team-qust-at-semeval-2024-task-8-a</link>
      <description><![CDATA[Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/team-qust-at-semeval-2024-task-8-a</guid>
    </item>
    <item>
      <title>Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-as-a-parsimonious</link>
      <description><![CDATA[On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-as-a-parsimonious</guid>
    </item>
    <item>
      <title>Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets</title>
      <link>https://paperswithcode.com/paper/flexible-robust-optimal-bidding-of-renewable</link>
      <description><![CDATA[In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/flexible-robust-optimal-bidding-of-renewable</guid>
    </item>
    <item>
      <title>Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations</title>
      <link>https://paperswithcode.com/paper/diffusion-tempering-improves-parameter</link>
      <description><![CDATA[Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/diffusion-tempering-improves-parameter</guid>
    </item>
    <item>
      <title>High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models</title>
      <link>https://paperswithcode.com/paper/high-quality-data-to-text-generation-for</link>
      <description><![CDATA[The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/high-quality-data-to-text-generation-for</guid>
    </item>
    <item>
      <title>A Critical Evaluation of AI Feedback for Aligning Large Language Models</title>
      <link>https://paperswithcode.com/paper/a-critical-evaluation-of-ai-feedback-for</link>
      <description><![CDATA[RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-critical-evaluation-of-ai-feedback-for</guid>
    </item>
    <item>
      <title>Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric</title>
      <link>https://paperswithcode.com/paper/semantic-textual-similarity-assessment-in</link>
      <description><![CDATA[Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-textual-similarity-assessment-in</guid>
    </item>
    <item>
      <title>FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema</title>
      <link>https://paperswithcode.com/paper/fipo-free-form-instruction-oriented-prompt</link>
      <description><![CDATA[In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fipo-free-form-instruction-oriented-prompt</guid>
    </item>
    <item>
      <title>Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization</title>
      <link>https://paperswithcode.com/paper/towards-explainable-lidar-point-cloud</link>
      <description><![CDATA[Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-explainable-lidar-point-cloud</guid>
    </item>
    <item>
      <title>Modularized Networks for Few-shot Hateful Meme Detection</title>
      <link>https://paperswithcode.com/paper/modularized-networks-for-few-shot-hateful</link>
      <description><![CDATA[We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/modularized-networks-for-few-shot-hateful</guid>
    </item>
    <item>
      <title>TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs</title>
      <link>https://paperswithcode.com/paper/tilp-differentiable-learning-of-temporal</link>
      <description><![CDATA[Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tilp-differentiable-learning-of-temporal</guid>
    </item>
    <item>
      <title>Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling</title>
      <link>https://paperswithcode.com/paper/weakly-supervised-object-detection-in-chest-x</link>
      <description><![CDATA[Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/weakly-supervised-object-detection-in-chest-x</guid>
    </item>
    <item>
      <title>Class-incremental Learning for Time Series: Benchmark and Evaluation</title>
      <link>https://paperswithcode.com/paper/class-incremental-learning-for-time-series</link>
      <description><![CDATA[Real-world environments are inherently non-stationary, frequently introducing new classes over time.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/class-incremental-learning-for-time-series</guid>
    </item>
    <item>
      <title>Dynamic Multi-Network Mining of Tensor Time Series</title>
      <link>https://paperswithcode.com/paper/dynamic-multi-network-mining-of-tensor-time</link>
      <description><![CDATA[(a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dynamic-multi-network-mining-of-tensor-time</guid>
    </item>
    <item>
      <title>Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic</title>
      <link>https://paperswithcode.com/paper/language-models-are-homer-simpson-safety-re</link>
      <description><![CDATA[We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/language-models-are-homer-simpson-safety-re</guid>
    </item>
    <item>
      <title>Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement</title>
      <link>https://paperswithcode.com/paper/meta-ranking-less-capable-language-models-are</link>
      <description><![CDATA[Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/meta-ranking-less-capable-language-models-are</guid>
    </item>
    <item>
      <title>Evaluating Program Repair with Semantic-Preserving Transformations: A Naturalness Assessment</title>
      <link>https://paperswithcode.com/paper/evaluating-program-repair-with-semantic</link>
      <description><![CDATA[In this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of NPR.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/evaluating-program-repair-with-semantic</guid>
    </item>
    <item>
      <title>Reformatted Alignment</title>
      <link>https://paperswithcode.com/paper/reformatted-alignment</link>
      <description><![CDATA[This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reformatted-alignment</guid>
    </item>
    <item>
      <title>FiT: Flexible Vision Transformer for Diffusion Model</title>
      <link>https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion</link>
      <description><![CDATA[Nature is infinitely resolution-free.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fit-flexible-vision-transformer-for-diffusion</guid>
    </item>
    <item>
      <title>CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking</title>
      <link>https://paperswithcode.com/paper/codeart-better-code-models-by-attention</link>
      <description><![CDATA[Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/codeart-better-code-models-by-attention</guid>
    </item>
    <item>
      <title>Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models</title>
      <link>https://paperswithcode.com/paper/scaffolding-coordinates-to-promote-vision</link>
      <description><![CDATA[State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/scaffolding-coordinates-to-promote-vision</guid>
    </item>
    <item>
      <title>Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models</title>
      <link>https://paperswithcode.com/paper/privacy-preserving-low-rank-adaptation-for</link>
      <description><![CDATA[To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/privacy-preserving-low-rank-adaptation-for</guid>
    </item>
    <item>
      <title>Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships</title>
      <link>https://paperswithcode.com/paper/amplifying-training-data-exposure-through</link>
      <description><![CDATA[To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/amplifying-training-data-exposure-through</guid>
    </item>
    <item>
      <title>3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection</title>
      <link>https://paperswithcode.com/paper/3d-vascular-segmentation-supervised-by-2d</link>
      <description><![CDATA[To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/3d-vascular-segmentation-supervised-by-2d</guid>
    </item>
    <item>
      <title>Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations</title>
      <link>https://paperswithcode.com/paper/compress-to-impress-unleashing-the-potential</link>
      <description><![CDATA[Existing retrieval-based methods have made significant strides in maintaining long-term conversations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/compress-to-impress-unleashing-the-potential</guid>
    </item>
    <item>
      <title>Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers</title>
      <link>https://paperswithcode.com/paper/empirical-study-on-updating-key-value</link>
      <description><![CDATA[The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/empirical-study-on-updating-key-value</guid>
    </item>
    <item>
      <title>How Interpretable are Reasoning Explanations from Prompting Large Language Models?</title>
      <link>https://paperswithcode.com/paper/how-interpretable-are-reasoning-explanations</link>
      <description><![CDATA[We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/how-interpretable-are-reasoning-explanations</guid>
    </item>
    <item>
      <title>Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks</title>
      <link>https://paperswithcode.com/paper/hebbian-learning-based-orthogonal-projection</link>
      <description><![CDATA[Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/hebbian-learning-based-orthogonal-projection</guid>
    </item>
    <item>
      <title>Multi-View Conformal Learning for Heterogeneous Sensor Fusion</title>
      <link>https://paperswithcode.com/paper/multi-view-conformal-learning-for</link>
      <description><![CDATA[Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-view-conformal-learning-for</guid>
    </item>
    <item>
      <title>On the Byzantine-Resilience of Distillation-Based Federated Learning</title>
      <link>https://paperswithcode.com/paper/on-the-byzantine-resilience-of-distillation</link>
      <description><![CDATA[In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/on-the-byzantine-resilience-of-distillation</guid>
    </item>
    <item>
      <title>ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</title>
      <link>https://paperswithcode.com/paper/chartx-chartvlm-a-versatile-benchmark-and</link>
      <description><![CDATA[Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/chartx-chartvlm-a-versatile-benchmark-and</guid>
    </item>
    <item>
      <title>Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</title>
      <link>https://paperswithcode.com/paper/robust-clip-unsupervised-adversarial-fine</link>
      <description><![CDATA[The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e. g. LLaVA and OpenFlamingo.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/robust-clip-unsupervised-adversarial-fine</guid>
    </item>
    <item>
      <title>Generative Kaleidoscopic Networks</title>
      <link>https://paperswithcode.com/paper/generative-kaleidoscopic-networks</link>
      <description><![CDATA[We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-kaleidoscopic-networks</guid>
    </item>
    <item>
      <title>LVCHAT: Facilitating Long Video Comprehension</title>
      <link>https://paperswithcode.com/paper/lvchat-facilitating-long-video-comprehension</link>
      <description><![CDATA[To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/lvchat-facilitating-long-video-comprehension</guid>
    </item>
    <item>
      <title>Molecule Generation and Optimization for Efficient Fragrance Creation</title>
      <link>https://paperswithcode.com/paper/molecule-generation-and-optimization-for</link>
      <description><![CDATA[This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/molecule-generation-and-optimization-for</guid>
    </item>
    <item>
      <title>Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs</title>
      <link>https://paperswithcode.com/paper/can-llms-reason-with-rules-logic-scaffolding</link>
      <description><![CDATA[Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/can-llms-reason-with-rules-logic-scaffolding</guid>
    </item>
    <item>
      <title>Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI</title>
      <link>https://paperswithcode.com/paper/simplifying-hyperparameter-tuning-in-online</link>
      <description><![CDATA[The `spotRiver` package provides a framework for hyperparameter tuning of OML models.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/simplifying-hyperparameter-tuning-in-online</guid>
    </item>
    <item>
      <title>Opening the black box of language acquisition</title>
      <link>https://paperswithcode.com/paper/opening-the-black-box-of-language-acquisition</link>
      <description><![CDATA[However, it is unclear whether or how these models represent grammatical information from the learned languages.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/opening-the-black-box-of-language-acquisition</guid>
    </item>
    <item>
      <title>PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction</title>
      <link>https://paperswithcode.com/paper/pascl-supervised-contrastive-learning-with</link>
      <description><![CDATA[In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/pascl-supervised-contrastive-learning-with</guid>
    </item>
    <item>
      <title>FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence</title>
      <link>https://paperswithcode.com/paper/factpico-factuality-evaluation-for-plain</link>
      <description><![CDATA[But how factual are these summaries in a high-stakes domain like medicine?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/factpico-factuality-evaluation-for-plain</guid>
    </item>
    <item>
      <title>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</title>
      <link>https://paperswithcode.com/paper/learning-from-failure-integrating-negative</link>
      <description><![CDATA[Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learning-from-failure-integrating-negative</guid>
    </item>
  </channel>
</rss>
