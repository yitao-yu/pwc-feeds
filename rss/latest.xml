<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Papers with Code: Latest (unofficial)</title>
    <link>https://github.com/yitao-yu/pwc-feeds</link>
    <description>As a disclaimer, this is an unofficial feed and has no affiliation with Papers with Code.</description>
    <atom:link href="https://github.com/yitao-yu/pwc-feeds" rel="self"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Fri, 22 Jul 2022 09:14:59 +0000</lastBuildDate>
    <item>
      <title>D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights</title>
      <link>https://paperswithcode.com/paper/d2-tpred-discontinuous-dependency-for</link>
      <description><![CDATA[We present a trajectory prediction approach with respect to traffic lights, D2-TPred, which uses a spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG) to handle the problem of discontinuous dependency in the spatial-temporal space.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/d2-tpred-discontinuous-dependency-for</guid>
    </item>
    <item>
      <title>Reinforcement learning for Energies of the future and carbon neutrality: a Challenge Design</title>
      <link>https://paperswithcode.com/paper/reinforcement-learning-for-energies-of-the</link>
      <description><![CDATA[Current rapid changes in climate increase the urgency to change energy production and consumption management, to reduce carbon and other green-house gas production.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/reinforcement-learning-for-energies-of-the</guid>
    </item>
    <item>
      <title>Mining Relations among Cross-Frame Affinities for Video Semantic Segmentation</title>
      <link>https://paperswithcode.com/paper/mining-relations-among-cross-frame-affinities</link>
      <description><![CDATA[The essence of video semantic segmentation (VSS) is how to leverage temporal information for prediction.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/mining-relations-among-cross-frame-affinities</guid>
    </item>
    <item>
      <title>Generative Multiplane Images: Making a 2D GAN 3D-Aware</title>
      <link>https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</link>
      <description><![CDATA[What is really needed to make an existing 2D GAN 3D-aware?]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/generative-multiplane-images-making-a-2d-gan</guid>
    </item>
    <item>
      <title>TinyViT: Fast Pretraining Distillation for Small Vision Transformers</title>
      <link>https://paperswithcode.com/paper/tinyvit-fast-pretraining-distillation-for</link>
      <description><![CDATA[It achieves a top-1 accuracy of 84. 8% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4. 2 times fewer parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/tinyvit-fast-pretraining-distillation-for</guid>
    </item>
    <item>
      <title>Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives</title>
      <link>https://paperswithcode.com/paper/sobolev-training-for-implicit-neural</link>
      <description><![CDATA[Recently, Implicit Neural Representations (INRs) parameterized by neural networks have emerged as a powerful and promising tool to represent different kinds of signals due to its continuous, differentiable properties, showing superiorities to classical discretized representations.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/sobolev-training-for-implicit-neural</guid>
    </item>
    <item>
      <title>Efficient CNN Architecture Design Guided by Visualization</title>
      <link>https://paperswithcode.com/paper/efficient-cnn-architecture-design-guided-by</link>
      <description><![CDATA[Our VGNetG-1. 0MP achieves 67. 7% top-1 accuracy with 0. 99M parameters and 69. 2% top-1 accuracy with 1. 14M parameters on ImageNet classification dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/efficient-cnn-architecture-design-guided-by</guid>
    </item>
    <item>
      <title>Correspondence Matters for Video Referring Expression Comprehension</title>
      <link>https://paperswithcode.com/paper/correspondence-matters-for-video-referring</link>
      <description><![CDATA[Extensive experiments demonstrate that our DCNet achieves state-of-the-art performance on both video and image REC benchmarks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/correspondence-matters-for-video-referring</guid>
    </item>
    <item>
      <title>Human Trajectory Prediction via Neural Social Physics</title>
      <link>https://paperswithcode.com/paper/human-trajectory-prediction-via-neural-social</link>
      <description><![CDATA[Our new model (Neural Social Physics or NSP) is a deep neural network within which we use an explicit physics model with learnable parameters.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-trajectory-prediction-via-neural-social</guid>
    </item>
    <item>
      <title>SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks</title>
      <link>https://paperswithcode.com/paper/spin-an-empirical-evaluation-on-sharing</link>
      <description><![CDATA[In this paper, we perform an empirical evaluation on methods for sharing parameters in isotropic networks (SPIN).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/spin-an-empirical-evaluation-on-sharing</guid>
    </item>
    <item>
      <title>ProMix: Combating Label Noise via Maximizing Clean Sample Utility</title>
      <link>https://paperswithcode.com/paper/promix-combating-label-noise-via-maximizing</link>
      <description><![CDATA[Combining with the small-loss selection, our method is able to achieve a precision of 99. 27 and a recall of 98. 22 in detecting clean samples on the CIFAR-10N dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/promix-combating-label-noise-via-maximizing</guid>
    </item>
    <item>
      <title>A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing</title>
      <link>https://paperswithcode.com/paper/a-dense-material-segmentation-dataset-for</link>
      <description><![CDATA[A key algorithm for understanding the world is material segmentation, which assigns a label (metal, glass, etc.)]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-dense-material-segmentation-dataset-for</guid>
    </item>
    <item>
      <title>Magic ELF: Image Deraining Meets Association Learning and Transformer</title>
      <link>https://paperswithcode.com/paper/magic-elf-image-deraining-meets-association</link>
      <description><![CDATA[Convolutional neural network (CNN) and Transformer have achieved great success in multimedia applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/magic-elf-image-deraining-meets-association</guid>
    </item>
    <item>
      <title>Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration</title>
      <link>https://paperswithcode.com/paper/weakly-supervised-object-localization-via</link>
      <description><![CDATA[Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/weakly-supervised-object-localization-via</guid>
    </item>
    <item>
      <title>LocVTP: Video-Text Pre-training for Temporal Localization</title>
      <link>https://paperswithcode.com/paper/locvtp-video-text-pre-training-for-temporal</link>
      <description><![CDATA[To further enhance the temporal reasoning ability of the learned feature, we propose a context projection head and a temporal aware contrastive loss to perceive the contextual relationships.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/locvtp-video-text-pre-training-for-temporal</guid>
    </item>
    <item>
      <title>Semi-Supervised Learning of Optical Flow by Flow Supervisor</title>
      <link>https://paperswithcode.com/paper/semi-supervised-learning-of-optical-flow-by</link>
      <description><![CDATA[A training pipeline for optical flow CNNs consists of a pretraining stage on a synthetic dataset followed by a fine tuning stage on a target dataset.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semi-supervised-learning-of-optical-flow-by</guid>
    </item>
    <item>
      <title>CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution</title>
      <link>https://paperswithcode.com/paper/cadyq-content-aware-dynamic-quantization-for</link>
      <description><![CDATA[In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/cadyq-content-aware-dynamic-quantization-for</guid>
    </item>
    <item>
      <title>SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer</title>
      <link>https://paperswithcode.com/paper/seedformer-patch-seeds-based-point-cloud</link>
      <description><![CDATA[Point cloud completion has become increasingly popular among generation tasks of 3D point clouds, as it is a challenging yet indispensable problem to recover the complete shape of a 3D object from its partial observation.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/seedformer-patch-seeds-based-point-cloud</guid>
    </item>
    <item>
      <title>Semantic-Aware Fine-Grained Correspondence</title>
      <link>https://paperswithcode.com/paper/semantic-aware-fine-grained-correspondence</link>
      <description><![CDATA[Establishing visual correspondence across images is a challenging and essential task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/semantic-aware-fine-grained-correspondence</guid>
    </item>
    <item>
      <title>Leveraging Natural Supervision for Language Representation Learning and Generation</title>
      <link>https://paperswithcode.com/paper/leveraging-natural-supervision-for-language</link>
      <description><![CDATA[In this thesis, we describe three lines of work that seek to improve the training and evaluation of neural models using naturally-occurring supervision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/leveraging-natural-supervision-for-language</guid>
    </item>
    <item>
      <title>A Primer on Topological Data Analysis to Support Image Analysis Tasks in Environmental Science</title>
      <link>https://paperswithcode.com/paper/a-primer-on-topological-data-analysis-to</link>
      <description><![CDATA[One of the core strengths of persistent homology is how interpretable it can be, so throughout this paper we discuss not just the patterns we find, but why those results are to be expected given what we know about the theory of persistent homology.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-primer-on-topological-data-analysis-to</guid>
    </item>
    <item>
      <title>KD-MVS: Knowledge Distillation Based Self-supervised Learning for MVS</title>
      <link>https://paperswithcode.com/paper/kd-mvs-knowledge-distillation-based-self</link>
      <description><![CDATA[Supervised multi-view stereo (MVS) methods have achieved remarkable progress in terms of reconstruction quality, but suffer from the challenge of collecting large-scale ground-truth depth.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/kd-mvs-knowledge-distillation-based-self</guid>
    </item>
    <item>
      <title>Designing An Illumination-Aware Network for Deep Image Relighting</title>
      <link>https://paperswithcode.com/paper/designing-an-illumination-aware-network-for</link>
      <description><![CDATA[Lighting is a determining factor in photography that affects the style, expression of emotion, and even quality of images.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/designing-an-illumination-aware-network-for</guid>
    </item>
    <item>
      <title>The Birth of Bias: A case study on the evolution of gender bias in an English language model</title>
      <link>https://paperswithcode.com/paper/the-birth-of-bias-a-case-study-on-the-1</link>
      <description><![CDATA[With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model's internal state relates to the bias in a downstream task (semantic textual similarity).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/the-birth-of-bias-a-case-study-on-the-1</guid>
    </item>
    <item>
      <title>In Defense of Online Models for Video Instance Segmentation</title>
      <link>https://paperswithcode.com/paper/in-defense-of-online-models-for-video</link>
      <description><![CDATA[In recent years, video instance segmentation (VIS) has been largely advanced by offline models, while online models gradually attracted less attention possibly due to their inferior performance.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/in-defense-of-online-models-for-video</guid>
    </item>
    <item>
      <title>Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild</title>
      <link>https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</link>
      <description><![CDATA[Omni3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 97 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/omni3d-a-large-benchmark-and-model-for-3d</guid>
    </item>
    <item>
      <title>Exploring Fine-Grained Audiovisual Categorization with the SSW60 Dataset</title>
      <link>https://paperswithcode.com/paper/exploring-fine-grained-audiovisual</link>
      <description><![CDATA[We thoroughly benchmark audiovisual classification performance and modality fusion experiments through the use of state-of-the-art transformer methods.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/exploring-fine-grained-audiovisual</guid>
    </item>
    <item>
      <title>AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection</title>
      <link>https://paperswithcode.com/paper/autoalignv2-deformable-feature-aggregation</link>
      <description><![CDATA[Recently, AutoAlign presents a learnable paradigm in combining these two modalities for 3D object detection.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/autoalignv2-deformable-feature-aggregation</guid>
    </item>
    <item>
      <title>Boosting 3D Object Detection via Object-Focused Image Fusion</title>
      <link>https://paperswithcode.com/paper/boosting-3d-object-detection-via-object</link>
      <description><![CDATA[Given a set of point features and image feature maps, DeMF adaptively aggregates image features by taking the projected 2D location of the 3D point as reference.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/boosting-3d-object-detection-via-object</guid>
    </item>
    <item>
      <title>Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions</title>
      <link>https://paperswithcode.com/paper/online-domain-adaptation-for-semantic</link>
      <description><![CDATA[Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between training and testing data and is, in most cases, carried out in offline manner.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/online-domain-adaptation-for-semantic</guid>
    </item>
    <item>
      <title>Unsupervised Night Image Enhancement: When Layer Decomposition Meets Light-Effects Suppression</title>
      <link>https://paperswithcode.com/paper/unsupervised-night-image-enhancement-when</link>
      <description><![CDATA[To address this problem, we need to suppress the light effects in bright regions while, at the same time, boosting the intensity of dark regions.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-night-image-enhancement-when</guid>
    </item>
    <item>
      <title>UniFed: A Benchmark for Federated Learning Frameworks</title>
      <link>https://paperswithcode.com/paper/unifed-a-benchmark-for-federated-learning</link>
      <description><![CDATA[Federated Learning (FL) has become a practical and popular paradigm in machine learning.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unifed-a-benchmark-for-federated-learning</guid>
    </item>
    <item>
      <title>Synthesizing Light Field Video from Monocular Video</title>
      <link>https://paperswithcode.com/paper/synthesizing-light-field-video-from-monocular</link>
      <description><![CDATA[Hence, we propose a self-supervised learning-based algorithm for LF video reconstruction from monocular videos.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/synthesizing-light-field-video-from-monocular</guid>
    </item>
    <item>
      <title>Bayesian Recurrent Units and the Forward-Backward Algorithm</title>
      <link>https://paperswithcode.com/paper/bayesian-recurrent-units-and-the-forward</link>
      <description><![CDATA[Using Bayes's theorem, we derive a unit-wise recurrence as well as a backward recursion similar to the forward-backward algorithm.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/bayesian-recurrent-units-and-the-forward</guid>
    </item>
    <item>
      <title>Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis</title>
      <link>https://paperswithcode.com/paper/injecting-3d-perception-of-controllable-nerf</link>
      <description><![CDATA[To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/injecting-3d-perception-of-controllable-nerf</guid>
    </item>
    <item>
      <title>Human-centric Image Cropping with Partition-aware and Content-preserving Features</title>
      <link>https://paperswithcode.com/paper/human-centric-image-cropping-with-partition</link>
      <description><![CDATA[Image cropping aims to find visually appealing crops in an image, which is an important yet challenging task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/human-centric-image-cropping-with-partition</guid>
    </item>
    <item>
      <title>DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta</title>
      <link>https://paperswithcode.com/paper/deltagan-towards-diverse-few-shot-image-1</link>
      <description><![CDATA[In this work, we propose a novel Delta Generative Adversarial Network (DeltaGAN), which consists of a reconstruction subnetwork and a generation subnetwork.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deltagan-towards-diverse-few-shot-image-1</guid>
    </item>
    <item>
      <title>Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context</title>
      <link>https://paperswithcode.com/paper/don-t-forget-me-accurate-background-recovery</link>
      <description><![CDATA[To address this issue, we propose a Contextual-guided Text Removal Network, termed as CTRNet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/don-t-forget-me-accurate-background-recovery</guid>
    </item>
    <item>
      <title>Beyond single receptive field: A receptive field fusion-and-stratification network for airborne laser scanning point cloud classification</title>
      <link>https://paperswithcode.com/paper/beyond-single-receptive-field-a-receptive</link>
      <description><![CDATA[With receptive field fusion-and-stratification, RFFS-Net is more adaptable to the classification of regions with complex structures and extreme scale variations in large-scale ALS point clouds.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/beyond-single-receptive-field-a-receptive</guid>
    </item>
    <item>
      <title>Grounding Visual Representations with Texts for Domain Generalization</title>
      <link>https://paperswithcode.com/paper/grounding-visual-representations-with-texts</link>
      <description><![CDATA[In this work, we advocate for leveraging natural language supervision for the domain generalization task.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/grounding-visual-representations-with-texts</guid>
    </item>
    <item>
      <title>Learn From All: Erasing Attention Consistency for Noisy Label Facial Expression Recognition</title>
      <link>https://paperswithcode.com/paper/learn-from-all-erasing-attention-consistency</link>
      <description><![CDATA[We find that FER models remember noisy samples by focusing on a part of the features that can be considered related to the noisy labels instead of learning from the whole features that lead to the latent truth.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/learn-from-all-erasing-attention-consistency</guid>
    </item>
    <item>
      <title>A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and Restoration</title>
      <link>https://paperswithcode.com/paper/a-survey-on-leveraging-pre-trained-generative</link>
      <description><![CDATA[Generative adversarial networks (GANs) have drawn enormous attention due to the simple yet effective training mechanism and superior image generation quality.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/a-survey-on-leveraging-pre-trained-generative</guid>
    </item>
    <item>
      <title>Unsupervised pre-training of graph transformers on patient population graphs</title>
      <link>https://paperswithcode.com/paper/unsupervised-pre-training-of-graph</link>
      <description><![CDATA[We find that our proposed pre-training methods help in modeling the data at a patient and population level and improve performance in different fine-tuning tasks on all datasets.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/unsupervised-pre-training-of-graph</guid>
    </item>
    <item>
      <title>DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network</title>
      <link>https://paperswithcode.com/paper/dc-shadownet-single-image-hard-and-soft-1</link>
      <description><![CDATA[To address the problem, in this paper, we propose an unsupervised domain-classifier guided shadow removal network, DC-ShadowNet.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/dc-shadownet-single-image-hard-and-soft-1</guid>
    </item>
    <item>
      <title>SplitMixer: Fat Trimmed From MLP-like Models</title>
      <link>https://paperswithcode.com/paper/splitmixer-fat-trimmed-from-mlp-like-models</link>
      <description><![CDATA[We show, both theoretically and experimentally, that SplitMixer performs on par with the state-of-the-art MLP-like models while having a significantly lower number of parameters and FLOPS.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/splitmixer-fat-trimmed-from-mlp-like-models</guid>
    </item>
    <item>
      <title>Multi Resolution Analysis (MRA) for Approximate Self-Attention</title>
      <link>https://paperswithcode.com/paper/multi-resolution-analysis-mra-for-approximate</link>
      <description><![CDATA[Transformers have emerged as a preferred model for many tasks in natural langugage processing and vision.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/multi-resolution-analysis-mra-for-approximate</guid>
    </item>
    <item>
      <title>Towards Better Evaluation for Dynamic Link Prediction</title>
      <link>https://paperswithcode.com/paper/towards-better-evaluation-for-dynamic-link</link>
      <description><![CDATA[We design new, more stringent evaluation procedures for link prediction specific to dynamic graphs, which reflect real-world considerations and can better compare different methods' strengths and weaknesses.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/towards-better-evaluation-for-dynamic-link</guid>
    </item>
    <item>
      <title>Fully Sparse 3D Object Detection</title>
      <link>https://paperswithcode.com/paper/fully-sparse-3d-object-detection</link>
      <description><![CDATA[To enable efficient long-range LiDAR-based object detection, we build a fully sparse 3D object detector (FSD).]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/fully-sparse-3d-object-detection</guid>
    </item>
    <item>
      <title>Visual Knowledge Tracing</title>
      <link>https://paperswithcode.com/paper/visual-knowledge-tracing</link>
      <description><![CDATA[In this work, we propose a novel task of tracing the evolving classification behavior of human learners as they engage in challenging visual classification tasks.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/visual-knowledge-tracing</guid>
    </item>
    <item>
      <title>DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in Real Environments</title>
      <link>https://paperswithcode.com/paper/deepipc-deeply-integrated-perception-and</link>
      <description><![CDATA[We propose DeepIPC, an end-to-end multi-task model that handles both perception and control tasks in driving a mobile robot autonomously.]]></description>
      <guid isPermaLink="true">https://paperswithcode.com/paper/deepipc-deeply-integrated-perception-and</guid>
    </item>
  </channel>
</rss>
